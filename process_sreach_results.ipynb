{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T11:36:31.494938Z",
     "start_time": "2025-03-21T11:36:31.265303Z"
    }
   },
   "source": [
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "27cb331389ae82bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T11:36:42.446143Z",
     "start_time": "2025-03-21T11:36:33.056733Z"
    }
   },
   "source": [
    "nli_df = pd.read_csv('/Users/noamkashi/Documents/תוצאות ספרית שאטרסטום/merged_books_data.csv')\n",
    "google_df = pd.read_csv('/Users/noamkashi/Documents/תוצאות ספרית שאטרסטום/merged_results_google.csv')\n",
    "print(google_df.columns)\n",
    "print(nli_df.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['original_index', 'book name', 'result_rank', 'title', 'authors',\n",
      "       'published_date', 'page_count', 'categories', 'language',\n",
      "       'preview_link', 'google_books_id', 'description', 'index', 'level_0'],\n",
      "      dtype='object')\n",
      "Index(['book name', 'nli_search', 'book_index', 'nli_id', 'nli_language',\n",
      "       'nli_identifier', 'nli_creator', 'nli_date', 'nli_accessRights',\n",
      "       'nli_title', 'nli_type', 'nli_recordid', 'nli_source', 'nli_linkToMarc',\n",
      "       'nli_thumbnail', 'nli_download', 'nli_relation',\n",
      "       'nli_non_standard_date', 'nli_publisher', 'nli_contributor',\n",
      "       'nli_format', 'nli_subject', 'nli_wgs84_pos', 'nli_isbn', 'source',\n",
      "       'index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a86bbd1c-009b-416b-8ba3-7726306ec8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, add a source column to each dataframe\n",
    "google_df['row_source'] = 'google'\n",
    "nli_df['row_source'] = 'nli'\n",
    "\n",
    "# Use pandas concat to stack the dataframes vertically (like UNION)\n",
    "merged_df = pd.concat([google_df, nli_df], ignore_index=True)\n",
    "\n",
    "# If you want to sort by book name to have similar books near each other\n",
    "merged_df = merged_df.sort_values(by='book name')\n",
    "\n",
    "# Optionally add a rank column\n",
    "merged_df['rank'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "278109c1-11d3-4415-adbf-fa4e60fab045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1706027 entries, 0 to 1706026\n",
      "Data columns (total 40 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   original_index         float64\n",
      " 1   book name              object \n",
      " 2   result_rank            float64\n",
      " 3   title                  object \n",
      " 4   authors                object \n",
      " 5   published_date         float64\n",
      " 6   page_count             float64\n",
      " 7   categories             object \n",
      " 8   language               object \n",
      " 9   preview_link           object \n",
      " 10  google_books_id        object \n",
      " 11  description            object \n",
      " 12  index_google           float64\n",
      " 13  level_0                float64\n",
      " 14  nli_search             object \n",
      " 15  book_index             float64\n",
      " 16  nli_id                 object \n",
      " 17  nli_language           object \n",
      " 18  nli_identifier         object \n",
      " 19  nli_creator            object \n",
      " 20  nli_date               object \n",
      " 21  nli_accessRights       object \n",
      " 22  nli_title              object \n",
      " 23  nli_type               object \n",
      " 24  nli_recordid           object \n",
      " 25  nli_source             object \n",
      " 26  nli_linkToMarc         object \n",
      " 27  nli_thumbnail          object \n",
      " 28  nli_download           object \n",
      " 29  nli_relation           object \n",
      " 30  nli_non_standard_date  object \n",
      " 31  nli_publisher          object \n",
      " 32  nli_contributor        object \n",
      " 33  nli_format             object \n",
      " 34  nli_subject            object \n",
      " 35  nli_wgs84_pos          object \n",
      " 36  nli_isbn               object \n",
      " 37  index_nli              float64\n",
      " 38  row_source             object \n",
      " 39  rank                   float64\n",
      "dtypes: float64(9), object(31)\n",
      "memory usage: 520.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "951b41a4-3d31-417e-ac2e-5086f7d476f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matching_words_improved(book_name, target_text):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(book_name) or pd.isna(target_text):\n",
    "        return 0\n",
    "    \n",
    "    # Convert to lowercase and split into words\n",
    "    book_words = set(book_name.lower().split())\n",
    "    target_words = target_text.lower().split()\n",
    "    \n",
    "    matches = 0\n",
    "    \n",
    "    # For each word in the book name\n",
    "    for book_word in book_words:\n",
    "        # Check for direct match\n",
    "        if book_word in target_words:\n",
    "            matches += 1\n",
    "        # Check for match without initial ה if the word starts with ה and is long enough\n",
    "        elif book_word.startswith('ה') and len(book_word) > 2:\n",
    "            word_without_h = book_word[1:]  # Remove the first letter (ה)\n",
    "            if word_without_h in target_words:\n",
    "                matches += 1\n",
    "        # Check if target words have ה prefix that book_word doesn't have\n",
    "        else:\n",
    "            prefixed_word = 'ה' + book_word\n",
    "            if prefixed_word in target_words:\n",
    "                matches += 1\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Define the word matching function with a different name\n",
    "def count_matching_words_authors(book_name, target_text):\n",
    "    # Handle NaN values\n",
    "    if pd.isna(book_name) or pd.isna(target_text):\n",
    "        return 0\n",
    "    \n",
    "    # Convert to lowercase and split into words\n",
    "    book_words = set(book_name.lower().split())\n",
    "    target_words = target_text.lower().split()\n",
    "    \n",
    "    # Count matches\n",
    "    matches = sum(1 for word in book_words if word in target_words)\n",
    "    return matches\n",
    "\n",
    "# Create a function to apply to each row for authors and nli_creator with a unique name\n",
    "def update_rank_for_author_matches(row):\n",
    "    additional_rank = 0\n",
    "    \n",
    "    # Check authors column (from google_df)\n",
    "    if not pd.isna(row.get('authors')):\n",
    "        matches = count_matching_words_authors(row['book name'], row['authors'])\n",
    "        additional_rank += matches * 0.5\n",
    "    \n",
    "    # Check nli_creator column (from nli_df)\n",
    "    if not pd.isna(row.get('nli_creator')):\n",
    "        matches = count_matching_words_authors(row['book name'], row['nli_creator'])\n",
    "        additional_rank += matches * 0.5\n",
    "    \n",
    "    # Return the additional rank to be added\n",
    "    return additional_rank\n",
    "    \n",
    "# Define the list of words to check for\n",
    "publisher_words = [\"ירושלים\", \"חיפה\", \"תל אביב\"]\n",
    "\n",
    "# Function to check if any of the specified words are in the publisher text\n",
    "def check_publisher_words(publisher_text):\n",
    "    if pd.isna(publisher_text):\n",
    "        return False\n",
    "    \n",
    "    # Convert to string in case it's not already\n",
    "    publisher_text = str(publisher_text)\n",
    "    \n",
    "    # Check if any of the words are in the text\n",
    "    for word in publisher_words:\n",
    "        if word in publisher_text:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "# Define the list of words to check for\n",
    "title_words = [\"מתוך\", \"חלק\"]\n",
    "\n",
    "# Function to check if any of the specified words are in the title text\n",
    "def check_title_words(row):\n",
    "    # Initialize result\n",
    "    contains_word = False\n",
    "    \n",
    "    # Check title column\n",
    "    if not pd.isna(row.get('title')):\n",
    "        title_text = str(row['title'])\n",
    "        for word in title_words:\n",
    "            if word in title_text:\n",
    "                contains_word = True\n",
    "                break\n",
    "    \n",
    "    # Check nli_title column if we haven't found a match yet\n",
    "    if not contains_word and not pd.isna(row.get('nli_title')):\n",
    "        nli_title_text = str(row['nli_title'])\n",
    "        for word in title_words:\n",
    "            if word in nli_title_text:\n",
    "                contains_word = True\n",
    "                break\n",
    "    \n",
    "    return contains_word\n",
    "format_words = [\"כרך\", \"כרכים\"]\n",
    "\n",
    "def check_format_words(format_text):\n",
    "    if pd.isna(format_text):\n",
    "        return False\n",
    "    \n",
    "    # Convert to string in case it's not already\n",
    "    format_text = str(format_text)\n",
    "    \n",
    "    # Check if any of the words are in the text\n",
    "    for word in format_words:\n",
    "        if word in format_text:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "#ran all ranking\n",
    "\n",
    "# Apply the function to calculate additional rank for each row\n",
    "additional_ranks_authors = merged_df.apply(update_rank_for_author_matches, axis=1)\n",
    "\n",
    "# Add the additional ranks to the existing rank column\n",
    "merged_df['rank'] += additional_ranks_authors\n",
    "\n",
    "# Apply the function to calculate additional rank for each row\n",
    "additional_ranks = merged_df.apply(update_rank_for_matching_words, axis=1)\n",
    "\n",
    "# Add the additional ranks to the existing rank column\n",
    "merged_df['rank'] += additional_ranks\n",
    "\n",
    "#add 1 pint for nli sourceֿ\n",
    "merged_df.loc[merged_df['row_source'] == 'nli', 'rank'] += 1.0\n",
    "\n",
    "merged_df.loc[merged_df['nli_publisher'].apply(check_publisher_words), 'rank'] -= 1\n",
    "merged_df.loc[merged_df.apply(check_title_words, axis=1), 'rank'] -= 1\n",
    "merged_df.loc[merged_df['nli_format'].apply(check_format_words), 'rank'] += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33213189-7d21-4a95-86a7-1c23043ea453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove rows where 'book name' contains 'הדור'\n",
    "filtered_df = merged_df[~merged_df['book name'].str.contains('הדור', na=False)]\n",
    "\n",
    "# 2. Sort by rank in descending order (highest rank first)\n",
    "filtered_df = filtered_df.sort_values(by='rank', ascending=False)\n",
    "\n",
    "# 3. Then remove duplicates based on the specified columns\n",
    "# This keeps the first occurrence of each duplicate set (which will be the one with highest rank due to sorting)\n",
    "deduplicated_df = filtered_df.drop_duplicates(\n",
    "    subset=['title', 'authors', 'published_date', 'page_count', 'categories'],\n",
    "    keep='first'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfcb83a6-bcf2-429c-bb36-ac9d2f987469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the first 600 rows\n",
    "top_600_df = deduplicated_df.head(600)\n",
    "\n",
    "# Save to CSV\n",
    "top_600_df.to_csv('top_600_merged_books.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2095be39-181e-4528-a12b-9076bdf1e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/l8nqtyf91pb7vks5m2l_jpgm0000gn/T/ipykernel_13912/3823076772.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  google_top = merged_df[merged_df['row_source'] == 'google'].groupby('book name').apply(\n",
      "/var/folders/p1/l8nqtyf91pb7vks5m2l_jpgm0000gn/T/ipykernel_13912/3823076772.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  nli_top = merged_df[merged_df['row_source'] == 'nli'].groupby('book name').apply(\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store DataFrames\n",
    "top_dfs = []\n",
    "\n",
    "# Process Google data\n",
    "google_top = merged_df[merged_df['row_source'] == 'google'].groupby('book name').apply(\n",
    "    lambda x: x.nlargest(2, 'rank')\n",
    ").reset_index(drop=True)\n",
    "top_dfs.append(google_top)\n",
    "\n",
    "# Process NLI data\n",
    "nli_top = merged_df[merged_df['row_source'] == 'nli'].groupby('book name').apply(\n",
    "    lambda x: x.nlargest(2, 'rank')\n",
    ").reset_index(drop=True)\n",
    "top_dfs.append(nli_top)\n",
    "\n",
    "# Combine the top entries from both sources\n",
    "final_df = pd.concat(top_dfs, ignore_index=True)\n",
    "\n",
    "# Sort by book name and then by rank in descending order\n",
    "final_df = final_df.sort_values(by=['book name', 'rank'], ascending=[True, False])\n",
    "\n",
    "# Reset index\n",
    "final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "final_df.to_csv('final_top2_per_source_per_book.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3c3b35f-e4c2-485c-b1e9-97182bcb0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the desired columns from the final dataframe\n",
    "selected_columns = [\n",
    "    'book name',\n",
    "    'title',\n",
    "    'nli_title',\n",
    "    'authors',\n",
    "    'rank',\n",
    "    'published_date',\n",
    "    'nli_creator',\n",
    "    'row_source',\n",
    "]\n",
    "\n",
    "# Create the new dataframe with only these columns\n",
    "simplified_df = final_df[selected_columns]\n",
    "\n",
    "# Save to CSV\n",
    "simplified_df.to_csv('simplified_book_data.csv', index=False)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T08:45:56.428127Z",
     "start_time": "2025-03-23T08:45:55.115383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import json\n",
    "import time\n",
    "\n",
    "API_KEYS = [\n",
    "    'ohSupNWHPC2ncMlosQYmMYzHp7GwmZ4HKY0PXoMX',\n",
    "    'mZcuSKOCiaboR9ctxclfeMACDtXAgoYQL3cdTRWS',\n",
    "    't2A91mZgPnhShnDsvGWZWmnj9s7OsTDW072xrg3e',\n",
    "    'm5TELRWYrhqJmVj10VEjUQVPTIduz02QiFpFtWEh',\n",
    "    'Od48KLpdU4sQK4k4d5Lf0rtbcTP1NGpVmJkXurM7'\n",
    "]\n",
    "BASE_URL = 'https://api.nli.org.il/openlibrary/search'\n",
    "\n",
    "current_key_index = 0  # Start with the first key\n",
    "\n",
    "def get_current_api_key():\n",
    "    return API_KEYS[current_key_index]\n",
    "\n",
    "def switch_to_next_api_key():\n",
    "    global current_key_index\n",
    "    current_key_index = (current_key_index + 1) % len(API_KEYS)\n",
    "    print(f\"Switching to API key: {get_current_api_key()[:5]}...\")\n",
    "    return get_current_api_key()\n",
    "\n",
    "def search_nli(search_string, retry_count=0):\n",
    "    \"\"\"\n",
    "    Search the NLI API for a specific string and return the response\n",
    "    \"\"\"\n",
    "    if retry_count >= len(API_KEYS):\n",
    "        print(f\"All API keys are rate limited. Please try again later.\")\n",
    "        return None\n",
    "\n",
    "    current_api_key = get_current_api_key()\n",
    "\n",
    "    try:\n",
    "        # Encode the search string for URL\n",
    "        encoded_search = urllib.parse.quote(search_string)\n",
    "\n",
    "        # Construct the URL - using 'any,contains' to search across all fields\n",
    "        url = f\"{BASE_URL}?api_key={current_api_key}&query=any,contains,{encoded_search}&bulkSize=30&index=0&sort=rank\"\n",
    "\n",
    "        print(f\"Request URL: {url}\")\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 429 or \"OVER_RATE_LIMIT\" in response.text:\n",
    "            print(f\"Rate limit exceeded for key {current_api_key[:5]}...\")\n",
    "            switch_to_next_api_key()\n",
    "            # Retry with the new key\n",
    "            return search_nli(search_string, retry_count + 1)\n",
    "        else:\n",
    "            # Print error information\n",
    "            error_msg = f\"Error: {response.status_code} - {response.text}\"\n",
    "            print(f\"Search term: '{search_string}' - {error_msg}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Exception occurred: {e}\"\n",
    "        print(f\"Search term: '{search_string}' - {error_msg}\")\n",
    "        return None\n",
    "\n",
    "# The specific ID you want to search\n",
    "search_id = \"990020209770205171\"\n",
    "\n",
    "print(f\"Searching for ID: {search_id}\")\n",
    "print(f\"Starting with API key: {get_current_api_key()[:5]}...\")\n",
    "\n",
    "# Perform the search\n",
    "result = search_nli(search_id)\n",
    "\n",
    "# Process and display the results\n",
    "if result:\n",
    "    # Check the type of result and process accordingly\n",
    "    if isinstance(result, dict):\n",
    "        total_found = result.get('total', 0)\n",
    "        print(f\"\\nSearch complete. Found {total_found} results.\")\n",
    "\n",
    "        # Get the docs part if it exists\n",
    "        if 'docs' in result and result['docs']:\n",
    "            docs = result['docs']\n",
    "            print(f\"Number of documents returned: {len(docs)}\")\n",
    "\n",
    "            # Process the first document\n",
    "            if docs:\n",
    "                print(\"\\nDetails of first result:\")\n",
    "                first_doc = docs[0]\n",
    "\n",
    "                # Display key information if available\n",
    "                print(f\"Title: {first_doc.get('title', 'N/A')}\")\n",
    "                print(f\"Author: {', '.join(first_doc.get('author', ['N/A'])) if 'author' in first_doc and isinstance(first_doc['author'], list) else 'N/A'}\")\n",
    "                print(f\"Year: {first_doc.get('year', 'N/A')}\")\n",
    "                print(f\"Record ID: {first_doc.get('recordid', 'N/A')}\")\n",
    "\n",
    "                # Save the full result to a JSON file\n",
    "                with open(f'nli_search_result_{search_id}.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "                print(f\"\\nFull results saved to nli_search_result_{search_id}.json\")\n",
    "        else:\n",
    "            print(\"No documents found in the response.\")\n",
    "    elif isinstance(result, list):\n",
    "        # The result is already a list of documents\n",
    "        docs = result\n",
    "        print(f\"\\nSearch complete. Found {len(docs)} results.\")\n",
    "\n",
    "        if docs:\n",
    "            print(\"\\nDetails of first result:\")\n",
    "            first_doc = docs[0]\n",
    "\n",
    "            # Display key information if available (using dict.get() for safety)\n",
    "            print(f\"Title: {first_doc.get('title', 'N/A')}\")\n",
    "            print(f\"Author: {', '.join(first_doc.get('author', ['N/A'])) if 'author' in first_doc and isinstance(first_doc['author'], list) else 'N/A'}\")\n",
    "            print(f\"Year: {first_doc.get('year', 'N/A')}\")\n",
    "            print(f\"Record ID: {first_doc.get('recordid', 'N/A')}\")\n",
    "\n",
    "            # Save the full result to a JSON file\n",
    "            with open(f'nli_search_result_{search_id}.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nFull results saved to nli_search_result_{search_id}.json\")\n",
    "        else:\n",
    "            print(\"No documents found in the response.\")\n",
    "    else:\n",
    "        print(\"No documents found in the response.\")\n",
    "else:\n",
    "    print(\"The search did not return any results or encountered an error.\")"
   ],
   "id": "dc36deaaa3a1bfbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for ID: 990020209770205171\n",
      "Starting with API key: ohSup...\n",
      "Request URL: https://api.nli.org.il/openlibrary/search?api_key=ohSupNWHPC2ncMlosQYmMYzHp7GwmZ4HKY0PXoMX&query=any,contains,990020209770205171&bulkSize=30&index=0&sort=rank\n",
      "\n",
      "Search complete. Found 1 results.\n",
      "\n",
      "Details of first result:\n",
      "Title: N/A\n",
      "Author: N/A\n",
      "Year: N/A\n",
      "Record ID: N/A\n",
      "\n",
      "Full results saved to nli_search_result_990020209770205171.json\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T10:44:38.365842Z",
     "start_time": "2025-04-01T10:44:07.706485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Your API keys\n",
    "API_KEYS = [\n",
    "    'mZcuSKOCiaboR9ctxclfeMACDtXAgoYQL3cdTRWS',\n",
    "    't2A91mZgPnhShnDsvGWZWmnj9s7OsTDW072xrg3e',\n",
    "    'm5TELRWYrhqJmVj10VEjUQVPTIduz02QiFpFtWEh',\n",
    "    'Od48KLpdU4sQK4k4d5Lf0rtbcTP1NGpVmJkXurM7'\n",
    "]\n",
    "BASE_URL = 'https://api.nli.org.il/openlibrary/search'\n",
    "\n",
    "current_key_index = 0  # Start with the first key\n",
    "\n",
    "def get_current_api_key():\n",
    "    return API_KEYS[current_key_index]\n",
    "\n",
    "def switch_to_next_api_key():\n",
    "    global current_key_index\n",
    "    current_key_index = (current_key_index + 1) % len(API_KEYS)\n",
    "    print(f\"Switching to API key: {get_current_api_key()[:5]}...\")\n",
    "    return get_current_api_key()\n",
    "\n",
    "def search_nli(search_string, retry_count=0):\n",
    "    \"\"\"\n",
    "    Search the NLI API for a specific string and return the response\n",
    "    \"\"\"\n",
    "    if retry_count >= len(API_KEYS):\n",
    "        print(f\"All API keys are rate limited. Please try again later.\")\n",
    "        return None\n",
    "\n",
    "    current_api_key = get_current_api_key()\n",
    "\n",
    "    try:\n",
    "        # Encode the search string for URL\n",
    "        encoded_search = urllib.parse.quote(search_string)\n",
    "\n",
    "        # Construct the URL - using 'any,contains' to search across all fields\n",
    "        url = f\"{BASE_URL}?api_key={current_api_key}&query=any,contains,{encoded_search}&bulkSize=30&index=0&sort=rank\"\n",
    "\n",
    "        print(f\"Searching for ID: {search_string}\")\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        elif response.status_code == 429 or \"OVER_RATE_LIMIT\" in response.text:\n",
    "            print(f\"Rate limit exceeded for key {current_api_key[:5]}...\")\n",
    "            switch_to_next_api_key()\n",
    "            # Retry with the new key\n",
    "            return search_nli(search_string, retry_count + 1)\n",
    "        else:\n",
    "            # Print error information\n",
    "            error_msg = f\"Error: {response.status_code} - {response.text}\"\n",
    "            print(f\"Search term: '{search_string}' - {error_msg}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Exception occurred: {e}\"\n",
    "        print(f\"Search term: '{search_string}' - {error_msg}\")\n",
    "        return None\n",
    "\n",
    "def clean_value(value):\n",
    "    # Remove any special formatting codes like $$Q, $$V, etc.\n",
    "    return re.sub(r'\\$\\$[A-Z][^$]*', ' ', value).strip()\n",
    "\n",
    "def parse_contributors(contrib_str):\n",
    "    # Split the contributors by the pattern \"$$Q\"\n",
    "    contributors = []\n",
    "    parts = contrib_str.split('$$Q')\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            contributors.append(clean_value(part))\n",
    "    return contributors\n",
    "\n",
    "def convert_nli_json_to_dict(data):\n",
    "    # Create a dictionary to store the extracted data\n",
    "    result = {}\n",
    "\n",
    "    if data and len(data) > 0:\n",
    "        item = data[0]\n",
    "\n",
    "        # Extract basic metadata\n",
    "        result['id'] = item.get('@id', '')\n",
    "\n",
    "        # Title\n",
    "        title_list = item.get('http://purl.org/dc/elements/1.1/title', [])\n",
    "        if title_list:\n",
    "            result['title'] = title_list[0].get('@value', '')\n",
    "\n",
    "        # Type\n",
    "        type_list = item.get('http://purl.org/dc/elements/1.1/type', [])\n",
    "        if type_list:\n",
    "            result['type'] = type_list[0].get('@value', '')\n",
    "\n",
    "        # Record ID\n",
    "        record_id_list = item.get('http://purl.org/dc/elements/1.1/recordid', [])\n",
    "        if record_id_list:\n",
    "            result['record_id'] = record_id_list[0].get('@value', '')\n",
    "\n",
    "        # Language\n",
    "        lang_list = item.get('http://purl.org/dc/elements/1.1/language', [])\n",
    "        if lang_list:\n",
    "            result['language'] = lang_list[0].get('@value', '')\n",
    "\n",
    "        # Subject\n",
    "        subject_list = item.get('http://purl.org/dc/elements/1.1/subject', [])\n",
    "        if subject_list:\n",
    "            result['subject'] = subject_list[0].get('@value', '')\n",
    "\n",
    "        # Publisher\n",
    "        publisher_list = item.get('http://purl.org/dc/elements/1.1/publisher', [])\n",
    "        if publisher_list:\n",
    "            result['publisher'] = publisher_list[0].get('@value', '')\n",
    "\n",
    "        # Date\n",
    "        date_list = item.get('http://purl.org/dc/elements/1.1/non_standard_date', [])\n",
    "        if date_list:\n",
    "            result['date'] = date_list[0].get('@value', '')\n",
    "\n",
    "        # Contributors - parse the complex string into a list and join with semicolons for CSV\n",
    "        contrib_list = item.get('http://purl.org/dc/elements/1.1/contributor', [])\n",
    "        if contrib_list:\n",
    "            contrib_str = contrib_list[0].get('@value', '')\n",
    "            result['contributors'] = '; '.join(parse_contributors(contrib_str))\n",
    "\n",
    "        # Format\n",
    "        format_list = item.get('http://purl.org/dc/elements/1.1/format', [])\n",
    "        if format_list:\n",
    "            result['format'] = format_list[0].get('@value', '')\n",
    "\n",
    "        # Thumbnail\n",
    "        thumbnail_list = item.get('http://purl.org/dc/elements/1.1/thumbnail', [])\n",
    "        if thumbnail_list:\n",
    "            result['thumbnail'] = thumbnail_list[0].get('@value', '')\n",
    "\n",
    "        # Access Rights\n",
    "        access_list = item.get('http://purl.org/dc/elements/1.1/accessRights', [])\n",
    "        if access_list:\n",
    "            result['access_rights'] = access_list[0].get('@value', '')\n",
    "\n",
    "        # Link to MARC\n",
    "        marc_list = item.get('http://purl.org/dc/elements/1.1/linkToMarc', [])\n",
    "        if marc_list:\n",
    "            result['link_to_marc'] = marc_list[0].get('@id', '')\n",
    "\n",
    "    return result\n",
    "\n",
    "# Main function to process all IDs\n",
    "def process_missing_data(csv_file_path):\n",
    "    # Read the CSV file with missing data\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Check if 'id' column exists\n",
    "    if 'id' not in df.columns:\n",
    "        print(\"Error: 'id' column not found in the CSV file.\")\n",
    "        return\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Process each ID in the CSV file\n",
    "    for index, row in df.iterrows():\n",
    "        search_id = str(row['id']).strip()\n",
    "        print(f\"\\nProcessing ID {index+1}/{len(df)}: {search_id}\")\n",
    "\n",
    "        # Skip empty IDs\n",
    "        if not search_id or pd.isna(search_id):\n",
    "            print(\"Skipping empty ID\")\n",
    "            results.append({})\n",
    "            continue\n",
    "\n",
    "        # Search the NLI API\n",
    "        result = search_nli(search_id)\n",
    "\n",
    "        # If we got a result, convert it to a dict\n",
    "        if result:\n",
    "            # Check if the result is a dict with 'docs' or a list\n",
    "            if isinstance(result, dict) and 'docs' in result and result['docs']:\n",
    "                converted_data = convert_nli_json_to_dict(result['docs'])\n",
    "                results.append(converted_data)\n",
    "            elif isinstance(result, list) and result:\n",
    "                converted_data = convert_nli_json_to_dict(result)\n",
    "                results.append(converted_data)\n",
    "            else:\n",
    "                print(f\"No valid data found for ID: {search_id}\")\n",
    "                results.append({})\n",
    "        else:\n",
    "            print(f\"No result found for ID: {search_id}\")\n",
    "            results.append({})\n",
    "\n",
    "        # Add a small delay to avoid overwhelming the API\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # Merge the original dataframe with the results\n",
    "    if not results_df.empty:\n",
    "        # Remove columns from original df that will be replaced by results_df\n",
    "        # to avoid duplicate columns with suffix '_x' and '_y'\n",
    "        for col in results_df.columns:\n",
    "            if col in df.columns and col != 'id':\n",
    "                df = df.drop(columns=[col])\n",
    "\n",
    "        # Merge the dataframes\n",
    "        merged_df = pd.concat([df, results_df], axis=1)\n",
    "    else:\n",
    "        merged_df = df\n",
    "\n",
    "    # Save the result to a new CSV file\n",
    "    output_file = csv_file_path.replace('.csv', '_enriched.csv')\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"\\nEnriched data saved to: {output_file}\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    # File path\n",
    "    csv_file_path = '/Users/noamkashi/Downloads/last_books_for_real - Sheet1.csv'\n",
    "\n",
    "    print(f\"Starting with API key: {get_current_api_key()[:5]}...\")\n",
    "\n",
    "    # Process the data\n",
    "    enriched_data = process_missing_data(csv_file_path)\n",
    "\n",
    "    print(\"Process completed successfully!\")"
   ],
   "id": "66b1decd4f2c655e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with API key: mZcuS...\n",
      "\n",
      "Processing ID 1/21: 990020950010205171\n",
      "Searching for ID: 990020950010205171\n",
      "\n",
      "Processing ID 2/21: 990020636480205171\n",
      "Searching for ID: 990020636480205171\n",
      "\n",
      "Processing ID 3/21: 990020636480205171\n",
      "Searching for ID: 990020636480205171\n",
      "\n",
      "Processing ID 4/21: 990020636480205171\n",
      "Searching for ID: 990020636480205171\n",
      "\n",
      "Processing ID 5/21: 990020636480205171\n",
      "Searching for ID: 990020636480205171\n",
      "\n",
      "Processing ID 6/21: 990020636480205171\n",
      "Searching for ID: 990020636480205171\n",
      "\n",
      "Processing ID 7/21: 990050547310205171\n",
      "Searching for ID: 990050547310205171\n",
      "\n",
      "Processing ID 8/21: 990018647290205171\n",
      "Searching for ID: 990018647290205171\n",
      "\n",
      "Processing ID 9/21: 990018647290205171\n",
      "Searching for ID: 990018647290205171\n",
      "\n",
      "Processing ID 10/21: 990018647290205171\n",
      "Searching for ID: 990018647290205171\n",
      "\n",
      "Processing ID 11/21: 990052861010205171\n",
      "Searching for ID: 990052861010205171\n",
      "\n",
      "Processing ID 12/21: 990019740380205171\n",
      "Searching for ID: 990019740380205171\n",
      "\n",
      "Processing ID 13/21: 990018084370205171\n",
      "Searching for ID: 990018084370205171\n",
      "\n",
      "Processing ID 14/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 15/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 16/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 17/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 18/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 19/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 20/21: 990020027720205171\n",
      "Searching for ID: 990020027720205171\n",
      "\n",
      "Processing ID 21/21: 990019817670205171\n",
      "Searching for ID: 990019817670205171\n",
      "\n",
      "Enriched data saved to: /Users/noamkashi/Downloads/last_books_for_real - Sheet1_enriched.csv\n",
      "Process completed successfully!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:30:16.726526Z",
     "start_time": "2025-03-24T19:30:14.262789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "def match_and_merge_csvs(first_csv_path, second_csv_path):\n",
    "    # Read both CSV files\n",
    "    print(f\"Reading files...\")\n",
    "    df1 = pd.read_csv(first_csv_path)\n",
    "    df2 = pd.read_csv(second_csv_path)\n",
    "\n",
    "    print(f\"First CSV shape: {df1.shape}\")\n",
    "    print(f\"Second CSV shape: {df2.shape}\")\n",
    "\n",
    "    # Columns to match (all columns except 'book name' and 'nli_recordid')\n",
    "    match_columns = ['title', 'nli_title', 'authors', 'rank', 'published_date', 'nli_creator', 'row_source']\n",
    "\n",
    "    # Check if all required columns exist in both dataframes\n",
    "    missing_cols_df1 = [col for col in match_columns if col not in df1.columns]\n",
    "    missing_cols_df2 = [col for col in match_columns if col not in df2.columns]\n",
    "\n",
    "    if missing_cols_df1:\n",
    "        print(f\"Error: Columns {missing_cols_df1} missing in first CSV.\")\n",
    "        return\n",
    "\n",
    "    if missing_cols_df2:\n",
    "        print(f\"Error: Columns {missing_cols_df2} missing in second CSV.\")\n",
    "        return\n",
    "\n",
    "    if 'nli_recordid' not in df2.columns:\n",
    "        print(f\"Error: Column 'nli_recordid' missing in second CSV.\")\n",
    "        return\n",
    "\n",
    "    # Create a new 'id' column in df1 initialized with None/NaN\n",
    "    df1['id'] = None\n",
    "\n",
    "    # Count matches\n",
    "    match_count = 0\n",
    "\n",
    "    print(\"Finding matches and merging data...\")\n",
    "\n",
    "    # For each row in df1, find matching rows in df2\n",
    "    for index1, row1 in df1.iterrows():\n",
    "        # Create a mask for matching all specified columns\n",
    "        mask = pd.Series(True, index=df2.index)\n",
    "\n",
    "        for col in match_columns:\n",
    "            # Handle NaN values specially\n",
    "            if pd.isna(row1[col]):\n",
    "                mask = mask & df2[col].isna()\n",
    "            else:\n",
    "                mask = mask & (df2[col] == row1[col])\n",
    "\n",
    "        # Find matching rows in df2\n",
    "        matching_rows = df2[mask]\n",
    "\n",
    "        # If there's a match, get the nli_recordid from df2\n",
    "        if not matching_rows.empty:\n",
    "            match_count += 1\n",
    "            # Take the first match if multiple exist\n",
    "            df1.at[index1, 'id'] = matching_rows.iloc[0]['nli_recordid']\n",
    "\n",
    "    print(f\"Found {match_count} matches out of {len(df1)} rows.\")\n",
    "\n",
    "    # Save the updated df1 as a new CSV\n",
    "    output_path = first_csv_path.replace('.csv', '_with_ids.csv')\n",
    "    df1.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Updated data saved to: {output_path}\")\n",
    "\n",
    "    return df1\n",
    "\n",
    "# File paths\n",
    "first_csv_path = '/Users/noamkashi/Downloads/missing data - nli books.csv'\n",
    "second_csv_path = '/Users/noamkashi/Documents/Strashum Library/final_top2_per_source_per_book.csv'\n",
    "\n",
    "# Run the function\n",
    "result_df = match_and_merge_csvs(first_csv_path, second_csv_path)"
   ],
   "id": "f5763885e272496d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "First CSV shape: (537, 8)\n",
      "Second CSV shape: (4406, 40)\n",
      "Finding matches and merging data...\n",
      "Found 534 matches out of 537 rows.\n",
      "Updated data saved to: /Users/noamkashi/Downloads/missing data - nli books_with_ids.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T11:33:25.388033Z",
     "start_time": "2025-04-01T11:33:24.654654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Your API keys\n",
    "API_KEYS = [\n",
    "    'mZcuSKOCiaboR9ctxclfeMACDtXAgoYQL3cdTRWS',\n",
    "    't2A91mZgPnhShnDsvGWZWmnj9s7OsTDW072xrg3e',\n",
    "    'm5TELRWYrhqJmVj10VEjUQVPTIduz02QiFpFtWEh',\n",
    "    'Od48KLpdU4sQK4k4d5Lf0rtbcTP1NGpVmJkXurM7'\n",
    "]\n",
    "BASE_URL = 'https://api.nli.org.il/openlibrary/search'\n",
    "\n",
    "current_key_index = 0  # Start with the first key\n",
    "\n",
    "def get_current_api_key():\n",
    "    return API_KEYS[current_key_index]\n",
    "\n",
    "def switch_to_next_api_key():\n",
    "    global current_key_index\n",
    "    current_key_index = (current_key_index + 1) % len(API_KEYS)\n",
    "    print(f\"Switching to API key: {get_current_api_key()[:5]}...\")\n",
    "    return get_current_api_key()\n",
    "\n",
    "def search_nli(search_string, bulk_size=30, retry_count=0):\n",
    "    \"\"\"\n",
    "    Search the NLI API for a specific string and return all results\n",
    "    \"\"\"\n",
    "    if retry_count >= len(API_KEYS):\n",
    "        print(f\"All API keys are rate limited. Please try again later.\")\n",
    "        return None\n",
    "\n",
    "    current_api_key = get_current_api_key()\n",
    "\n",
    "    try:\n",
    "        # Encode the search string for URL\n",
    "        encoded_search = urllib.parse.quote(search_string)\n",
    "\n",
    "        # Construct the URL - using 'any,contains' to search across all fields\n",
    "        url = f\"{BASE_URL}?api_key={current_api_key}&query=any,contains,{encoded_search}&bulkSize={bulk_size}&index=0&sort=rank\"\n",
    "\n",
    "        print(f\"Searching for: {search_string}\")\n",
    "        print(f\"Request URL: {url}\")\n",
    "\n",
    "        # Make the request\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Print the raw response for debugging\n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        print(f\"Response content type: {response.headers.get('Content-Type', 'unknown')}\")\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                result = response.json()\n",
    "                # Print response structure for debugging\n",
    "                print(f\"Response contains keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dictionary'}\")\n",
    "\n",
    "                # Check the structure - NLI sometimes puts docs in different places\n",
    "                if isinstance(result, dict):\n",
    "                    if 'docs' in result:\n",
    "                        print(f\"Found {len(result['docs'])} documents in 'docs' key\")\n",
    "                    elif 'records' in result:\n",
    "                        print(f\"Found {len(result['records'])} documents in 'records' key\")\n",
    "                        # Adjust the structure for our processing\n",
    "                        result['docs'] = result['records']\n",
    "                    elif 'results' in result:\n",
    "                        print(f\"Found {len(result['results'])} documents in 'results' key\")\n",
    "                        # Adjust the structure for our processing\n",
    "                        result['docs'] = result['results']\n",
    "                    else:\n",
    "                        print(\"Response structure is not as expected. Raw data (first 500 chars):\")\n",
    "                        print(json.dumps(result)[:500] + \"...\")\n",
    "                else:\n",
    "                    print(\"Response is not a dictionary. Raw data (first 500 chars):\")\n",
    "                    print(json.dumps(result)[:500] + \"...\")\n",
    "\n",
    "                return result\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON response: {e}\")\n",
    "                print(\"Raw response (first 500 chars):\")\n",
    "                print(response.text[:500] + \"...\")\n",
    "                return None\n",
    "\n",
    "        elif response.status_code == 429 or \"OVER_RATE_LIMIT\" in response.text:\n",
    "            print(f\"Rate limit exceeded for key {current_api_key[:5]}...\")\n",
    "            switch_to_next_api_key()\n",
    "            # Retry with the new key\n",
    "            return search_nli(search_string, bulk_size, retry_count + 1)\n",
    "        else:\n",
    "            # Print error information\n",
    "            error_msg = f\"Error: {response.status_code} - {response.text[:200]}\"\n",
    "            print(f\"Search term: '{search_string}' - {error_msg}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Exception occurred: {e}\"\n",
    "        print(f\"Search term: '{search_string}' - {error_msg}\")\n",
    "        return None\n",
    "\n",
    "def clean_value(value):\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    # Remove any special formatting codes like $$Q, $$V, etc.\n",
    "    return re.sub(r'\\$\\$[A-Z][^$]*', ' ', value).strip()\n",
    "\n",
    "def parse_contributors(contrib_str):\n",
    "    if not contrib_str:\n",
    "        return []\n",
    "    # Split the contributors by the pattern \"$$Q\"\n",
    "    contributors = []\n",
    "    parts = contrib_str.split('$$Q')\n",
    "    for part in parts:\n",
    "        if part.strip():\n",
    "            contributors.append(clean_value(part))\n",
    "    return contributors\n",
    "\n",
    "def extract_value(item, field):\n",
    "    \"\"\"Extract a value from an item's field\"\"\"\n",
    "    value_list = item.get(field, [])\n",
    "    if value_list:\n",
    "        return value_list[0].get('@value', '') if '@value' in value_list[0] else value_list[0].get('@id', '')\n",
    "    return \"\"\n",
    "\n",
    "def convert_nli_docs_to_dicts(docs):\n",
    "    \"\"\"Convert NLI API docs to a list of dictionaries\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for item in docs:\n",
    "        # Create a dictionary to store the extracted data\n",
    "        result = {}\n",
    "\n",
    "        # Extract basic metadata\n",
    "        result['id'] = item.get('@id', '')\n",
    "\n",
    "        # Title\n",
    "        result['title'] = extract_value(item, 'http://purl.org/dc/elements/1.1/title')\n",
    "\n",
    "        # Type\n",
    "        result['type'] = extract_value(item, 'http://purl.org/dc/elements/1.1/type')\n",
    "\n",
    "        # Record ID\n",
    "        result['record_id'] = extract_value(item, 'http://purl.org/dc/elements/1.1/recordid')\n",
    "\n",
    "        # Language\n",
    "        result['language'] = extract_value(item, 'http://purl.org/dc/elements/1.1/language')\n",
    "\n",
    "        # Subject\n",
    "        result['subject'] = extract_value(item, 'http://purl.org/dc/elements/1.1/subject')\n",
    "\n",
    "        # Publisher\n",
    "        result['publisher'] = extract_value(item, 'http://purl.org/dc/elements/1.1/publisher')\n",
    "\n",
    "        # Date\n",
    "        result['date'] = extract_value(item, 'http://purl.org/dc/elements/1.1/non_standard_date')\n",
    "\n",
    "        # Contributors - parse the complex string into a list and join with semicolons for CSV\n",
    "        contrib_list = item.get('http://purl.org/dc/elements/1.1/contributor', [])\n",
    "        if contrib_list:\n",
    "            contrib_str = contrib_list[0].get('@value', '')\n",
    "            result['contributors'] = '; '.join(parse_contributors(contrib_str))\n",
    "        else:\n",
    "            result['contributors'] = ''\n",
    "\n",
    "        # Format\n",
    "        result['format'] = extract_value(item, 'http://purl.org/dc/elements/1.1/format')\n",
    "\n",
    "        # Thumbnail\n",
    "        result['thumbnail'] = extract_value(item, 'http://purl.org/dc/elements/1.1/thumbnail')\n",
    "\n",
    "        # Access Rights\n",
    "        result['access_rights'] = extract_value(item, 'http://purl.org/dc/elements/1.1/accessRights')\n",
    "\n",
    "        # Link to MARC\n",
    "        result['link_to_marc'] = extract_value(item, 'http://purl.org/dc/elements/1.1/linkToMarc')\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_to_csv(data, output_file):\n",
    "    \"\"\"Save data to CSV file\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data to save\")\n",
    "        return False\n",
    "\n",
    "    # Get all unique keys from all dictionaries\n",
    "    fieldnames = set()\n",
    "    for item in data:\n",
    "        fieldnames.update(item.keys())\n",
    "\n",
    "    fieldnames = sorted(list(fieldnames))\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    dir_name = os.path.dirname(output_file)\n",
    "    if dir_name and not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    return True\n",
    "\n",
    "def try_alternative_search(search_string, bulk_size=30):\n",
    "    \"\"\"Try different search formats if the main search fails\"\"\"\n",
    "    alternative_formats = [\n",
    "        # Try with explicit field searches\n",
    "        f\"dc.title,contains,{search_string}\",\n",
    "        f\"dc.creator,contains,{search_string}\",\n",
    "        # Try changing the search operator\n",
    "        f\"any,exact,{search_string}\",\n",
    "        # Try with quotes\n",
    "        f'any,contains,\"{search_string}\"'\n",
    "    ]\n",
    "\n",
    "    for query_format in alternative_formats:\n",
    "        print(f\"\\nTrying alternative search format: {query_format}\")\n",
    "        try:\n",
    "            # Construct the URL with the alternative format\n",
    "            current_api_key = get_current_api_key()\n",
    "            encoded_query = urllib.parse.quote(query_format)\n",
    "            url = f\"{BASE_URL}?api_key={current_api_key}&query={encoded_query}&bulkSize={bulk_size}&index=0&sort=rank\"\n",
    "\n",
    "            print(f\"Request URL: {url}\")\n",
    "            response = requests.get(url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    result = response.json()\n",
    "                    if isinstance(result, dict) and ('docs' in result or 'records' in result or 'results' in result):\n",
    "                        # Adjust the structure if needed\n",
    "                        if 'records' in result:\n",
    "                            result['docs'] = result['records']\n",
    "                        elif 'results' in result:\n",
    "                            result['docs'] = result['results']\n",
    "\n",
    "                        doc_count = len(result.get('docs', []))\n",
    "                        print(f\"Found {doc_count} documents with alternative format\")\n",
    "                        if doc_count > 0:\n",
    "                            return result\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing alternative response: {e}\")\n",
    "\n",
    "            print(f\"Alternative format did not yield results (status code: {response.status_code})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with alternative search: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def nli_search_to_csv(search_string, output_file=None, bulk_size=30):\n",
    "    \"\"\"\n",
    "    Simple function to search NLI API and save results to CSV.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    search_string : str\n",
    "        The search query to send to the NLI API\n",
    "    output_file : str, optional\n",
    "        Path to save the CSV file. If None, a filename will be generated based on the search string\n",
    "    bulk_size : int, optional\n",
    "        Number of results to retrieve (default: 30)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The path to the saved CSV file, or None if the search failed\n",
    "    \"\"\"\n",
    "    # Generate output filename if not provided\n",
    "    if not output_file:\n",
    "        # Create a safe filename from the search string\n",
    "        safe_filename = re.sub(r'[^\\w\\s-]', '', search_string).strip().replace(' ', '_')\n",
    "        output_file = f\"nli_search_{safe_filename}_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "\n",
    "    print(f\"Starting with API key: {get_current_api_key()[:5]}...\")\n",
    "\n",
    "    # Search the NLI API\n",
    "    result = search_nli(search_string, bulk_size)\n",
    "\n",
    "    # If standard search fails, try alternative formats\n",
    "    if not result:\n",
    "        print(\"\\nNo result returned. Trying alternative search formats...\")\n",
    "        result = try_alternative_search(search_string, bulk_size)\n",
    "    # Check if result is a list (direct docs array) instead of a dictionary with 'docs' key\n",
    "    elif isinstance(result, list):\n",
    "        print(f\"\\nAPI returned a list of {len(result)} documents directly instead of a dictionary\")\n",
    "        # Convert to expected format\n",
    "        result = {'docs': result}\n",
    "    # Check if docs exists in the dictionary result\n",
    "    elif isinstance(result, dict) and not result.get('docs', []):\n",
    "        print(\"\\nAPI returned a dictionary without 'docs' key. Trying alternative search formats...\")\n",
    "        result = try_alternative_search(search_string, bulk_size)\n",
    "\n",
    "    if not result:\n",
    "        print(\"No results found after trying all search methods\")\n",
    "        return None\n",
    "\n",
    "    # Check if we have results\n",
    "    if isinstance(result, dict) and 'docs' in result and result['docs']:\n",
    "        print(f\"Found {len(result['docs'])} results\")\n",
    "        # Convert to list of dictionaries\n",
    "        data = convert_nli_docs_to_dicts(result['docs'])\n",
    "        # Save to CSV\n",
    "        if save_to_csv(data, output_file):\n",
    "            return output_file\n",
    "    elif isinstance(result, list) and result:\n",
    "        print(f\"Found {len(result)} results in list format\")\n",
    "        # Convert to list of dictionaries\n",
    "        data = convert_nli_docs_to_dicts(result)\n",
    "        # Save to CSV\n",
    "        if save_to_csv(data, output_file):\n",
    "            return output_file\n",
    "    else:\n",
    "        print(\"No documents found in the response after all attempts\")\n",
    "\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # STEP 1: CHANGE THIS LINE TO YOUR SEARCH QUERY\n",
    "    search_query = \"990012026630205171\"  # <-- ENTER YOUR SEARCH QUERY HERE\n",
    "\n",
    "    # Optional parameters\n",
    "    results_count = 50  # Number of results to retrieve\n",
    "    csv_filename = None  # Output filename (None for auto-generated)\n",
    "\n",
    "    # Run the search\n",
    "    output_file = nli_search_to_csv(search_query, csv_filename, results_count)\n",
    "\n",
    "    if output_file and os.path.exists(output_file):\n",
    "        print(f\"Successfully saved search results to: {output_file}\")\n",
    "    else:\n",
    "        print(\"Search completed but no results were found or saved.\")"
   ],
   "id": "f6ae89e52b7d1ca3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with API key: mZcuS...\n",
      "Searching for: 990012026630205171\n",
      "Request URL: https://api.nli.org.il/openlibrary/search?api_key=mZcuSKOCiaboR9ctxclfeMACDtXAgoYQL3cdTRWS&query=any,contains,990012026630205171&bulkSize=50&index=0&sort=rank\n",
      "Response status code: 200\n",
      "Response content type: application/json;charset=UTF-8\n",
      "Response contains keys: Not a dictionary\n",
      "Response is not a dictionary. Raw data (first 500 chars):\n",
      "[{\"@id\": \"https://www.nli.org.il/en/books/NNL_ALEPH990012026630205171\", \"http://purl.org/dc/elements/1.1/source\": [{\"@value\": \"The National Library of Israel\"}], \"http://purl.org/dc/elements/1.1/non_standard_date\": [{\"@value\": \"[\\u05ea\\u05e8\\u05e2\\\"\\u05d1]\"}], \"http://purl.org/dc/elements/1.1/identifier\": [{\"@value\": \"NNL_ALEPH71539165820005171\"}], \"http://purl.org/dc/elements/1.1/language\": [{\"@value\": \"heb\"}], \"http://purl.org/dc/elements/1.1/format\": [{\"@value\": \"(13) \\u05e2\\u05de\\u05d5\\u05d3...\n",
      "\n",
      "API returned a list of 2 documents directly instead of a dictionary\n",
      "Found 2 results\n",
      "Data saved to nli_search_990012026630205171_20250401_133324.csv\n",
      "Successfully saved search results to: nli_search_990012026630205171_20250401_133324.csv\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
