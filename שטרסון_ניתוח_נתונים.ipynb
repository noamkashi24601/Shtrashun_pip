{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXM1G9171lxi"
      },
      "source": [
        "# חלוקות חברתיות של קוראים"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYoh2u6k1s25"
      },
      "source": [
        "**גברים מול נשים**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8J3Iu1b14SJ"
      },
      "source": [
        "השתמשתי באינדקס שהוספנו לטבלאות בשביל לחשב ולראות שבכל הטבלאות יש 1586 אנשים שונים (יש לציין שהאינדוקס התעלם מהאפשרות שיש אנשים שונים בעלי אותו השם או אנשים שנכתבו פעם באותיות עבריות ופעם באותיות קיריליות). מתוכם 72 נשים והשאר (1514) גברים. מה שאומר שמתוך כלל השואלים רק 4.54% הן שואלות. אם נסתכל על כמות הפעמים שכל אדם שאל ספר, נראה שמתוך 5145 שאילות רק 100 נעשו על ידי נשים. מה שאומר שמתוך כלל ההשאלות, 1.94% מהספרים נשאלו על ידי נשים. בממוצע נשים שאלו 1.39 ספרים לעומת גברים ששאלו 3.33 ספרים בממוצע.\n",
        "\n",
        "\n",
        "\n",
        "עבור שנת 1902: בכל הנתונים של שנה זו ישנם 622 אנשים שונים. מתוכם 1 נשים והשאר (621) גברים. מה שאומר שמתוך כלל השואלים רק\n",
        "0.16\n",
        " הן שואלות. אם נסתכל על כמות הפעמים שכל אדם שאל ספר, נראה שמתוך 23537 שאילות רק 2 נעשו על ידי נשים. מה שאומר שמתוך כלל ההשאלות,\n",
        "0.01\n",
        " מהספרים נשאלו על ידי נשים. בממוצע נשים שאלו\n",
        "2.00\n",
        " ספרים לעומת גברים ששאלו\n",
        "37.84\n",
        " ספרים בממוצע.\n",
        "\n",
        "עבור שנת 1920: בכל הנתונים של שנה זו ישנם 747 אנשים שונים. מתוכם 29 נשים והשאר (733) גברים. מה שאומר שמתוך כלל השואלים רק\n",
        "3.88\n",
        " הן שואלות. אם נסתכל על כמות הפעמים שכל אדם שאל ספר, נראה שמתוך 14299 שאילות רק 42 נעשו על ידי נשים. מה שאומר שמתוך כלל ההשאלות,\n",
        "0.29\n",
        " מהספרים נשאלו על ידי נשים. בממוצע נשים שאלו\n",
        "1.45\n",
        " ספרים לעומת גברים ששאלו\n",
        "19.45\n",
        " ספרים בממוצע.\n",
        "\n",
        "עבור שנת 1934: בכל הנתונים של שנה זו ישנם 350 אנשים שונים. מתוכם 66 נשים והשאר (284) גברים. מה שאומר שמתוך כלל השואלים רק\n",
        "18.86\n",
        " הן שואלות. אם נסתכל על כמות הפעמים שכל אדם שאל ספר, נראה שמתוך 367 שאילות רק 67 נעשו על ידי נשים. מה שאומר שמתוך כלל ההשאלות,\n",
        "18.26\n",
        " מהספרים נשאלו על ידי נשים. בממוצע נשים שאלו\n",
        "1.02\n",
        " ספרים לעומת גברים ששאלו\n",
        "1.06\n",
        " ספרים בממוצע.\n",
        "\n",
        "עבור שנת 1940: בכל הנתונים של שנה זו ישנם 109 אנשים שונים. מתוכם 2 נשים והשאר (107) גברים. מה שאומר שמתוך כלל השואלים רק\n",
        "1.83\n",
        " הן שואלות. אם נסתכל על כמות הפעמים שכל אדם שאל ספר, נראה שמתוך 208 שאילות רק 2 נעשו על ידי נשים. מה שאומר שמתוך כלל ההשאלות,\n",
        "0.96\n",
        " מהספרים נשאלו על ידי נשים. בממוצע נשים שאלו\n",
        "1.00\n",
        " ספרים לעומת גברים ששאלו\n",
        "1.93\n",
        " ספרים בממוצע."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G458vuhB19PW"
      },
      "source": [
        "צעדים להמשך:\n",
        "האם יש צורך בלחשב את כל הנתונים הנ״ל לפי כל טבלה ולא רק על כל הנתונים?\n",
        "\n",
        "יהיה מעניין לחשב את ההבדלים המגדריים ביחס לסוגה? למשל, כמה נשים שאלו ספרי יהדות או מה החלוקה המגדרית של שאילת ספרי פרוזה.\n",
        "\n",
        "רוצים שאני אייצר גרפים של התוצאות?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7mnrRi2SKZ"
      },
      "source": [
        "**בעלי שמות עבריים / יידשאיים / רוסיים / אחרים**\n",
        "\n",
        "יכול להיות עדות לפרקטיס רישום ולא למקום החברתי של הקוראים."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT8_4LsD2b4T"
      },
      "source": [
        "לפני שאתחיל לעבוד על זה מספר שאלות:\n",
        "\n",
        "איך אנחנו מגדירים שם ביידיש ושם בעברית? חשבתי לנסות לפי שם פרטי, אבל האם לאה או שרה הן שמות בעברית?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3wDwUX12rQI"
      },
      "source": [
        "**שייכות זהותית**\n",
        "\n",
        "נגזרת מרשתות שנלמדות מתוך הספרים"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C0x-Zhi2u0C"
      },
      "source": [
        "אשמח להסבר למה הכוונה"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkEq8RtX28Uk"
      },
      "source": [
        "# רשתות"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pjZXtdn2_Y7"
      },
      "source": [
        "**אנשים שקוראים אותם ספרים**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsCLj2DL3pMd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "import os\n",
        "\n",
        "# --- REQUIRED LIBRARIES ---\n",
        "# To run this script, you first need to install the required libraries.\n",
        "# Open a terminal or command prompt and run the following command:\n",
        "# pip install pandas networkx pyvis networkx[community]\n",
        "from pyvis.network import Network\n",
        "from networkx.algorithms import community\n",
        "\n",
        "def analyze_shared_books(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyzes borrower data to find people who read the same books,\n",
        "    generates CSVs, and creates interactive HTML network graphs\n",
        "    with community detection and enhanced user controls.\n",
        "    \"\"\"\n",
        "    # --- Set a threshold to only show stronger connections ---\n",
        "    MIN_SHARED_BOOKS = 2\n",
        "\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "        df_clean = df.dropna(subset=[\"person's name\", 'id']).copy()\n",
        "        df_clean['id'] = df_clean['id'].astype(int)\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid borrowing records.\")\n",
        "\n",
        "        # --- 2. Group by Book and Create Pairs ---\n",
        "        readers_per_book = df_clean.groupby('id')[\"person's name\"].apply(list)\n",
        "        all_pairs = []\n",
        "        for readers in readers_per_book:\n",
        "            unique_readers = sorted(list(set(readers)))\n",
        "            if len(unique_readers) > 1:\n",
        "                pairs = list(combinations(unique_readers, 2))\n",
        "                all_pairs.extend(pairs)\n",
        "        print(f\"Generated {len(all_pairs)} initial reader pairs.\")\n",
        "\n",
        "        # --- 3. Calculate Weights and Filter ---\n",
        "        if not all_pairs:\n",
        "            print(\"No shared books found. Exiting.\")\n",
        "            return\n",
        "        pair_counts = pd.DataFrame(all_pairs, columns=['Person1', 'Person2']).value_counts().reset_index()\n",
        "        pair_counts.columns = ['Person1', 'Person2', 'Shared_Book_Count']\n",
        "        filtered_pairs = pair_counts[pair_counts['Shared_Book_Count'] >= MIN_SHARED_BOOKS].copy()\n",
        "        print(f\"Filtered pairs: Kept {len(filtered_pairs)} connections with {MIN_SHARED_BOOKS} or more shared books.\")\n",
        "\n",
        "        # --- 4. Generate Connection CSV Files ---\n",
        "        filtered_pairs.to_csv('shared_book_connections_weighted.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Successfully generated weighted CSV file: shared_book_connections_weighted.csv\")\n",
        "\n",
        "        simple_pairs_df = filtered_pairs[['Person1', 'Person2']]\n",
        "        simple_pairs_df.to_csv('shared_book_connections_simple.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Successfully generated simple pairs CSV file: shared_book_connections_simple.csv\")\n",
        "\n",
        "        # --- 5. Create Graph, Detect Communities, and Calculate Layout ---\n",
        "        G = nx.from_pandas_edgelist(filtered_pairs, 'Person1', 'Person2', edge_attr='Shared_Book_Count')\n",
        "\n",
        "        print(\"Performing community detection...\")\n",
        "        communities_generator = community.louvain_communities(G, seed=42)\n",
        "        communities_list = list(communities_generator)\n",
        "\n",
        "        node_community = {}\n",
        "        for i, comm in enumerate(communities_list):\n",
        "            for node in comm:\n",
        "                node_community[node] = i\n",
        "\n",
        "        nx.set_node_attributes(G, node_community, 'community')\n",
        "        community_df = pd.DataFrame(list(node_community.items()), columns=['Person', 'Community_ID'])\n",
        "        community_df.to_csv('reader_communities.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Successfully generated community data CSV: reader_communities.csv\")\n",
        "\n",
        "        # --- 6. Visualize the FULL INTERACTIVE Network with Pyvis ---\n",
        "        print(\"Generating main interactive network graph with all communities...\")\n",
        "        net_full = Network(\n",
        "            height='900px',\n",
        "            width='100%',\n",
        "            bgcolor='#222222',\n",
        "            font_color='white',\n",
        "            notebook=True,\n",
        "            cdn_resources='in_line'\n",
        "        )\n",
        "\n",
        "        # Use from_nx which handles positioning better\n",
        "        net_full.from_nx(G)\n",
        "\n",
        "        # Enhanced node customization\n",
        "        max_degree = max(dict(G.degree()).values()) if G.nodes() else 1\n",
        "        for node in net_full.nodes:\n",
        "            node_name = node['id']\n",
        "            community_id = node_community.get(node_name, 0)\n",
        "            degree = G.degree(node_name)\n",
        "\n",
        "            # Better size scaling\n",
        "            node['size'] = 10 + (degree / max_degree) * 30\n",
        "            node['group'] = community_id\n",
        "            node['title'] = f\"Name: {node_name}<br>Community: {community_id}<br>Connections: {degree}\"\n",
        "            node['font'] = {'size': 12, 'color': 'white'}\n",
        "\n",
        "        # Enhanced edge customization\n",
        "        for edge in net_full.edges:\n",
        "            # Make edge thickness proportional to shared book count\n",
        "            edge_data = G.get_edge_data(edge['from'], edge['to'])\n",
        "            shared_count = edge_data.get('Shared_Book_Count', 1)\n",
        "            edge['width'] = max(1, shared_count * 2)\n",
        "            edge['title'] = f\"Shared books: {shared_count}\"\n",
        "\n",
        "        # Better physics configuration for natural spreading\n",
        "        net_full.repulsion(node_distance=200, central_gravity=0.01, spring_length=200, spring_strength=0.05, damping=0.09)\n",
        "\n",
        "        # Add proper pyvis controls that integrate with the interface\n",
        "        net_full.show_buttons(filter_=['physics', 'interaction', 'layout', 'selection', 'renderer'])\n",
        "\n",
        "        # Configure the network with proper options for built-in controls\n",
        "        net_full.set_edge_smooth(False)  # Better performance with many edges\n",
        "\n",
        "        # Generate the HTML file with built-in pyvis controls\n",
        "        output_interactive_path = 'shared_book_network_interactive.html'\n",
        "        net_full.show(output_interactive_path)\n",
        "\n",
        "        # Add custom JavaScript functionality to work with pyvis controls\n",
        "        try:\n",
        "            with open(output_interactive_path, 'r', encoding='utf-8') as file:\n",
        "                html_content = file.read()\n",
        "\n",
        "            # JavaScript code as a simple string without complex formatting\n",
        "            js_code = \"\"\"\n",
        "<script type=\"text/javascript\">\n",
        "// Custom network analysis functionality\n",
        "var originalNodes = [];\n",
        "var originalEdges = [];\n",
        "var isHighlighting = false;\n",
        "\n",
        "function initCustomControls() {\n",
        "    if (typeof network !== 'undefined' && network.body && network.body.data) {\n",
        "        originalNodes = network.body.data.nodes.get();\n",
        "        originalEdges = network.body.data.edges.get();\n",
        "        addControlPanel();\n",
        "        setupNetworkEvents();\n",
        "        console.log('Custom controls initialized!');\n",
        "    } else {\n",
        "        setTimeout(initCustomControls, 500);\n",
        "    }\n",
        "}\n",
        "\n",
        "function addControlPanel() {\n",
        "    var panel = document.createElement('div');\n",
        "    panel.id = 'customControls';\n",
        "    panel.style.cssText = 'position:fixed;top:150px;left:10px;background:white;border:1px solid #ccc;border-radius:8px;padding:15px;z-index:1000;font-family:Arial;box-shadow:0 2px 10px rgba(0,0,0,0.1);max-width:280px;';\n",
        "\n",
        "    panel.innerHTML = '<h3 style=\"margin:0 0 15px 0;color:#333;\">📊 Network Analysis</h3>' +\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">🔍 Search:</label>' +\n",
        "    '<input type=\"text\" id=\"searchBox\" placeholder=\"Enter name...\" style=\"width:150px;padding:5px;border:1px solid #ccc;border-radius:4px;\">' +\n",
        "    '<button onclick=\"searchNode()\" style=\"padding:5px 10px;margin-left:5px;background:#4CAF50;color:white;border:none;border-radius:4px;cursor:pointer;\">Go</button></div>' +\n",
        "\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">📈 Min Connections:</label>' +\n",
        "    '<input type=\"range\" id=\"connectionSlider\" min=\"1\" max=\"20\" value=\"1\" style=\"width:180px;\" onchange=\"filterConnections(this.value)\">' +\n",
        "    '<span id=\"connectionValue\" style=\"margin-left:10px;\">1</span></div>' +\n",
        "\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">🏘️ Community:</label>' +\n",
        "    '<select id=\"communitySelect\" onchange=\"filterCommunity(this.value)\" style=\"width:150px;padding:5px;border:1px solid #ccc;border-radius:4px;\">' +\n",
        "    '<option value=\"all\">All Communities</option></select></div>' +\n",
        "\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">🔄 Node Spacing:</label>' +\n",
        "    '<input type=\"range\" id=\"spacingSlider\" min=\"0\" max=\"2000\" value=\"200\" style=\"width:180px;\" onchange=\"adjustSpacing(this.value)\">' +\n",
        "    '<span id=\"spacingValue\" style=\"margin-left:10px;\">200</span></div>' +\n",
        "\n",
        "    '<div style=\"margin:15px 0;\"><button onclick=\"resetAll()\" style=\"padding:8px 15px;margin-right:5px;background:#2196F3;color:white;border:none;border-radius:4px;cursor:pointer;\">🔄 Reset</button>' +\n",
        "    '<button onclick=\"stabilize()\" style=\"padding:8px 15px;background:#FF9800;color:white;border:none;border-radius:4px;cursor:pointer;\">⚡ Stabilize</button></div>' +\n",
        "\n",
        "    '<div id=\"statusPanel\" style=\"margin-top:15px;padding:10px;background:#f5f5f5;border-radius:4px;font-size:12px;\">' +\n",
        "    '<strong>💡 Instructions:</strong><br>• Click nodes to highlight connections<br>• Double-click to focus on a node<br>• Use sliders to filter and adjust layout</div>';\n",
        "\n",
        "    document.body.appendChild(panel);\n",
        "    populateCommunities();\n",
        "}\n",
        "\n",
        "function populateCommunities() {\n",
        "    var communities = new Set();\n",
        "    originalNodes.forEach(function(node) {\n",
        "        if (node.group !== undefined) {\n",
        "            communities.add(node.group);\n",
        "        }\n",
        "    });\n",
        "\n",
        "    var select = document.getElementById('communitySelect');\n",
        "    Array.from(communities).sort(function(a,b){return a-b;}).forEach(function(community) {\n",
        "        var option = document.createElement('option');\n",
        "        option.value = community;\n",
        "        option.textContent = 'Community ' + community;\n",
        "        select.appendChild(option);\n",
        "    });\n",
        "}\n",
        "\n",
        "function setupNetworkEvents() {\n",
        "    network.on('click', function(params) {\n",
        "        if (params.nodes.length > 0) {\n",
        "            highlightNode(params.nodes[0]);\n",
        "        } else if (isHighlighting) {\n",
        "            resetHighlight();\n",
        "        }\n",
        "    });\n",
        "\n",
        "    network.on('doubleClick', function(params) {\n",
        "        if (params.nodes.length > 0) {\n",
        "            focusNode(params.nodes[0]);\n",
        "        }\n",
        "    });\n",
        "}\n",
        "\n",
        "function searchNode() {\n",
        "    var searchTerm = document.getElementById('searchBox').value.toLowerCase().trim();\n",
        "    if (!searchTerm) {\n",
        "        alert('Please enter a search term');\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    var node = originalNodes.find(function(n) {\n",
        "        return (n.label && n.label.toLowerCase().includes(searchTerm)) ||\n",
        "               (n.id && n.id.toString().toLowerCase().includes(searchTerm));\n",
        "    });\n",
        "\n",
        "    if (node) {\n",
        "        focusNode(node.id);\n",
        "        highlightNode(node.id);\n",
        "        updateStatus('Found: ' + (node.label || node.id));\n",
        "    } else {\n",
        "        alert('Node \"' + searchTerm + '\" not found');\n",
        "    }\n",
        "}\n",
        "\n",
        "function focusNode(nodeId) {\n",
        "    network.focus(nodeId, {\n",
        "        scale: 1.5,\n",
        "        animation: {\n",
        "            duration: 1000,\n",
        "            easingFunction: 'easeInOutQuad'\n",
        "        }\n",
        "    });\n",
        "}\n",
        "\n",
        "function highlightNode(nodeId) {\n",
        "    var connectedNodes = network.getConnectedNodes(nodeId);\n",
        "    var connectedEdges = network.getConnectedEdges(nodeId);\n",
        "\n",
        "    var updatedNodes = originalNodes.map(function(node) {\n",
        "        var newNode = Object.assign({}, node);\n",
        "        if (node.id === nodeId) {\n",
        "            newNode.color = {background: '#ff0000', border: '#cc0000'};\n",
        "            newNode.borderWidth = 4;\n",
        "            newNode.size = (newNode.size || 15) * 1.5;\n",
        "        } else if (connectedNodes.includes(node.id)) {\n",
        "            newNode.color = {background: '#ffa500', border: '#ff8c00'};\n",
        "            newNode.borderWidth = 2;\n",
        "            newNode.size = (newNode.size || 15) * 1.2;\n",
        "        } else {\n",
        "            newNode.color = {background: '#d3d3d3', border: '#a9a9a9'};\n",
        "            newNode.opacity = 0.3;\n",
        "        }\n",
        "        return newNode;\n",
        "    });\n",
        "\n",
        "    var updatedEdges = originalEdges.map(function(edge) {\n",
        "        var newEdge = Object.assign({}, edge);\n",
        "        if (connectedEdges.includes(edge.id)) {\n",
        "            newEdge.color = {color: '#ff0000'};\n",
        "            newEdge.width = (newEdge.width || 1) * 3;\n",
        "        } else {\n",
        "            newEdge.opacity = 0.1;\n",
        "        }\n",
        "        return newEdge;\n",
        "    });\n",
        "\n",
        "    network.body.data.nodes.update(updatedNodes);\n",
        "    network.body.data.edges.update(updatedEdges);\n",
        "    isHighlighting = true;\n",
        "\n",
        "    var node = originalNodes.find(function(n) { return n.id === nodeId; });\n",
        "    if (node) {\n",
        "        updateStatus('Highlighting: ' + (node.label || node.id) + ' (' + connectedNodes.length + ' connections)');\n",
        "    }\n",
        "}\n",
        "\n",
        "function resetHighlight() {\n",
        "    network.body.data.nodes.update(originalNodes);\n",
        "    network.body.data.edges.update(originalEdges);\n",
        "    isHighlighting = false;\n",
        "    updateStatus('Highlight reset');\n",
        "}\n",
        "\n",
        "function filterConnections(minConnections) {\n",
        "    document.getElementById('connectionValue').textContent = minConnections;\n",
        "\n",
        "    var filteredNodes = originalNodes.filter(function(node) {\n",
        "        var connections = network.getConnectedNodes(node.id);\n",
        "        return connections.length >= parseInt(minConnections);\n",
        "    });\n",
        "\n",
        "    var filteredNodeIds = new Set(filteredNodes.map(function(n) { return n.id; }));\n",
        "    var filteredEdges = originalEdges.filter(function(edge) {\n",
        "        return filteredNodeIds.has(edge.from) && filteredNodeIds.has(edge.to);\n",
        "    });\n",
        "\n",
        "    // Clear and add filtered data\n",
        "    network.body.data.nodes.clear();\n",
        "    network.body.data.edges.clear();\n",
        "    network.body.data.nodes.add(filteredNodes);\n",
        "    network.body.data.edges.add(filteredEdges);\n",
        "\n",
        "    // Fit the view to the filtered nodes\n",
        "    network.fit();\n",
        "\n",
        "    updateStatus('Showing ' + filteredNodes.length + ' nodes with ' + minConnections + '+ connections');\n",
        "}\n",
        "\n",
        "function filterCommunity(communityId) {\n",
        "    if (communityId === 'all') {\n",
        "        // Show all nodes and edges\n",
        "        network.body.data.nodes.update(originalNodes);\n",
        "        network.body.data.edges.update(originalEdges);\n",
        "        updateStatus('Showing all communities');\n",
        "    } else {\n",
        "        // Filter to show only nodes from the selected community\n",
        "        var filteredNodes = originalNodes.filter(function(node) {\n",
        "            return node.group == communityId;\n",
        "        });\n",
        "\n",
        "        // Only show edges between nodes in the selected community\n",
        "        var filteredNodeIds = new Set(filteredNodes.map(function(n) { return n.id; }));\n",
        "        var filteredEdges = originalEdges.filter(function(edge) {\n",
        "            return filteredNodeIds.has(edge.from) && filteredNodeIds.has(edge.to);\n",
        "        });\n",
        "\n",
        "        // Clear the network first, then add only the filtered nodes and edges\n",
        "        network.body.data.nodes.clear();\n",
        "        network.body.data.edges.clear();\n",
        "\n",
        "        // Add the filtered nodes and edges\n",
        "        network.body.data.nodes.add(filteredNodes);\n",
        "        network.body.data.edges.add(filteredEdges);\n",
        "\n",
        "        // Fit the view to the filtered community\n",
        "        network.fit();\n",
        "\n",
        "        updateStatus('Showing Community ' + communityId + ' (' + filteredNodes.length + ' nodes)');\n",
        "    }\n",
        "}\n",
        "\n",
        "function adjustSpacing(spacing) {\n",
        "    document.getElementById('spacingValue').textContent = spacing;\n",
        "\n",
        "    var options = {\n",
        "        physics: {\n",
        "            enabled: true,\n",
        "            repulsion: {\n",
        "                nodeDistance: parseInt(spacing),\n",
        "                centralGravity: 0.01,\n",
        "                springLength: parseInt(spacing) * 0.8,\n",
        "                springConstant: 0.05,\n",
        "                damping: 0.09\n",
        "            }\n",
        "        }\n",
        "    };\n",
        "\n",
        "    network.setOptions(options);\n",
        "    updateStatus('Node spacing adjusted to ' + spacing);\n",
        "}\n",
        "\n",
        "function resetAll() {\n",
        "    // Clear and restore all nodes and edges\n",
        "    network.body.data.nodes.clear();\n",
        "    network.body.data.edges.clear();\n",
        "    network.body.data.nodes.add(originalNodes);\n",
        "    network.body.data.edges.add(originalEdges);\n",
        "    network.fit();\n",
        "\n",
        "    document.getElementById('connectionSlider').value = 1;\n",
        "    document.getElementById('connectionValue').textContent = '1';\n",
        "    document.getElementById('communitySelect').value = 'all';\n",
        "    document.getElementById('searchBox').value = '';\n",
        "    document.getElementById('spacingSlider').value = 200;\n",
        "    document.getElementById('spacingValue').textContent = '200';\n",
        "\n",
        "    isHighlighting = false;\n",
        "    updateStatus('All filters reset');\n",
        "}\n",
        "\n",
        "function stabilize() {\n",
        "    network.stabilize();\n",
        "    updateStatus('Network stabilized');\n",
        "}\n",
        "\n",
        "function updateStatus(message) {\n",
        "    var panel = document.getElementById('statusPanel');\n",
        "    if (panel) {\n",
        "        panel.innerHTML = '<strong>💡 Status:</strong><br>' + message;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Initialize when page loads\n",
        "setTimeout(initCustomControls, 1000);\n",
        "</script>\n",
        "</body>\"\"\"\n",
        "\n",
        "            # Insert before closing body tag\n",
        "            html_content = html_content.replace('</body>', js_code)\n",
        "\n",
        "            with open(output_interactive_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(html_content)\n",
        "\n",
        "            print(f\"Successfully generated enhanced interactive HTML graph: {output_interactive_path}\")\n",
        "            print(\"🎉 Custom controls integrated with pyvis interface!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not add custom controls: {e}\")\n",
        "            print(f\"Basic interactive graph available at: {output_interactive_path}\")\n",
        "\n",
        "        # --- 7. Generate a separate graph for each community ---\n",
        "        print(\"\\nGenerating individual graphs for each community...\")\n",
        "        if not os.path.exists('communities'):\n",
        "            os.makedirs('communities')\n",
        "\n",
        "        for i, comm_nodes in enumerate(communities_list):\n",
        "            if not comm_nodes: continue\n",
        "            subgraph = G.subgraph(comm_nodes)\n",
        "\n",
        "            net_comm = Network(\n",
        "                height='800px',\n",
        "                width='100%',\n",
        "                bgcolor='#333333',\n",
        "                font_color='white',\n",
        "                notebook=True,\n",
        "                cdn_resources='in_line',\n",
        "                heading=f'Community {i}'\n",
        "            )\n",
        "            net_comm.from_nx(subgraph)\n",
        "\n",
        "            # Enhanced community graph styling\n",
        "            for node in net_comm.nodes:\n",
        "                node['title'] = f\"Name: {node['label']}<br>Connections: {subgraph.degree(node['id'])}\"\n",
        "                node['size'] = 15 + subgraph.degree(node['id']) * 3\n",
        "                node['font'] = {'size': 14, 'color': 'white'}\n",
        "\n",
        "            # Better physics for smaller community graphs\n",
        "            net_comm.repulsion(node_distance=120, central_gravity=0.01, spring_length=150, spring_strength=0.08, damping=0.2)\n",
        "\n",
        "            net_comm.show_buttons(filter_=['physics'])\n",
        "\n",
        "            comm_file_path = f'communities/community_{i}.html'\n",
        "            net_comm.show(comm_file_path)\n",
        "            print(f\"  - Created community graph: {comm_file_path}\")\n",
        "\n",
        "        print(\"\\nAnalysis complete. Open the HTML files in your browser to explore.\")\n",
        "        print(\"The graph will initially spread out naturally. You can:\")\n",
        "        print(\"- Use the physics controls to adjust the layout\")\n",
        "        print(\"- Drag nodes to reposition them\")\n",
        "        print(\"- Use mouse wheel to zoom\")\n",
        "        print(\"- Click and drag to pan around\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Run the analysis\n",
        "analyze_shared_books()\n",
        "\n",
        "def create_daily_borrower_list(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Loads borrower data, cleans it, and creates a simple CSV file\n",
        "    listing all borrowers with their ID, sorted chronologically by their visit date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Drop rows where person's name, date, or ID is missing, as they are essential.\n",
        "        df_clean = df.dropna(subset=[\"person's name\", 'date', 'index_name']).copy()\n",
        "\n",
        "        # Convert the 'date' column to a standardized datetime format.\n",
        "        # `errors='coerce'` will handle various formats and turn any un-parseable\n",
        "        # dates into 'NaT' (Not a Time), which we can then remove.\n",
        "        df_clean['date_parsed'] = pd.to_datetime(df_clean['date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "        # Convert the 'index_name' to a clean integer format.\n",
        "        df_clean['index_name'] = df_clean['index_name'].astype(int)\n",
        "\n",
        "        # Remove any rows where the date could not be successfully parsed.\n",
        "        df_clean = df_clean.dropna(subset=['date_parsed'])\n",
        "\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid visit records.\")\n",
        "\n",
        "        # --- 2. Sort by Date ---\n",
        "        # Sort the DataFrame chronologically based on the new 'date_parsed' column.\n",
        "        sorted_df = df_clean.sort_values(by='date_parsed')\n",
        "\n",
        "        # --- 3. Select Relevant Columns ---\n",
        "        # Create a final, clean list with the date, person's ID, and person's name.\n",
        "        output_df = sorted_df[['date_parsed', 'index_name', \"person's name\"]]\n",
        "\n",
        "        # --- 4. Save to CSV ---\n",
        "        output_csv_path = 'daily_borrower_list.csv'\n",
        "        output_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nSuccessfully created the file: {output_csv_path}\")\n",
        "        print(\"This file contains a list of all borrowers with their IDs, ordered by date.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Run the analysis\n",
        "create_daily_borrower_list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE7vgCS8cVUD"
      },
      "source": [
        "הקוד לעיל התמקד בניתוח רשתות חברתיות בין השואלים שנמצאים אצלנו בדאטא, בדגש על אנשים ששאלו את אותם ספרים.\n",
        "\n",
        "הקבצים שהוא יצר הם: -\n",
        "\n",
        "\n",
        "[רשימה של כל זוג שואלים עם מספר הספרים המשותפים שהם שאלו](https://docs.google.com/spreadsheets/d/1l-pr-9JMPwCErBZJE1wSDQPbZWZpTcHMgSPjzYwPJdU/edit?usp=sharing)\n",
        "\n",
        "[רשימה של השואלים מסודרת לפי תאריכים](https://docs.google.com/spreadsheets/d/1kyJ-I3jG0KqPFgz_6VGx_vCKRWz4s8DG5uzORHebeYw/edit?usp=sharing)\n",
        "\n",
        "[רשימה של השואלים מחולקים לקהילות](https://docs.google.com/spreadsheets/d/1RCfHlzG3QziYMq7E43GBKHrbIC_R8Ed3we8FP8GidjA/edit?usp=sharing)\n",
        "\n",
        "הכוונה בקהילות היא אנשים שיותר מקושרים אחד לשני  - חולקים את אותם ספרים, מאשר לשאר האנשים בדאטא.\n",
        "\n",
        "[קובץ אינטראקטיבי לניתוח גרף קשרים של השואלים](https://drive.google.com/file/d/1NQCWZhGMjaz-Zs_RmqP3V7Rs9ToTGtCv/view?usp=drive_link)\n",
        "\n",
        "בשביל להשתמש בקובץ זה, צריך לפתוח אותו בדרייב ולהוריד אותו למחשב. לאחר מכן הוא יפתח בדפדפן שלכם.\n",
        "בקובץ אפשר להגדיל ולהקטין את המרחקים בין הצמתים, בשביל לראות יותר טוב את החלקים המעניינים בגרף, לבחור שואל בו אתם רוצים להתמקד, לבחור קהילה אותה אתם רוצים לנתח וכמות קשרים מינימאלית.\n",
        "אם תצביעו על צומת עם העכבר, יופיע לכם פופ אפ עם שם השואל, מספר הקשרים ומאיזה קהילה הוא.\n",
        "\n",
        "הקובץ קצת כבד, אז ייתכן שיקח קצת זמן עד שהוא יופיע ולפעמים פעולות שתעשו יגרמו לגרף לרעוד קצת ויקח לו כמה שניות להתייצב."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4p4xszItcFk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def track_book_trends(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyzes borrower data to track the popularity of each book over all years.\n",
        "    It creates a pivot table CSV showing borrow counts for each book per year.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the borrower data CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Drop rows where essential information is missing.\n",
        "        df_clean = df.dropna(subset=['date', 'book name', 'id']).copy()\n",
        "\n",
        "        # Convert 'date' to datetime objects, coercing errors.\n",
        "        df_clean['date_parsed'] = pd.to_datetime(df_clean['date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "        # Ensure book ID is a clean integer.\n",
        "        df_clean['id'] = df_clean['id'].astype(int)\n",
        "\n",
        "        # Drop rows where date parsing failed.\n",
        "        df_clean = df_clean.dropna(subset=['date_parsed'])\n",
        "\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid borrowing records.\")\n",
        "\n",
        "        # --- 2. Extract Year ---\n",
        "        df_clean['year'] = df_clean['date_parsed'].dt.year\n",
        "\n",
        "        # --- 3. Calculate Annual Popularity for All Books ---\n",
        "        # Group by year, book id, and book name, then count the size of each group.\n",
        "        yearly_counts = df_clean.groupby(['year', 'id', 'book name']).size().reset_index(name='borrow_count')\n",
        "\n",
        "        print(\"Calculated borrow counts for all books across all years.\")\n",
        "\n",
        "        # --- 4. Create Pivot Table ---\n",
        "        # This will transform the data so that:\n",
        "        # - Each book (id and name) is a row.\n",
        "        # - Each year is a column.\n",
        "        # - The values are the number of times the book was borrowed in that year.\n",
        "        # 'fill_value=0' ensures that if a book wasn't borrowed in a year, it shows a 0 instead of being blank.\n",
        "        trends_pivot = yearly_counts.pivot_table(\n",
        "            index=['id', 'book name'],\n",
        "            columns='year',\n",
        "            values='borrow_count',\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        print(\"Created a pivot table to show trends over time.\")\n",
        "\n",
        "        # --- 5. Save to CSV ---\n",
        "        output_csv_path = 'book_trends_over_time.csv'\n",
        "        trends_pivot.to_csv(output_csv_path, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nSuccessfully created the file: {output_csv_path}\")\n",
        "        print(\"This file contains the borrow count for every book for every year.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Run the trend analysis.\n",
        "track_book_trends()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_PFRcTvO7o"
      },
      "source": [
        "הקוד לעיל יצר טבלת פיבוט של [כמות הפעמים ששאלו ספר לאורך השנים](https://docs.google.com/spreadsheets/d/1H9VmIlKmtE74x93art9RbmM3TsnWvUa5euiUIe90Ido/edit?usp=sharing)\n",
        "\n",
        "\n",
        "שימו לב שיש הבדלים בכנות הנתונים שיש לנו בכל שנה, מה שיכול ליצור הטיה בבדיקה זו"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKIgp7gj0a4w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_person_folder_matrix(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyzes borrower data to create a matrix showing which people appear\n",
        "    in which folders.\n",
        "\n",
        "    The output CSV will have:\n",
        "    - A row for each unique person.\n",
        "    - A column for the person's ID and name.\n",
        "    - A column counting the total number of unique folders they appear in.\n",
        "    - A column for every unique folder, with a 1 if the person is present, 0 otherwise.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the borrower data CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Drop rows where the essential columns are missing.\n",
        "        # NOTE: Only check for name and folder - index_name is only available for some folders\n",
        "        df_clean = df.dropna(subset=[\"person's name\", 'Folder']).copy()\n",
        "\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid records with folder information.\")\n",
        "\n",
        "        # --- 2. Create a consistent person identifier ---\n",
        "        # Since index_name is only available for Vol_1_1902, we'll create our own unique identifier\n",
        "        # based on the person's name and handle the existing index_name where available\n",
        "\n",
        "        # First, let's see which records have index_name\n",
        "        has_index = df_clean['index_name'].notna()\n",
        "        print(f\"Records with index_name: {has_index.sum()}\")\n",
        "        print(f\"Records without index_name: {(~has_index).sum()}\")\n",
        "\n",
        "        # Create a unique person identifier\n",
        "        # For records with index_name, use it. For others, create a new sequential ID\n",
        "        df_clean = df_clean.copy()\n",
        "\n",
        "        # Get the maximum existing index_name to continue numbering from there\n",
        "        max_existing_index = df_clean['index_name'].max() if df_clean['index_name'].notna().any() else -1\n",
        "        max_existing_index = int(max_existing_index) if pd.notna(max_existing_index) else -1\n",
        "\n",
        "        # Create a mapping for people without index_name\n",
        "        people_without_index = df_clean[df_clean['index_name'].isna()][\"person's name\"].unique()\n",
        "\n",
        "        # Assign new index numbers to people without existing index_name\n",
        "        new_index_mapping = {}\n",
        "        current_index = max_existing_index + 1\n",
        "\n",
        "        for person in people_without_index:\n",
        "            new_index_mapping[person] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "        # Fill in the missing index_name values\n",
        "        mask = df_clean['index_name'].isna()\n",
        "        df_clean.loc[mask, 'index_name'] = df_clean.loc[mask, \"person's name\"].map(new_index_mapping)\n",
        "\n",
        "        # Ensure index_name is integer\n",
        "        df_clean['index_name'] = df_clean['index_name'].astype(int)\n",
        "\n",
        "        print(f\"Assigned unique IDs to all people. Total unique people: {df_clean['index_name'].nunique()}\")\n",
        "\n",
        "        # --- 3. Handle Multiple Folders in a Single Cell (Robustly) ---\n",
        "        # Convert 'Folder' column to string type to handle any non-string values safely.\n",
        "        df_clean['Folder'] = df_clean['Folder'].astype(str)\n",
        "\n",
        "        # Split the 'Folder' string by a comma into a list of folder names.\n",
        "        df_clean['Folder_list'] = df_clean['Folder'].str.split(r',')\n",
        "\n",
        "        # Explode the DataFrame on the new list column. Each folder for a person now gets its own row.\n",
        "        df_exploded = df_clean.explode('Folder_list')\n",
        "\n",
        "        # Clean up each individual folder name after exploding.\n",
        "        df_exploded['Folder'] = df_exploded['Folder_list'].str.strip()\n",
        "        df_exploded['Folder'] = df_exploded['Folder'].str.strip('\\'\"')\n",
        "\n",
        "        # Remove any rows that might be empty after stripping\n",
        "        df_exploded.dropna(subset=['Folder'], inplace=True)\n",
        "        df_exploded = df_exploded[df_exploded['Folder'] != '']\n",
        "\n",
        "        print(\"Processed records to handle multiple folders per entry and cleaned folder names.\")\n",
        "\n",
        "        # Print folder distribution\n",
        "        folder_counts = df_exploded['Folder'].value_counts()\n",
        "        print(f\"\\nFolder distribution:\")\n",
        "        for folder, count in folder_counts.items():\n",
        "            print(f\"  {folder}: {count} records\")\n",
        "\n",
        "        # --- 4. Create the Person-Folder Matrix ---\n",
        "        # We use crosstab on the exploded and cleaned data to create a frequency table.\n",
        "        person_folder_crosstab = pd.crosstab(\n",
        "            df_exploded['index_name'],\n",
        "            df_exploded['Folder']\n",
        "        )\n",
        "\n",
        "        # Convert the counts to a binary format (1 for present, 0 for absent).\n",
        "        person_folder_matrix = (person_folder_crosstab > 0).astype(int)\n",
        "\n",
        "        print(f\"\\nCreated a matrix for the following folders: {person_folder_matrix.columns.tolist()}\")\n",
        "\n",
        "        # --- 5. Calculate Folder Count ---\n",
        "        # Sum across the rows to get the total number of unique folders for each person.\n",
        "        person_folder_matrix['folder_count'] = person_folder_matrix.sum(axis=1)\n",
        "\n",
        "        # --- 6. Add Person's Name ---\n",
        "        # Create a unique mapping of ID to name from the original cleaned data.\n",
        "        name_map = df_clean[['index_name', \"person's name\"]].drop_duplicates().set_index('index_name')\n",
        "\n",
        "        # Join the names to our matrix.\n",
        "        final_matrix = name_map.join(person_folder_matrix, on='index_name')\n",
        "\n",
        "        # Reset the index to bring 'index_name' back as a column.\n",
        "        final_matrix.reset_index(inplace=True)\n",
        "\n",
        "        # --- 7. Format and Save the Final CSV ---\n",
        "        # Reorder columns to put the count first for better readability.\n",
        "        folder_columns = sorted([col for col in person_folder_crosstab.columns])\n",
        "        final_columns = ['index_name', \"person's name\", 'folder_count'] + folder_columns\n",
        "        final_matrix = final_matrix[final_columns]\n",
        "\n",
        "        # Sort by the folder count to see the most active people at the top.\n",
        "        final_matrix.sort_values(by='folder_count', ascending=False, inplace=True)\n",
        "\n",
        "        output_csv_path = 'person_folder_matrix.csv'\n",
        "        final_matrix.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nSuccessfully created the file: {output_csv_path}\")\n",
        "        print(\"This file contains the matrix of people and their folder appearances.\")\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(f\"\\nSummary Statistics:\")\n",
        "        print(f\"Total unique people: {len(final_matrix)}\")\n",
        "        print(f\"Total unique folders: {len(folder_columns)}\")\n",
        "        print(f\"People appearing in multiple folders: {(final_matrix['folder_count'] > 1).sum()}\")\n",
        "        print(f\"Maximum folders per person: {final_matrix['folder_count'].max()}\")\n",
        "\n",
        "        return final_matrix\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    result = create_person_folder_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZ0SAREHbRc"
      },
      "source": [
        "בקוד הזה יצרתי [טבלה שמראה כמה שואלים מופיעים בכמה טבלאות](https://docs.google.com/spreadsheets/d/15ebSK2RHUX1BAYgAh_V8ScDO383K7aaAejLw7Xepe0A/edit?usp=sharing)\n",
        "\n",
        "היא לא כל כך מעניינת כי נראה שיש רק שואל אחד שנמצא ביותר מטבלה אחת ומדובר בברעגער ראובן - אינדקס 1560"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdVBu40BkKt"
      },
      "source": [
        "# דאשבורד"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kV8KtBJaBnl8",
        "outputId": "ac01fe85-cda5-46cf-e5c7-614e75e8836f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 12:52:50--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 35.71.179.82, 13.248.244.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  17.3MB/s    in 0.8s    \n",
            "\n",
            "2025-07-28 12:52:51 (17.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Please enter the full path to your CSV file:\n",
            "   Example: /content/borrowers_data.csv\n",
            "   Or simply: borrowers_data.csv (if file is in same directory)\n",
            "\n",
            "File path: /content/borrowers_data.csv\n",
            "File found: /content/borrowers_data.csv\n",
            "\n",
            "Creating dashboard with file: /content/borrowers_data.csv\n",
            "Loading data and building networks...\n",
            "\n",
            "Dashboard created successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic tunnel setup failed.\n",
            "Please run this command in a NEW Colab cell:\n",
            "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "!unzip ngrok-stable-linux-amd64.zip\n",
            "!./ngrok http 8050\n",
            "Then copy the https URL that appears\n",
            "Local access: http://localhost:8050\n",
            "\n",
            "Starting dashboard server...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install required packages (run this cell first in Google Colab)\n",
        "!pip install plotly dash dash-bootstrap-components networkx pandas numpy pyngrok -q\n",
        "!pip install dash_bootstrap_components -q\n",
        "!pip install dash -q\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output, State, callback_context, dash_table\n",
        "import dash_bootstrap_components as dbc\n",
        "from networkx.algorithms import community\n",
        "import warnings\n",
        "import time\n",
        "from functools import lru_cache\n",
        "import json\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class OptimizedLibraryNetworkAnalyzer:\n",
        "    \"\"\"\n",
        "    Optimized version with all original features restored and performance improvements\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        self.df = pd.read_csv(filepath)\n",
        "        self.prepare_data()\n",
        "        self.create_networks()\n",
        "        self._layout_cache = {}\n",
        "        self._subgraph_cache = {}\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Clean and prepare data with optimizations\"\"\"\n",
        "        # Clean the data\n",
        "        self.df_clean = self.df.dropna(subset=[\"person's name\", 'id', 'index_name']).copy()\n",
        "\n",
        "        # Optimize data types\n",
        "        self.df_clean['id'] = self.df_clean['id'].apply(lambda x: f\"{float(x):.0f}\" if pd.notna(x) else str(x))\n",
        "        self.df_clean['index_name'] = self.df_clean['index_name'].astype(int)\n",
        "\n",
        "        # Process categorical data\n",
        "        self.df_clean['gender_processed'] = self.df_clean['gender'].apply(\n",
        "            lambda x: 'Female' if pd.notna(x) and str(x).strip().lower() == 'f' else 'Male'\n",
        "        )\n",
        "\n",
        "        # Fill NaN values efficiently and process ebooks\n",
        "        self.df_clean['language_processed'] = self.df_clean['language_nli'].fillna('Unknown')\n",
        "        self.df_clean['type_processed'] = self.df_clean['type'].fillna('Unknown')\n",
        "        # Convert ebooks/ebook variations to Unknown (case insensitive, handles variations)\n",
        "        self.df_clean['type_processed'] = self.df_clean['type_processed'].apply(\n",
        "            lambda x: 'Unknown' if str(x).lower().strip() in ['ebooks', 'ebook', 'e-book', 'e-books'] else x\n",
        "        )\n",
        "        self.df_clean['subject_processed'] = self.df_clean['subject'].fillna('Unknown')\n",
        "\n",
        "        # Cache unique values\n",
        "        self.unique_languages = sorted(self.df_clean['language_processed'].unique())\n",
        "        self.unique_types = sorted(self.df_clean['type_processed'].unique())\n",
        "        self.unique_subjects = sorted(self.df_clean['subject_processed'].unique())\n",
        "        self.unique_folders = sorted(self.df_clean['Folder'].unique())\n",
        "\n",
        "        # Pre-compute person and book stats\n",
        "        self._compute_basic_stats()\n",
        "\n",
        "    def _compute_basic_stats(self):\n",
        "        \"\"\"Pre-compute basic statistics for faster filtering\"\"\"\n",
        "        # Books per person\n",
        "        self.books_per_person = self.df_clean.groupby('index_name')['id'].nunique().to_dict()\n",
        "\n",
        "        # Readers per book\n",
        "        self.readers_per_book = self.df_clean.groupby('id')['index_name'].nunique().to_dict()\n",
        "\n",
        "        # Person metadata\n",
        "        person_meta = self.df_clean.groupby('index_name').first()\n",
        "        self.person_metadata = {\n",
        "            idx: {\n",
        "                'name': row[\"person's name\"],\n",
        "                'gender': row['gender_processed'],\n",
        "                'folder': row['Folder']\n",
        "            } for idx, row in person_meta.iterrows()\n",
        "        }\n",
        "\n",
        "        # Book metadata\n",
        "        book_meta = self.df_clean.groupby('id').first()\n",
        "        self.book_metadata = {\n",
        "            book_id: {\n",
        "                'name': row['book name'],\n",
        "                'folder': row['Folder'],\n",
        "                'language': row['language_processed'],\n",
        "                'type': row['type_processed'],\n",
        "                'subject': row['subject_processed']\n",
        "            } for book_id, row in book_meta.iterrows()\n",
        "        }\n",
        "\n",
        "    def create_networks(self):\n",
        "        \"\"\"Create networks with optimization\"\"\"\n",
        "        self.create_person_network()\n",
        "        self.create_book_network()\n",
        "        self.detect_communities()\n",
        "        self.calculate_network_metrics()\n",
        "\n",
        "        # Calculate maximum nodes for dynamic limits\n",
        "        self.max_people_nodes = len([n for n in self.person_graph.nodes() if self.person_graph.degree(n) > 0])\n",
        "        self.max_book_nodes = len([n for n in self.book_graph.nodes() if self.book_graph.degree(n) > 0])\n",
        "\n",
        "        # Calculate metric ranges for dynamic sliders\n",
        "        self._calculate_metric_ranges()\n",
        "\n",
        "    def create_person_network(self):\n",
        "        \"\"\"Optimized person network creation\"\"\"\n",
        "        # Group by book to find shared readers\n",
        "        readers_per_book = self.df_clean.groupby('id')['index_name'].apply(list)\n",
        "\n",
        "        person_connections = []\n",
        "        shared_books_info = {}\n",
        "\n",
        "        for book_id, readers in readers_per_book.items():\n",
        "            unique_readers = list(set(readers))\n",
        "            if len(unique_readers) > 1:\n",
        "                pairs = list(combinations(unique_readers, 2))\n",
        "                person_connections.extend(pairs)\n",
        "\n",
        "                # Store shared book info\n",
        "                book_name = self.book_metadata[book_id]['name']\n",
        "                for pair in pairs:\n",
        "                    pair_key = tuple(sorted(pair))\n",
        "                    if pair_key not in shared_books_info:\n",
        "                        shared_books_info[pair_key] = []\n",
        "                    shared_books_info[pair_key].append((book_name, book_id))\n",
        "\n",
        "        if person_connections:\n",
        "            # Create weighted edges\n",
        "            person_df = pd.DataFrame(person_connections, columns=['Person1', 'Person2'])\n",
        "            person_weights = person_df.value_counts().reset_index()\n",
        "            person_weights.columns = ['Person1', 'Person2', 'Weight']\n",
        "\n",
        "            # Create graph\n",
        "            self.person_graph = nx.from_pandas_edgelist(\n",
        "                person_weights, 'Person1', 'Person2', edge_attr='Weight'\n",
        "            )\n",
        "\n",
        "            # Add node attributes efficiently\n",
        "            for node in self.person_graph.nodes():\n",
        "                attrs = self.person_metadata[node].copy()\n",
        "                attrs['books_count'] = self.books_per_person[node]\n",
        "                self.person_graph.nodes[node].update(attrs)\n",
        "\n",
        "            # Add edge attributes\n",
        "            for idx, row in person_weights.iterrows():\n",
        "                pair_key = tuple(sorted([row['Person1'], row['Person2']]))\n",
        "                if pair_key in shared_books_info:\n",
        "                    self.person_graph.edges[row['Person1'], row['Person2']]['shared_books'] = shared_books_info[pair_key]\n",
        "\n",
        "            self.shared_books_info = shared_books_info\n",
        "        else:\n",
        "            self.person_graph = nx.Graph()\n",
        "            self.shared_books_info = {}\n",
        "\n",
        "    def create_book_network(self):\n",
        "        \"\"\"Optimized book network creation\"\"\"\n",
        "        books_per_person = self.df_clean.groupby('index_name')['id'].apply(list)\n",
        "\n",
        "        book_connections = []\n",
        "        shared_readers_info = {}\n",
        "\n",
        "        for person_id, books in books_per_person.items():\n",
        "            unique_books = list(set(books))\n",
        "            if len(unique_books) > 1:\n",
        "                pairs = list(combinations(unique_books, 2))\n",
        "                book_connections.extend(pairs)\n",
        "\n",
        "                person_name = self.person_metadata[person_id]['name']\n",
        "                for pair in pairs:\n",
        "                    pair_key = tuple(sorted(pair))\n",
        "                    if pair_key not in shared_readers_info:\n",
        "                        shared_readers_info[pair_key] = []\n",
        "                    shared_readers_info[pair_key].append((person_name, person_id))\n",
        "\n",
        "        if book_connections:\n",
        "            book_df = pd.DataFrame(book_connections, columns=['Book1', 'Book2'])\n",
        "            book_weights = book_df.value_counts().reset_index()\n",
        "            book_weights.columns = ['Book1', 'Book2', 'Weight']\n",
        "\n",
        "            self.book_graph = nx.from_pandas_edgelist(\n",
        "                book_weights, 'Book1', 'Book2', edge_attr='Weight'\n",
        "            )\n",
        "\n",
        "            # Add node attributes efficiently\n",
        "            for node in self.book_graph.nodes():\n",
        "                attrs = self.book_metadata[node].copy()\n",
        "                attrs['readers_count'] = self.readers_per_book[node]\n",
        "                self.book_graph.nodes[node].update(attrs)\n",
        "\n",
        "            # Add edge attributes\n",
        "            for idx, row in book_weights.iterrows():\n",
        "                pair_key = tuple(sorted([row['Book1'], row['Book2']]))\n",
        "                if pair_key in shared_readers_info:\n",
        "                    self.book_graph.edges[row['Book1'], row['Book2']]['shared_readers'] = shared_readers_info[pair_key]\n",
        "\n",
        "            self.shared_readers_info = shared_readers_info\n",
        "        else:\n",
        "            self.book_graph = nx.Graph()\n",
        "            self.shared_readers_info = {}\n",
        "\n",
        "    def detect_communities(self):\n",
        "        \"\"\"Detect communities with caching\"\"\"\n",
        "        if len(self.person_graph.nodes()) > 0:\n",
        "            communities = community.louvain_communities(self.person_graph, seed=42)\n",
        "\n",
        "            self.node_communities = {}\n",
        "            for i, comm in enumerate(communities):\n",
        "                for node in comm:\n",
        "                    self.node_communities[node] = i\n",
        "                    self.person_graph.nodes[node]['community'] = i\n",
        "\n",
        "            self.communities = list(communities)\n",
        "        else:\n",
        "            self.node_communities = {}\n",
        "            self.communities = []\n",
        "\n",
        "    def calculate_network_metrics(self):\n",
        "        \"\"\"Calculate centrality measures with optimization for both networks\"\"\"\n",
        "        # Calculate metrics for person network\n",
        "        if len(self.person_graph.nodes()) > 0:\n",
        "            # Use faster algorithms for large graphs\n",
        "            if len(self.person_graph.nodes()) > 1000:\n",
        "                # Sample for betweenness centrality\n",
        "                sample_size = min(1000, len(self.person_graph.nodes()))\n",
        "                sample_nodes = np.random.choice(list(self.person_graph.nodes()), sample_size, replace=False)\n",
        "                self.person_betweenness_centrality = nx.betweenness_centrality_subset(\n",
        "                    self.person_graph, sample_nodes, sample_nodes\n",
        "                )\n",
        "            else:\n",
        "                self.person_betweenness_centrality = nx.betweenness_centrality(self.person_graph)\n",
        "\n",
        "            self.person_degree_centrality = nx.degree_centrality(self.person_graph)\n",
        "            self.person_closeness_centrality = nx.closeness_centrality(self.person_graph)\n",
        "\n",
        "            # Add to node attributes\n",
        "            for node in self.person_graph.nodes():\n",
        "                self.person_graph.nodes[node]['degree_centrality'] = self.person_degree_centrality.get(node, 0)\n",
        "                self.person_graph.nodes[node]['betweenness_centrality'] = self.person_betweenness_centrality.get(node, 0)\n",
        "                self.person_graph.nodes[node]['closeness_centrality'] = self.person_closeness_centrality.get(node, 0)\n",
        "        else:\n",
        "            self.person_degree_centrality = {}\n",
        "            self.person_betweenness_centrality = {}\n",
        "            self.person_closeness_centrality = {}\n",
        "\n",
        "        # Calculate metrics for book network\n",
        "        if len(self.book_graph.nodes()) > 0:\n",
        "            # Use faster algorithms for large graphs\n",
        "            if len(self.book_graph.nodes()) > 1000:\n",
        "                # Sample for betweenness centrality\n",
        "                sample_size = min(1000, len(self.book_graph.nodes()))\n",
        "                sample_nodes = np.random.choice(list(self.book_graph.nodes()), sample_size, replace=False)\n",
        "                self.book_betweenness_centrality = nx.betweenness_centrality_subset(\n",
        "                    self.book_graph, sample_nodes, sample_nodes\n",
        "                )\n",
        "            else:\n",
        "                self.book_betweenness_centrality = nx.betweenness_centrality(self.book_graph)\n",
        "\n",
        "            self.book_degree_centrality = nx.degree_centrality(self.book_graph)\n",
        "            self.book_closeness_centrality = nx.closeness_centrality(self.book_graph)\n",
        "\n",
        "            # Add to node attributes\n",
        "            for node in self.book_graph.nodes():\n",
        "                self.book_graph.nodes[node]['degree_centrality'] = self.book_degree_centrality.get(node, 0)\n",
        "                self.book_graph.nodes[node]['betweenness_centrality'] = self.book_betweenness_centrality.get(node, 0)\n",
        "                self.book_graph.nodes[node]['closeness_centrality'] = self.book_closeness_centrality.get(node, 0)\n",
        "        else:\n",
        "            self.book_degree_centrality = {}\n",
        "            self.book_betweenness_centrality = {}\n",
        "            self.book_closeness_centrality = {}\n",
        "\n",
        "        # Keep backward compatibility\n",
        "        self.degree_centrality = self.person_degree_centrality\n",
        "        self.betweenness_centrality = self.person_betweenness_centrality\n",
        "        self.closeness_centrality = self.person_closeness_centrality\n",
        "\n",
        "    def _calculate_metric_ranges(self):\n",
        "        \"\"\"Calculate ranges for network metrics for dynamic sliders\"\"\"\n",
        "        # People network ranges\n",
        "        if self.person_betweenness_centrality:\n",
        "            self.person_betweenness_max = max(self.person_betweenness_centrality.values())\n",
        "            self.person_degree_max = max(self.person_degree_centrality.values())\n",
        "        else:\n",
        "            self.person_betweenness_max = 0.1\n",
        "            self.person_degree_max = 1.0\n",
        "\n",
        "        # Book network ranges\n",
        "        if self.book_betweenness_centrality:\n",
        "            self.book_betweenness_max = max(self.book_betweenness_centrality.values())\n",
        "            self.book_degree_max = max(self.book_degree_centrality.values())\n",
        "        else:\n",
        "            self.book_betweenness_max = 0.1\n",
        "            self.book_degree_max = 1.0\n",
        "\n",
        "    def get_optimized_layout(self, graph, max_iterations=30):\n",
        "        \"\"\"Get optimized layout based on graph size with maximum spacing\"\"\"\n",
        "        n_nodes = len(graph.nodes())\n",
        "\n",
        "        if n_nodes == 0:\n",
        "            return {}\n",
        "        elif n_nodes == 1:\n",
        "            return {list(graph.nodes())[0]: (0, 0)}\n",
        "        elif n_nodes > 500:\n",
        "            # For large graphs, use faster algorithm with maximum spacing\n",
        "            pos = nx.random_layout(graph, seed=42)\n",
        "            # Quick spring layout iterations with much larger k for maximum spacing\n",
        "            for _ in range(15):\n",
        "                pos = nx.spring_layout(\n",
        "                    graph, pos=pos, iterations=3,\n",
        "                    k=6/np.sqrt(n_nodes), seed=42\n",
        "                )\n",
        "            return pos\n",
        "        else:\n",
        "            # Standard spring layout for smaller graphs with maximum spacing\n",
        "            return nx.spring_layout(\n",
        "                graph, k=8/np.sqrt(n_nodes),  # Increased k significantly for maximum spacing\n",
        "                iterations=max_iterations, seed=42,\n",
        "                threshold=1e-4\n",
        "            )\n",
        "\n",
        "    def get_filtered_subgraph(self, graph_type, person_filter=None, book_filter=None,\n",
        "                              community_filter=None, folder_filter=None, gender_filter=None,\n",
        "                              centrality_filter=None, degree_centrality_filter=None,\n",
        "                              language_filter=None, type_filter=None, subject_filter=None,\n",
        "                              max_nodes=None, focus_mode=False):\n",
        "        \"\"\"\n",
        "        Get a filtered subgraph with all original filters restored\n",
        "        \"\"\"\n",
        "        if graph_type == 'person':\n",
        "            graph = self.person_graph\n",
        "            # Use all nodes if max_nodes not specified or exceeds available\n",
        "            if max_nodes is None:\n",
        "                max_nodes = self.max_people_nodes\n",
        "            else:\n",
        "                max_nodes = min(max_nodes, self.max_people_nodes)\n",
        "        else:\n",
        "            graph = self.book_graph\n",
        "            # Use all nodes if max_nodes not specified or exceeds available\n",
        "            if max_nodes is None:\n",
        "                max_nodes = self.max_book_nodes\n",
        "            else:\n",
        "                max_nodes = min(max_nodes, self.max_book_nodes)\n",
        "\n",
        "        # Start with all connected nodes\n",
        "        nodes_to_include = {n for n in graph.nodes() if graph.degree(n) > 0}\n",
        "\n",
        "        # Apply filters one by one (AND logic)\n",
        "        if graph_type == 'person':\n",
        "            if person_filter:\n",
        "                person_nodes = {n for n in nodes_to_include\n",
        "                                if (str(person_filter).lower() in graph.nodes[n].get('name', '').lower() or\n",
        "                                    str(person_filter) == str(n))}\n",
        "\n",
        "                if focus_mode:\n",
        "                    focus_nodes = set()\n",
        "                    for node in person_nodes:\n",
        "                        focus_nodes.add(node)\n",
        "                        focus_nodes.update(graph.neighbors(node))\n",
        "                    nodes_to_include = nodes_to_include.intersection(focus_nodes)\n",
        "                else:\n",
        "                    nodes_to_include = nodes_to_include.intersection(person_nodes)\n",
        "\n",
        "            if community_filter is not None and community_filter != 'all':\n",
        "                community_nodes = {n for n in nodes_to_include\n",
        "                                   if self.node_communities.get(n) == community_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(community_nodes)\n",
        "\n",
        "            if gender_filter and gender_filter != 'all':\n",
        "                gender_nodes = {n for n in nodes_to_include\n",
        "                                if graph.nodes[n].get('gender') == gender_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(gender_nodes)\n",
        "\n",
        "            if centrality_filter is not None and centrality_filter > 0:\n",
        "                centrality_nodes = {n for n in nodes_to_include\n",
        "                                    if self.person_betweenness_centrality.get(n, 0) >= centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(centrality_nodes)\n",
        "\n",
        "            if degree_centrality_filter is not None and degree_centrality_filter > 0:\n",
        "                degree_centrality_nodes = {n for n in nodes_to_include\n",
        "                                           if self.person_degree_centrality.get(n, 0) >= degree_centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(degree_centrality_nodes)\n",
        "\n",
        "            if folder_filter and folder_filter != 'all' and 'all' not in str(folder_filter):\n",
        "                if isinstance(folder_filter, list):\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') in folder_filter}\n",
        "                else:\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') == folder_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(folder_nodes)\n",
        "\n",
        "            # Apply language and type filters for person network by filtering books they read\n",
        "            if language_filter and language_filter != 'all':\n",
        "                if isinstance(language_filter, list) and 'all' not in language_filter:\n",
        "                    person_ids_with_language = set(self.df_clean[\n",
        "                        self.df_clean['language_processed'].isin(language_filter)\n",
        "                    ]['index_name'].unique())\n",
        "                elif not isinstance(language_filter, list):\n",
        "                    person_ids_with_language = set(self.df_clean[\n",
        "                        self.df_clean['language_processed'] == language_filter\n",
        "                    ]['index_name'].unique())\n",
        "                else:\n",
        "                    person_ids_with_language = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(person_ids_with_language)\n",
        "\n",
        "            if type_filter and type_filter != 'all':\n",
        "                if isinstance(type_filter, list) and 'all' not in type_filter:\n",
        "                    person_ids_with_type = set(self.df_clean[\n",
        "                        self.df_clean['type_processed'].isin(type_filter)\n",
        "                    ]['index_name'].unique())\n",
        "                elif not isinstance(type_filter, list):\n",
        "                    person_ids_with_type = set(self.df_clean[\n",
        "                        self.df_clean['type_processed'] == type_filter\n",
        "                    ]['index_name'].unique())\n",
        "                else:\n",
        "                    person_ids_with_type = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(person_ids_with_type)\n",
        "\n",
        "            if subject_filter and subject_filter != 'all':\n",
        "                if isinstance(subject_filter, list) and 'all' not in subject_filter:\n",
        "                    person_ids_with_subject = set(self.df_clean[\n",
        "                        self.df_clean['subject_processed'].isin(subject_filter)\n",
        "                    ]['index_name'].unique())\n",
        "                elif not isinstance(subject_filter, list):\n",
        "                    person_ids_with_subject = set(self.df_clean[\n",
        "                        self.df_clean['subject_processed'] == subject_filter\n",
        "                    ]['index_name'].unique())\n",
        "                else:\n",
        "                    person_ids_with_subject = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(person_ids_with_subject)\n",
        "\n",
        "        else:  # book network\n",
        "            if book_filter:\n",
        "                book_nodes = {n for n in nodes_to_include\n",
        "                              if (str(book_filter).lower() in graph.nodes[n].get('name', '').lower() or\n",
        "                                  str(book_filter) == str(n))}\n",
        "\n",
        "                if focus_mode:\n",
        "                    focus_nodes = set()\n",
        "                    for node in book_nodes:\n",
        "                        focus_nodes.add(node)\n",
        "                        focus_nodes.update(graph.neighbors(node))\n",
        "                    nodes_to_include = nodes_to_include.intersection(focus_nodes)\n",
        "                else:\n",
        "                    nodes_to_include = nodes_to_include.intersection(book_nodes)\n",
        "\n",
        "            if folder_filter and folder_filter != 'all' and 'all' not in str(folder_filter):\n",
        "                if isinstance(folder_filter, list):\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') in folder_filter}\n",
        "                else:\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') == folder_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(folder_nodes)\n",
        "\n",
        "            # Apply language filter for book network\n",
        "            if language_filter and language_filter != 'all':\n",
        "                if isinstance(language_filter, list) and 'all' not in language_filter:\n",
        "                    language_nodes = {n for n in nodes_to_include\n",
        "                                      if graph.nodes[n].get('language') in language_filter}\n",
        "                elif not isinstance(language_filter, list):\n",
        "                    language_nodes = {n for n in nodes_to_include\n",
        "                                      if graph.nodes[n].get('language') == language_filter}\n",
        "                else:\n",
        "                    language_nodes = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(language_nodes)\n",
        "\n",
        "            # Apply type filter for book network\n",
        "            if type_filter and type_filter != 'all':\n",
        "                if isinstance(type_filter, list) and 'all' not in type_filter:\n",
        "                    type_nodes = {n for n in nodes_to_include\n",
        "                                  if graph.nodes[n].get('type') in type_filter}\n",
        "                elif not isinstance(type_filter, list):\n",
        "                    type_nodes = {n for n in nodes_to_include\n",
        "                                  if graph.nodes[n].get('type') == type_filter}\n",
        "                else:\n",
        "                    type_nodes = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(type_nodes)\n",
        "\n",
        "            # Apply subject/genre filter for book network\n",
        "            if subject_filter and subject_filter != 'all':\n",
        "                if isinstance(subject_filter, list) and 'all' not in subject_filter:\n",
        "                    subject_nodes = {n for n in nodes_to_include\n",
        "                                     if graph.nodes[n].get('subject') in subject_filter}\n",
        "                elif not isinstance(subject_filter, list):\n",
        "                    subject_nodes = {n for n in nodes_to_include\n",
        "                                     if graph.nodes[n].get('subject') == subject_filter}\n",
        "                else:\n",
        "                    subject_nodes = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(subject_nodes)\n",
        "\n",
        "            # Apply centrality filters for book network\n",
        "            if centrality_filter is not None and centrality_filter > 0:\n",
        "                centrality_nodes = {n for n in nodes_to_include\n",
        "                                    if self.book_betweenness_centrality.get(n, 0) >= centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(centrality_nodes)\n",
        "\n",
        "            if degree_centrality_filter is not None and degree_centrality_filter > 0:\n",
        "                degree_centrality_nodes = {n for n in nodes_to_include\n",
        "                                           if self.book_degree_centrality.get(n, 0) >= degree_centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(degree_centrality_nodes)\n",
        "\n",
        "        # Limit nodes for performance, prioritize by degree\n",
        "        if len(nodes_to_include) > max_nodes:\n",
        "            # Sort by degree and take top nodes\n",
        "            node_degrees = [(n, graph.degree(n)) for n in nodes_to_include]\n",
        "            node_degrees.sort(key=lambda x: x[1], reverse=True)\n",
        "            nodes_to_include = {n for n, _ in node_degrees[:max_nodes]}\n",
        "\n",
        "        return graph.subgraph(nodes_to_include)\n",
        "\n",
        "def create_network_plot(analyzer, show_people=True, show_books=True,\n",
        "                        person_filter=None, book_filter=None, community_filter=None,\n",
        "                        folder_filter=None, gender_filter=None, centrality_filter=None,\n",
        "                        degree_centrality_filter=None, language_filter=None,\n",
        "                        type_filter=None, subject_filter=None, max_nodes=200):\n",
        "    \"\"\"\n",
        "    Create an interactive network plot using Plotly with all original features restored\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "    focus_mode = bool(person_filter or book_filter)\n",
        "\n",
        "    # Determine which graphs to include based on filters\n",
        "    graphs_to_show = []\n",
        "    if show_people and len(analyzer.person_graph.nodes()) > 0:\n",
        "        person_subgraph = analyzer.get_filtered_subgraph(\n",
        "            'person', person_filter, book_filter, community_filter,\n",
        "            folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "            language_filter, type_filter, subject_filter, max_nodes//2 if show_books else max_nodes, focus_mode=focus_mode\n",
        "        )\n",
        "        if len(person_subgraph.nodes()) > 0:\n",
        "            graphs_to_show.append(('person', person_subgraph))\n",
        "\n",
        "    if show_books and len(analyzer.book_graph.nodes()) > 0:\n",
        "        book_subgraph = analyzer.get_filtered_subgraph(\n",
        "            'book', person_filter, book_filter, community_filter,\n",
        "            folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "            language_filter, type_filter, subject_filter, max_nodes//2 if show_people else max_nodes, focus_mode=focus_mode\n",
        "        )\n",
        "        if len(book_subgraph.nodes()) > 0:\n",
        "            graphs_to_show.append(('book', book_subgraph))\n",
        "\n",
        "    if not graphs_to_show:\n",
        "        fig.add_annotation(\n",
        "            text=\"No data to display with current filters\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "            showarrow=False, font=dict(size=16)\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            title=\"Network Analysis\",\n",
        "            showlegend=False,\n",
        "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "        return fig\n",
        "\n",
        "    # Create combined graph for layout calculation\n",
        "    combined_graph = nx.Graph()\n",
        "    node_types = {}\n",
        "    highlighted_nodes = set()\n",
        "\n",
        "    for graph_type, graph in graphs_to_show:\n",
        "        for node in graph.nodes():\n",
        "            combined_graph.add_node(node)\n",
        "            node_types[node] = graph_type\n",
        "\n",
        "            if graph_type == 'person' and person_filter:\n",
        "                node_name = analyzer.person_graph.nodes[node].get('name', '')\n",
        "                if (str(person_filter).lower() in node_name.lower() or\n",
        "                        str(person_filter) == str(node)):\n",
        "                    highlighted_nodes.add(node)\n",
        "            elif graph_type == 'book' and book_filter:\n",
        "                node_name = analyzer.book_graph.nodes[node].get('name', '')\n",
        "                if (str(book_filter).lower() in node_name.lower() or\n",
        "                        str(book_filter) == str(node)):\n",
        "                    highlighted_nodes.add(node)\n",
        "\n",
        "        for edge in graph.edges(data=True):\n",
        "            combined_graph.add_edge(edge[0], edge[1], **edge[2])\n",
        "\n",
        "    # Calculate layout\n",
        "    pos = analyzer.get_optimized_layout(combined_graph)\n",
        "\n",
        "    # Create traces for edges with hover functionality\n",
        "    edge_traces = []\n",
        "\n",
        "    for graph_type, graph in graphs_to_show:\n",
        "        edge_x = []\n",
        "        edge_y = []\n",
        "        edge_midpoint_x = []\n",
        "        edge_midpoint_y = []\n",
        "        edge_midpoint_info = []\n",
        "\n",
        "        for edge in graph.edges(data=True):\n",
        "            if edge[0] in pos and edge[1] in pos:\n",
        "                x0, y0 = pos[edge[0]]\n",
        "                x1, y1 = pos[edge[1]]\n",
        "                edge_x.extend([x0, x1, None])\n",
        "                edge_y.extend([y0, y1, None])\n",
        "\n",
        "                mid_x = (x0 + x1) / 2\n",
        "                mid_y = (y0 + y1) / 2\n",
        "                edge_midpoint_x.append(mid_x)\n",
        "                edge_midpoint_y.append(mid_y)\n",
        "\n",
        "                if graph_type == 'person':\n",
        "                    shared_books = edge[2].get('shared_books', [])\n",
        "                    book_count = len(shared_books)\n",
        "                    book_names = [book[0] for book in shared_books]\n",
        "\n",
        "                    person1_name = analyzer.person_graph.nodes[edge[0]].get('name', f'Person {edge[0]}')\n",
        "                    person2_name = analyzer.person_graph.nodes[edge[1]].get('name', f'Person {edge[1]}')\n",
        "\n",
        "                    hover_text = f\"<b>{person1_name}</b> ↔ <b>{person2_name}</b><br>\"\n",
        "                    hover_text += f\"<b>Shared Books:</b> {book_count}<br>\"\n",
        "                    for i, book_name in enumerate(book_names[:3]):\n",
        "                        hover_text += f\"• {book_name}<br>\"\n",
        "                    if book_count > 3:\n",
        "                        hover_text += f\"... and {book_count - 3} more books\"\n",
        "\n",
        "                else:\n",
        "                    shared_readers = edge[2].get('shared_readers', [])\n",
        "                    reader_count = len(shared_readers)\n",
        "                    reader_names = [reader[0] for reader in shared_readers]\n",
        "\n",
        "                    book1_name = analyzer.book_graph.nodes[edge[0]].get('name', f'Book {edge[0]}')\n",
        "                    book2_name = analyzer.book_graph.nodes[edge[1]].get('name', f'Book {edge[1]}')\n",
        "\n",
        "                    hover_text = f\"<b>{book1_name}</b> ↔ <b>{book2_name}</b><br>\"\n",
        "                    hover_text += f\"<b>Shared Readers:</b> {reader_count}<br>\"\n",
        "                    for i, reader_name in enumerate(reader_names[:3]):\n",
        "                        hover_text += f\"• {reader_name}<br>\"\n",
        "                    if reader_count > 3:\n",
        "                        hover_text += f\"... and {reader_count - 3} more readers\"\n",
        "\n",
        "                edge_midpoint_info.append(hover_text)\n",
        "\n",
        "        # Add edge lines with thinner width\n",
        "        if edge_x:\n",
        "            edge_trace = go.Scatter(\n",
        "                x=edge_x, y=edge_y,\n",
        "                line=dict(width=1.5, color='rgba(136,136,136,0.6)'),  # Reduced from 4 to 1.5\n",
        "                mode='lines',\n",
        "                showlegend=False,\n",
        "                name=f'{graph_type}_edges',\n",
        "                opacity=0.8,\n",
        "                hoverinfo='skip'\n",
        "            )\n",
        "            fig.add_trace(edge_trace)\n",
        "\n",
        "            # Add invisible midpoint markers for hover\n",
        "            edge_hover_trace = go.Scatter(\n",
        "                x=edge_midpoint_x,\n",
        "                y=edge_midpoint_y,\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=8,\n",
        "                    color='rgba(0,0,0,0)',\n",
        "                    line=dict(width=0)\n",
        "                ),\n",
        "                hovertemplate='%{text}<extra></extra>',\n",
        "                text=edge_midpoint_info,\n",
        "                showlegend=False,\n",
        "                name=f'{graph_type}_edge_hovers'\n",
        "            )\n",
        "            fig.add_trace(edge_hover_trace)\n",
        "\n",
        "    # Create traces for nodes with full detail\n",
        "    for graph_type, graph in graphs_to_show:\n",
        "        node_x = []\n",
        "        node_y = []\n",
        "        node_info = []\n",
        "        node_colors = []\n",
        "        node_sizes = []\n",
        "        node_symbols = []\n",
        "\n",
        "        for node in graph.nodes():\n",
        "            if node in pos:\n",
        "                x, y = pos[node]\n",
        "                node_x.append(x)\n",
        "                node_y.append(y)\n",
        "\n",
        "                if graph_type == 'person':\n",
        "                    person_data = analyzer.person_graph.nodes[node]\n",
        "                    community_id = analyzer.node_communities.get(node, 'Unknown')\n",
        "                    connections_count = graph.degree(node)\n",
        "                    books_count = person_data.get('books_count', 0)\n",
        "                    betweenness = analyzer.person_betweenness_centrality.get(node, 0)\n",
        "                    degree_cent = analyzer.person_degree_centrality.get(node, 0)\n",
        "\n",
        "                    hover_text = f\"\"\"\n",
        "                    <b>Name:</b> {person_data.get('name', 'Unknown')}<br>\n",
        "                    <b>Index:</b> {node}<br>\n",
        "                    <b>Gender:</b> {person_data.get('gender', 'Unknown')}<br>\n",
        "                    <b>Folder:</b> {person_data.get('folder', 'Unknown')}<br>\n",
        "                    <b>Connected People:</b> {connections_count}<br>\n",
        "                    <b>Books Borrowed:</b> {books_count}<br>\n",
        "                    <b>Community:</b> {community_id}<br>\n",
        "                    <b>Betweenness Centrality:</b> {betweenness:.4f}<br>\n",
        "                    <b>Degree Centrality:</b> {degree_cent:.4f}\n",
        "                    \"\"\".strip()\n",
        "\n",
        "                    if node in highlighted_nodes:\n",
        "                        node_colors.append('red')\n",
        "                        node_sizes.append(max(20, min(40, 20 + connections_count * 3)))\n",
        "                    else:\n",
        "                        node_colors.append(community_id)\n",
        "                        node_sizes.append(max(8, min(25, 8 + connections_count * 1.5)))\n",
        "\n",
        "                    node_symbols.append('circle')\n",
        "\n",
        "                else:\n",
        "                    book_data = analyzer.book_graph.nodes[node]\n",
        "                    readers_count = book_data.get('readers_count', 0)\n",
        "                    connections_count = graph.degree(node)\n",
        "                    betweenness = analyzer.book_betweenness_centrality.get(node, 0)\n",
        "                    degree_cent = analyzer.book_degree_centrality.get(node, 0)\n",
        "\n",
        "                    hover_text = f\"\"\"\n",
        "                    <b>Book:</b> {book_data.get('name', 'Unknown')}<br>\n",
        "                    <b>ID:</b> {node}<br>\n",
        "                    <b>Readers:</b> {readers_count}<br>\n",
        "                    <b>Connections:</b> {connections_count}<br>\n",
        "                    <b>Folder:</b> {book_data.get('folder', 'Unknown')}<br>\n",
        "                    <b>Language:</b> {book_data.get('language', 'Unknown')}<br>\n",
        "                    <b>Type:</b> {book_data.get('type', 'Unknown')}<br>\n",
        "                    <b>Subject/Genre:</b> {book_data.get('subject', 'Unknown')}<br>\n",
        "                    <b>Betweenness Centrality:</b> {betweenness:.4f}<br>\n",
        "                    <b>Degree Centrality:</b> {degree_cent:.4f}\n",
        "                    \"\"\".strip()\n",
        "\n",
        "                    if node in highlighted_nodes:\n",
        "                        node_colors.append(10)\n",
        "                        node_sizes.append(max(15, min(35, 15 + readers_count * 2)))\n",
        "                    else:\n",
        "                        folder_map = {\n",
        "                            'Vol_1_1902': 0,\n",
        "                            'Vol 1_1': 1,\n",
        "                            'SL Ledger 1934': 2,\n",
        "                            'SL Ledger_1940': 3\n",
        "                        }\n",
        "                        folder_value = folder_map.get(book_data.get('folder', 'Unknown'), 0)\n",
        "                        node_colors.append(folder_value)\n",
        "                        node_sizes.append(max(6, min(20, 6 + readers_count * 1)))\n",
        "\n",
        "                    node_symbols.append('triangle-up')\n",
        "\n",
        "                node_info.append(hover_text)\n",
        "\n",
        "        # Add node trace\n",
        "        if node_x:\n",
        "            node_trace = go.Scatter(\n",
        "                x=node_x, y=node_y,\n",
        "                mode='markers',\n",
        "                hovertemplate='%{text}<extra></extra>',\n",
        "                text=node_info,\n",
        "                marker=dict(\n",
        "                    size=node_sizes,\n",
        "                    color=node_colors,\n",
        "                    symbol=node_symbols,\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(\n",
        "                        title=f\"{'Community' if graph_type == 'person' else 'Volume'}\",\n",
        "                        x=1.02 if graph_type == 'person' else 1.1\n",
        "                    ),\n",
        "                    line=dict(width=1, color='DarkSlateGrey')\n",
        "                ),\n",
        "                name=f\"{'People' if graph_type == 'person' else 'Books'}\",\n",
        "                showlegend=True\n",
        "            )\n",
        "            fig.add_trace(node_trace)\n",
        "\n",
        "    # Update layout for maximum space usage\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': f'Strashun Library Network Analysis{\"\" if not focus_mode else \" - Focus View\"}',\n",
        "            'font': {'size': 16}\n",
        "        },\n",
        "        showlegend=True,\n",
        "        hovermode='closest',\n",
        "        margin=dict(b=10, l=10, r=10, t=50),  # Reduced margins for more graph space\n",
        "        annotations=[dict(\n",
        "            text=\"Hover over nodes and edges to see details\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.005, y=-0.002)],\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        plot_bgcolor='white',\n",
        "        height=900,  # Increased from 800 to 900 for maximum space\n",
        "        dragmode='pan',\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def calculate_network_statistics(analyzer, show_people=True, show_books=True,\n",
        "                                 person_filter=None, book_filter=None,\n",
        "                                 community_filter=None, folder_filter=None,\n",
        "                                 gender_filter=None, centrality_filter=None,\n",
        "                                 degree_centrality_filter=None, language_filter=None,\n",
        "                                 type_filter=None, subject_filter=None, max_nodes=200):\n",
        "    \"\"\"\n",
        "    Calculate and format network statistics for display with full functionality\n",
        "    \"\"\"\n",
        "    try:\n",
        "        stats_components = []\n",
        "\n",
        "        if show_people and len(analyzer.person_graph.nodes()) > 0:\n",
        "            person_subgraph = analyzer.get_filtered_subgraph(\n",
        "                'person', person_filter, book_filter, community_filter,\n",
        "                folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "                language_filter, type_filter, subject_filter,\n",
        "                max_nodes//2 if show_books else max_nodes,\n",
        "                focus_mode=bool(person_filter or book_filter)\n",
        "            )\n",
        "\n",
        "            if len(person_subgraph.nodes()) > 0:\n",
        "                central_people = []\n",
        "                for node in person_subgraph.nodes():\n",
        "                    centrality = analyzer.person_betweenness_centrality.get(node, 0)\n",
        "                    if isinstance(centrality, (int, float)) and centrality > 0:\n",
        "                        name = analyzer.person_graph.nodes[node].get('name', f'Person {node}')\n",
        "                        central_people.append((name, centrality, node))\n",
        "\n",
        "                central_people.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_central = central_people[:5]\n",
        "\n",
        "                stats_components.extend([\n",
        "                    html.H5(\"People Network Statistics\"),\n",
        "                    html.P(f\"Displayed People: {len(person_subgraph.nodes())}\"),\n",
        "                    html.P(f\"Displayed Connections: {len(person_subgraph.edges())}\"),\n",
        "                    html.P(f\"Communities: {len(set(analyzer.node_communities.get(n, 0) for n in person_subgraph.nodes()))}\"),\n",
        "                    html.H6(\"Most Central People (Betweenness):\"),\n",
        "                    html.Ul([\n",
        "                        html.Li(f\"{name} (Index: {idx}) - {centrality:.3f}\")\n",
        "                        for name, centrality, idx in top_central\n",
        "                    ]) if top_central else html.P(\"No central people found\"),\n",
        "                    html.Hr()\n",
        "                ])\n",
        "\n",
        "        if show_books and len(analyzer.book_graph.nodes()) > 0:\n",
        "            book_subgraph = analyzer.get_filtered_subgraph(\n",
        "                'book', person_filter, book_filter, community_filter,\n",
        "                folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "                language_filter, type_filter, subject_filter,\n",
        "                max_nodes//2 if show_people else max_nodes,\n",
        "                focus_mode=bool(person_filter or book_filter)\n",
        "            )\n",
        "\n",
        "            if len(book_subgraph.nodes()) > 0:\n",
        "                popular_books = []\n",
        "                central_books = []\n",
        "                for node in book_subgraph.nodes():\n",
        "                    readers_count = analyzer.book_graph.nodes[node].get('readers_count', 0)\n",
        "                    centrality = analyzer.book_betweenness_centrality.get(node, 0)\n",
        "                    degree_cent = analyzer.book_degree_centrality.get(node, 0)\n",
        "                    name = analyzer.book_graph.nodes[node].get('name', f'Book {node}')\n",
        "\n",
        "                    if isinstance(readers_count, (int, float)) and readers_count > 0:\n",
        "                        popular_books.append((name, readers_count, node))\n",
        "\n",
        "                    if isinstance(centrality, (int, float)) and centrality > 0:\n",
        "                        central_books.append((name, centrality, node))\n",
        "\n",
        "                popular_books.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_popular = popular_books[:5]\n",
        "\n",
        "                central_books.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_central_books = central_books[:5]\n",
        "\n",
        "                stats_components.extend([\n",
        "                    html.H5(\"Books Network Statistics\"),\n",
        "                    html.P(f\"Displayed Books: {len(book_subgraph.nodes())}\"),\n",
        "                    html.P(f\"Displayed Connections: {len(book_subgraph.edges())}\"),\n",
        "                    html.H6(\"Most Popular Books:\"),\n",
        "                    html.Ul([\n",
        "                        html.Li(f\"{name} (ID: {book_id}) - {readers} readers\")\n",
        "                        for name, readers, book_id in top_popular\n",
        "                    ]) if top_popular else html.P(\"No popular books found\"),\n",
        "                    html.H6(\"Most Central Books (Betweenness):\"),\n",
        "                    html.Ul([\n",
        "                        html.Li(f\"{name} (ID: {book_id}) - {centrality:.3f}\")\n",
        "                        for name, centrality, book_id in top_central_books\n",
        "                    ]) if top_central_books else html.P(\"No central books found\")\n",
        "                ])\n",
        "\n",
        "        if not stats_components:\n",
        "            stats_components = [html.P(\"No statistics available for current selection.\")]\n",
        "\n",
        "        return stats_components\n",
        "\n",
        "    except Exception as e:\n",
        "        return [html.P(f\"Error calculating statistics: {str(e)}\")]\n",
        "\n",
        "def create_optimized_dashboard(csv_filepath):\n",
        "    \"\"\"Create the optimized dashboard with all original features restored\"\"\"\n",
        "\n",
        "    analyzer = OptimizedLibraryNetworkAnalyzer(csv_filepath)\n",
        "\n",
        "    # Calculate dynamic slider ranges\n",
        "    people_slider_max = max(50, min(1000, analyzer.max_people_nodes))\n",
        "    books_slider_max = max(50, min(1000, analyzer.max_book_nodes))\n",
        "\n",
        "    # Default values - back to 100\n",
        "    people_default = min(100, analyzer.max_people_nodes)  # Back to 100\n",
        "    books_default = min(100, analyzer.max_book_nodes)     # Back to 100\n",
        "\n",
        "    # Initialize app\n",
        "    app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "    app.title = \"Strashun Library Network Analysis\"\n",
        "\n",
        "    # App layout with all original filters and functionality\n",
        "    app.layout = dbc.Container([\n",
        "        dbc.Row([\n",
        "            dbc.Col([\n",
        "                html.H1(\"Strashun Library Network Analysis Dashboard\",\n",
        "                        className=\"text-center mb-4\"),\n",
        "                html.P(\"Analyzing connections between book borrowers in the historic Strashun Library in Lithuania\",\n",
        "                       className=\"text-center text-muted mb-4\"),\n",
        "                dbc.Alert([\n",
        "                    html.I(className=\"fas fa-info-circle me-2\"),\n",
        "                    f\"Optimal Network View: Displaying up to 100 most connected nodes by default for maximum readability and spacing. Use Max Nodes slider to show up to {max(people_slider_max, books_slider_max)} nodes or search fields to focus on specific individuals or books.\"\n",
        "                ], color=\"info\", className=\"mb-4\")\n",
        "            ])\n",
        "        ]),\n",
        "\n",
        "        dbc.Row([\n",
        "            # Filter Panel with all original filters\n",
        "            dbc.Col([\n",
        "                dbc.Card([\n",
        "                    dbc.CardHeader(html.H4(\"Filters\", className=\"mb-0\")),\n",
        "                    dbc.CardBody([\n",
        "                        # 1. Data Sources Selection\n",
        "                        html.H5(\"1. Data Sources\", className=\"mt-3\"),\n",
        "                        dbc.Checklist(\n",
        "                            id=\"data-sources-filter\",\n",
        "                            options=[\n",
        "                                {\"label\": \"People Network\", \"value\": \"people\"},\n",
        "                                {\"label\": \"Books Network\", \"value\": \"books\"}\n",
        "                            ],\n",
        "                            value=[\"people\", \"books\"],\n",
        "                            inline=True\n",
        "                        ),\n",
        "\n",
        "                        # 2. People Filters\n",
        "                        html.H5(\"2. People Filters\", className=\"mt-3\"),\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Search Person:\"),\n",
        "                                dbc.Input(\n",
        "                                    id=\"person-search\",\n",
        "                                    placeholder=\"Name or Index\",\n",
        "                                    type=\"text\"\n",
        "                                )\n",
        "                            ], width=6),\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Gender:\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"gender-filter\",\n",
        "                                    options=[\n",
        "                                        {\"label\": \"All\", \"value\": \"all\"},\n",
        "                                        {\"label\": \"Male\", \"value\": \"Male\"},\n",
        "                                        {\"label\": \"Female\", \"value\": \"Female\"}\n",
        "                                    ],\n",
        "                                    value=\"all\"\n",
        "                                )\n",
        "                            ], width=6)\n",
        "                        ]),\n",
        "\n",
        "                        # 3. Books Filters\n",
        "                        html.H5(\"3. Books Filters\", className=\"mt-3\"),\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Search Book:\"),\n",
        "                                dbc.Input(\n",
        "                                    id=\"book-search\",\n",
        "                                    placeholder=\"Name or ID\",\n",
        "                                    type=\"text\"\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        # Language, Type and Subject Filters - multi-select\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Language:\", className=\"mt-2\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"language-filter\",\n",
        "                                    options=[{\"label\": \"All Languages\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": lang, \"value\": lang} for lang in analyzer.unique_languages],\n",
        "                                    value=\"all\",\n",
        "                                    multi=True,\n",
        "                                    style={'fontSize': '12px'}\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Type:\", className=\"mt-2\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"type-filter\",\n",
        "                                    options=[{\"label\": \"All Types\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": type_val, \"value\": type_val} for type_val in analyzer.unique_types],\n",
        "                                    value=\"all\",\n",
        "                                    multi=True,\n",
        "                                    style={'fontSize': '12px'}\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Subject/Genre:\", className=\"mt-2\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"subject-filter\",\n",
        "                                    options=[{\"label\": \"All Subjects\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": subj, \"value\": subj} for subj in analyzer.unique_subjects],\n",
        "                                    value=\"all\",\n",
        "                                    multi=True,\n",
        "                                    style={'fontSize': '12px', 'whiteSpace': 'nowrap'},\n",
        "                                    optionHeight=35\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        # 4. Folder Filter\n",
        "                        html.H5(\"4. Folder Filter\", className=\"mt-3\"),\n",
        "                        dcc.Dropdown(\n",
        "                            id=\"folder-filter\",\n",
        "                            options=[\n",
        "                                {\"label\": \"All Folders\", \"value\": \"all\"}\n",
        "                            ] + [{\"label\": folder, \"value\": folder} for folder in analyzer.unique_folders],\n",
        "                            value=\"all\",\n",
        "                            multi=True\n",
        "                        ),\n",
        "\n",
        "                        # 5. Network Metrics\n",
        "                        html.H5(\"5. Network Metrics\", className=\"mt-3\"),\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label([\n",
        "                                    \"Community:\",\n",
        "                                    dbc.Button(\"?\", id=\"community-help\", size=\"sm\", color=\"info\",\n",
        "                                              className=\"ms-1\", style={\"fontSize\": \"10px\", \"padding\": \"0 5px\"})\n",
        "                                ]),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"community-filter\",\n",
        "                                    options=[{\"label\": \"All\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": f\"Community {i}\", \"value\": i}\n",
        "                                             for i in range(len(analyzer.communities))],\n",
        "                                    value=\"all\"\n",
        "                                )\n",
        "                            ], width=6),\n",
        "                            dbc.Col([\n",
        "                                dbc.Label([\n",
        "                                    \"Min Betweenness Centrality:\",\n",
        "                                    dbc.Button(\"?\", id=\"betweenness-help\", size=\"sm\", color=\"info\",\n",
        "                                              className=\"ms-1\", style={\"fontSize\": \"10px\", \"padding\": \"0 5px\"})\n",
        "                                ]),\n",
        "                                dcc.Slider(\n",
        "                                    id=\"centrality-filter\",\n",
        "                                    min=0,\n",
        "                                    max=0.1,\n",
        "                                    step=0.001,\n",
        "                                    value=0,\n",
        "                                    marks={},\n",
        "                                    tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
        "                                )\n",
        "                            ], width=6)\n",
        "                        ]),\n",
        "\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label([\n",
        "                                    \"Min Degree Centrality:\",\n",
        "                                    dbc.Button(\"?\", id=\"degree-help\", size=\"sm\", color=\"info\",\n",
        "                                              className=\"ms-1\", style={\"fontSize\": \"10px\", \"padding\": \"0 5px\"})\n",
        "                                ]),\n",
        "                                dcc.Slider(\n",
        "                                    id=\"degree-centrality-filter\",\n",
        "                                    min=0,\n",
        "                                    max=1.0,\n",
        "                                    step=0.01,\n",
        "                                    value=0,\n",
        "                                    marks={},\n",
        "                                    tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        # Max Nodes Slider with dynamic limits\n",
        "                        html.H5(\"6. Display Limits\", className=\"mt-3\"),\n",
        "                        dbc.Label(f\"Max Nodes (up to {max(people_slider_max, books_slider_max)}):\"),\n",
        "                        dcc.Slider(\n",
        "                            id=\"max-nodes-filter\",\n",
        "                            min=50,\n",
        "                            max=max(people_slider_max, books_slider_max),\n",
        "                            step=25,\n",
        "                            value=100,  # Back to 100\n",
        "                            marks={i: str(i) for i in range(50, max(people_slider_max, books_slider_max) + 1, max(50, max(people_slider_max, books_slider_max) // 10))},\n",
        "                            tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
        "                        ),\n",
        "\n",
        "                        # Tooltips for help buttons\n",
        "                        dbc.Tooltip(\n",
        "                            \"Communities are groups of nodes that are more densely connected to each other than to the rest of the network. \"\n",
        "                            \"This helps identify groups of people who read similar books or shared populations.\",\n",
        "                            target=\"community-help\",\n",
        "                            placement=\"top\"\n",
        "                        ),\n",
        "                        dbc.Tooltip(\n",
        "                            \"Betweenness Centrality measures how often a node lies on the shortest path between other pairs of nodes. \"\n",
        "                            \"A high value indicates an important 'bridge' or mediator between different groups of people.\",\n",
        "                            target=\"betweenness-help\",\n",
        "                            placement=\"top\"\n",
        "                        ),\n",
        "                        dbc.Tooltip(\n",
        "                            \"Degree Centrality measures the number of direct connections of a node divided by the maximum possible number. \"\n",
        "                            \"A high value indicates a person who is connected to many people - 'popular' or central in the network.\",\n",
        "                            target=\"degree-help\",\n",
        "                            placement=\"top\"\n",
        "                        ),\n",
        "\n",
        "                        # Reset Button\n",
        "                        dbc.Button(\n",
        "                            \"Reset All Filters\",\n",
        "                            id=\"reset-filters\",\n",
        "                            color=\"secondary\",\n",
        "                            className=\"mt-3 w-100\"\n",
        "                        )\n",
        "                    ])\n",
        "                ])\n",
        "            ], width=3),\n",
        "\n",
        "            # Main Network Graph\n",
        "            dbc.Col([\n",
        "                dbc.Card([\n",
        "                    dbc.CardBody([\n",
        "                        dcc.Loading(\n",
        "                            id=\"loading-graph\",\n",
        "                            type=\"default\",\n",
        "                            children=[\n",
        "                                dcc.Graph(\n",
        "                                    id=\"network-graph\",\n",
        "                                    style={'height': '900px'},  # Increased from 700px to 900px\n",
        "                                    config={\n",
        "                                        'displayModeBar': True,\n",
        "                                        'toImageButtonOptions': {'height': 900, 'width': 1200},  # Updated export size\n",
        "                                        'scrollZoom': True,\n",
        "                                        'doubleClick': 'reset',\n",
        "                                        'modeBarButtonsToAdd': ['pan2d'],\n",
        "                                        'modeBarButtonsToRemove': []\n",
        "                                    }\n",
        "                                ),\n",
        "                            ]\n",
        "                        )\n",
        "                    ])\n",
        "                ])\n",
        "            ], width=9)\n",
        "        ]),\n",
        "\n",
        "        # Statistics Row\n",
        "        dbc.Row([\n",
        "            dbc.Col([\n",
        "                dbc.Card([\n",
        "                    dbc.CardHeader(html.H4(\"Network Statistics\", className=\"mb-0\")),\n",
        "                    dbc.CardBody([\n",
        "                        html.Div(id=\"network-stats\")\n",
        "                    ])\n",
        "                ])\n",
        "            ])\n",
        "        ], className=\"mt-4\")\n",
        "    ], fluid=True)\n",
        "\n",
        "    # Callback for updating the network graph with all parameters\n",
        "    @app.callback(\n",
        "        [Output(\"network-graph\", \"figure\"),\n",
        "         Output(\"network-stats\", \"children\")],\n",
        "        [Input(\"data-sources-filter\", \"value\"),\n",
        "         Input(\"person-search\", \"value\"),\n",
        "         Input(\"book-search\", \"value\"),\n",
        "         Input(\"gender-filter\", \"value\"),\n",
        "         Input(\"folder-filter\", \"value\"),\n",
        "         Input(\"community-filter\", \"value\"),\n",
        "         Input(\"centrality-filter\", \"value\"),\n",
        "         Input(\"degree-centrality-filter\", \"value\"),\n",
        "         Input(\"language-filter\", \"value\"),\n",
        "         Input(\"type-filter\", \"value\"),\n",
        "         Input(\"subject-filter\", \"value\"),\n",
        "         Input(\"max-nodes-filter\", \"value\"),\n",
        "         Input(\"reset-filters\", \"n_clicks\")],\n",
        "        prevent_initial_call=False\n",
        "    )\n",
        "    def update_network_graph(data_sources, person_search, book_search, gender_filter,\n",
        "                             folder_filter, community_filter, centrality_filter, degree_centrality_filter,\n",
        "                             language_filter, type_filter, subject_filter, max_nodes, reset_clicks):\n",
        "        try:\n",
        "            ctx = callback_context\n",
        "            if ctx.triggered and ctx.triggered[0]['prop_id'] == 'reset-filters.n_clicks':\n",
        "                show_people = True\n",
        "                show_books = True\n",
        "                person_filter = None\n",
        "                book_filter = None\n",
        "                gender_filter = \"all\"\n",
        "                folder_filter = \"all\"\n",
        "                community_filter = \"all\"\n",
        "                centrality_filter = 0\n",
        "                degree_centrality_filter = 0\n",
        "                language_filter = \"all\"\n",
        "                type_filter = \"all\"\n",
        "                subject_filter = \"all\"\n",
        "                max_nodes = 100  # Back to 100\n",
        "            else:\n",
        "                show_people = \"people\" in (data_sources or [])\n",
        "                show_books = \"books\" in (data_sources or [])\n",
        "                person_filter = person_search if person_search else None\n",
        "                book_filter = book_search if book_search else None\n",
        "\n",
        "                if not folder_filter or folder_filter == \"all\":\n",
        "                    folder_filter = None\n",
        "                elif isinstance(folder_filter, list):\n",
        "                    if \"all\" in folder_filter:\n",
        "                        folder_filter = None\n",
        "\n",
        "                if not language_filter or language_filter == \"all\":\n",
        "                    language_filter = None\n",
        "                elif isinstance(language_filter, list):\n",
        "                    if \"all\" in language_filter or len(language_filter) == 0:\n",
        "                        language_filter = None\n",
        "\n",
        "                if not type_filter or type_filter == \"all\":\n",
        "                    type_filter = None\n",
        "                elif isinstance(type_filter, list):\n",
        "                    if \"all\" in type_filter or len(type_filter) == 0:\n",
        "                        type_filter = None\n",
        "\n",
        "                if not subject_filter or subject_filter == \"all\":\n",
        "                    subject_filter = None\n",
        "                elif isinstance(subject_filter, list):\n",
        "                    if \"all\" in subject_filter or len(subject_filter) == 0:\n",
        "                        subject_filter = None\n",
        "\n",
        "            fig = create_network_plot(\n",
        "                analyzer,\n",
        "                show_people=show_people,\n",
        "                show_books=show_books,\n",
        "                person_filter=person_filter,\n",
        "                book_filter=book_filter,\n",
        "                community_filter=community_filter,\n",
        "                folder_filter=folder_filter,\n",
        "                gender_filter=gender_filter,\n",
        "                centrality_filter=centrality_filter,\n",
        "                degree_centrality_filter=degree_centrality_filter,\n",
        "                language_filter=language_filter,\n",
        "                type_filter=type_filter,\n",
        "                subject_filter=subject_filter,\n",
        "                max_nodes=max_nodes\n",
        "            )\n",
        "\n",
        "            stats = calculate_network_statistics(\n",
        "                analyzer, show_people, show_books, person_filter, book_filter,\n",
        "                community_filter, folder_filter, gender_filter, centrality_filter,\n",
        "                degree_centrality_filter, language_filter, type_filter, subject_filter, max_nodes\n",
        "            )\n",
        "\n",
        "            return fig, stats\n",
        "\n",
        "        except Exception as e:\n",
        "            empty_fig = go.Figure()\n",
        "            empty_fig.add_annotation(\n",
        "                text=f\"Error loading graph: {str(e)}\",\n",
        "                xref=\"paper\", yref=\"paper\",\n",
        "                x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "                showarrow=False, font=dict(size=16)\n",
        "            )\n",
        "            empty_fig.update_layout(\n",
        "                title=\"Error\",\n",
        "                showlegend=False,\n",
        "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                plot_bgcolor='white'\n",
        "            )\n",
        "            return empty_fig, [html.P(f\"Error: {str(e)}\")]\n",
        "\n",
        "    # Callback for updating slider ranges based on network type\n",
        "    @app.callback(\n",
        "        [Output(\"centrality-filter\", \"max\"),\n",
        "         Output(\"centrality-filter\", \"marks\"),\n",
        "         Output(\"centrality-filter\", \"step\"),\n",
        "         Output(\"degree-centrality-filter\", \"max\"),\n",
        "         Output(\"degree-centrality-filter\", \"marks\")],\n",
        "        [Input(\"data-sources-filter\", \"value\")]\n",
        "    )\n",
        "    def update_centrality_sliders(data_sources):\n",
        "        show_people = \"people\" in (data_sources or [])\n",
        "        show_books = \"books\" in (data_sources or [])\n",
        "\n",
        "        if show_books and not show_people:\n",
        "            # Books only - use book ranges\n",
        "            betweenness_max = max(0.1, analyzer.book_betweenness_max)\n",
        "            degree_max = max(0.1, analyzer.book_degree_max)\n",
        "            step = betweenness_max / 1000\n",
        "            betweenness_marks = {0: \"0\", betweenness_max: f\"{betweenness_max:.3f}\"}\n",
        "            degree_marks = {0: \"0\", degree_max: f\"{degree_max:.3f}\"}\n",
        "        elif show_people and not show_books:\n",
        "            # People only - use people ranges\n",
        "            betweenness_max = max(0.1, analyzer.person_betweenness_max)\n",
        "            degree_max = max(0.1, analyzer.person_degree_max)\n",
        "            step = betweenness_max / 100\n",
        "            betweenness_marks = {0: \"0\", betweenness_max: f\"{betweenness_max:.3f}\"}\n",
        "            degree_marks = {0: \"0\", degree_max: f\"{degree_max:.3f}\"}\n",
        "        else:\n",
        "            # Both or default - use people ranges\n",
        "            betweenness_max = max(0.1, analyzer.person_betweenness_max)\n",
        "            degree_max = max(0.1, analyzer.person_degree_max)\n",
        "            step = betweenness_max / 100\n",
        "            betweenness_marks = {0: \"0\", betweenness_max: f\"{betweenness_max:.3f}\"}\n",
        "            degree_marks = {0: \"0\", degree_max: f\"{degree_max:.3f}\"}\n",
        "\n",
        "        return betweenness_max, betweenness_marks, step, degree_max, degree_marks\n",
        "\n",
        "    # Callback for resetting filters\n",
        "    @app.callback(\n",
        "        [Output(\"data-sources-filter\", \"value\"),\n",
        "         Output(\"person-search\", \"value\"),\n",
        "         Output(\"book-search\", \"value\"),\n",
        "         Output(\"gender-filter\", \"value\"),\n",
        "         Output(\"folder-filter\", \"value\"),\n",
        "         Output(\"community-filter\", \"value\"),\n",
        "         Output(\"centrality-filter\", \"value\"),\n",
        "         Output(\"degree-centrality-filter\", \"value\"),\n",
        "         Output(\"language-filter\", \"value\"),\n",
        "         Output(\"type-filter\", \"value\"),\n",
        "         Output(\"subject-filter\", \"value\"),\n",
        "         Output(\"max-nodes-filter\", \"value\")],\n",
        "        [Input(\"reset-filters\", \"n_clicks\")],\n",
        "        prevent_initial_call=True\n",
        "    )\n",
        "    def reset_all_filters(n_clicks):\n",
        "        if n_clicks:\n",
        "            return (\n",
        "                [\"people\", \"books\"],\n",
        "                \"\",\n",
        "                \"\",\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                0,\n",
        "                0,\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                100  # Back to 100\n",
        "            )\n",
        "        return dash.no_update\n",
        "\n",
        "    return app\n",
        "\n",
        "def get_csv_filepath():\n",
        "    \"\"\"Ask user for CSV file path and verify it exists\"\"\"\n",
        "    while True:\n",
        "        print(\"Please enter the full path to your CSV file:\")\n",
        "        print(\"   Example: /content/borrowers_data.csv\")\n",
        "        print(\"   Or simply: borrowers_data.csv (if file is in same directory)\")\n",
        "\n",
        "        filepath = input(\"\\nFile path: \").strip()\n",
        "\n",
        "        # Check if file exists\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"File found: {filepath}\")\n",
        "            return filepath\n",
        "        else:\n",
        "            print(f\"Error: File {filepath} not found!\")\n",
        "            print(\"Please try again with a valid path\\n\")\n",
        "\n",
        "def setup_simple_tunnel(port=8050):\n",
        "    \"\"\"Setup tunnel using the most reliable method\"\"\"\n",
        "    import subprocess\n",
        "    import threading\n",
        "    import time\n",
        "    import re\n",
        "\n",
        "    # Method 1: Try localtunnel first (most reliable)\n",
        "    try:\n",
        "        # Install Node.js and localtunnel\n",
        "        subprocess.run([\"curl\", \"-fsSL\", \"https://deb.nodesource.com/setup_16.x\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        subprocess.run([\"sudo\", \"bash\", \"/tmp/setup_16.x\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"nodejs\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        subprocess.run([\"sudo\", \"npm\", \"install\", \"-g\", \"localtunnel\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "        # Start localtunnel with custom subdomain to avoid password issues\n",
        "        subdomain = f\"dashboard-{int(time.time() % 10000)}\"\n",
        "\n",
        "        def run_localtunnel():\n",
        "            try:\n",
        "                process = subprocess.Popen([\n",
        "                    \"lt\", \"--port\", str(port), \"--subdomain\", subdomain\n",
        "                ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "                time.sleep(3)\n",
        "                stdout, stderr = process.communicate(timeout=10)\n",
        "\n",
        "                # Look for URL in output\n",
        "                if f\"https://{subdomain}.loca.lt\" in stdout or f\"https://{subdomain}.loca.lt\" in stderr:\n",
        "                    public_url = f\"https://{subdomain}.loca.lt\"\n",
        "                    print(f\"Public URL: {public_url}\")\n",
        "                    return True\n",
        "\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "        if run_localtunnel():\n",
        "            return True\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Method 2: Try ngrok without auth (limited time but works)\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "        # Try to create tunnel without auth (works for limited time)\n",
        "        public_url = ngrok.connect(port)\n",
        "\n",
        "        print(f\"Public URL: {public_url}\")\n",
        "        return True\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Method 3: Manual instructions as fallback\n",
        "    print(\"Automatic tunnel setup failed.\")\n",
        "    print(\"Please run this command in a NEW Colab cell:\")\n",
        "    print(\"!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\")\n",
        "    print(\"!unzip ngrok-stable-linux-amd64.zip\")\n",
        "    print(\"!./ngrok http 8050\")\n",
        "    print(\"Then copy the https URL that appears\")\n",
        "\n",
        "    return False\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask user for CSV file path\n",
        "    csv_path = get_csv_filepath()\n",
        "\n",
        "    print(f\"\\nCreating dashboard with file: {csv_path}\")\n",
        "    print(\"Loading data and building networks...\")\n",
        "\n",
        "    app = create_optimized_dashboard(csv_path)\n",
        "\n",
        "    print(\"\\nDashboard created successfully!\")\n",
        "\n",
        "    # Setup automatic public tunnel with multiple fallbacks\n",
        "    tunnel_success = setup_simple_tunnel(8050)\n",
        "\n",
        "    if not tunnel_success:\n",
        "        print(f\"Local access: http://localhost:8050\")\n",
        "\n",
        "    print(\"\\nStarting dashboard server...\")\n",
        "    app.run(debug=False, host='0.0.0.0', port=8050)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok http 8050"
      ],
      "metadata": {
        "id": "G_74BGynGJ8p",
        "outputId": "dbe76464-af4a-494e-bc22-293c3449e13e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 12:53:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 75.2.60.68, 35.71.179.82, 99.83.220.108, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|75.2.60.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  18.8MB/s    in 0.7s    \n",
            "\n",
            "2025-07-28 12:53:48 (18.8 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Usage of ngrok requires a verified account and authtoken.\n",
            "\n",
            "Sign up for an account: https://dashboard.ngrok.com/signup\n",
            "Install your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "\n",
            "ERR_NGROK_4018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD8lYyPuTZWG",
        "outputId": "e546af53-a1d0-476d-8fc3-49026985d92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}