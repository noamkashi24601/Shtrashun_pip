{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXM1G9171lxi"
      },
      "source": [
        "# ◊ó◊ú◊ï◊ß◊ï◊™ ◊ó◊ë◊®◊™◊ô◊ï◊™ ◊©◊ú ◊ß◊ï◊®◊ê◊ô◊ù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYoh2u6k1s25"
      },
      "source": [
        "**◊í◊ë◊®◊ô◊ù ◊û◊ï◊ú ◊†◊©◊ô◊ù**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8J3Iu1b14SJ"
      },
      "source": [
        "◊î◊©◊™◊û◊©◊™◊ô ◊ë◊ê◊ô◊†◊ì◊ß◊° ◊©◊î◊ï◊°◊§◊†◊ï ◊ú◊ò◊ë◊ú◊ê◊ï◊™ ◊ë◊©◊ë◊ô◊ú ◊ú◊ó◊©◊ë ◊ï◊ú◊®◊ê◊ï◊™ ◊©◊ë◊õ◊ú ◊î◊ò◊ë◊ú◊ê◊ï◊™ ◊ô◊© 1586 ◊ê◊†◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù (◊ô◊© ◊ú◊¶◊ô◊ô◊ü ◊©◊î◊ê◊ô◊†◊ì◊ï◊ß◊° ◊î◊™◊¢◊ú◊ù ◊û◊î◊ê◊§◊©◊®◊ï◊™ ◊©◊ô◊© ◊ê◊†◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù ◊ë◊¢◊ú◊ô ◊ê◊ï◊™◊ï ◊î◊©◊ù ◊ê◊ï ◊ê◊†◊©◊ô◊ù ◊©◊†◊õ◊™◊ë◊ï ◊§◊¢◊ù ◊ë◊ê◊ï◊™◊ô◊ï◊™ ◊¢◊ë◊®◊ô◊ï◊™ ◊ï◊§◊¢◊ù ◊ë◊ê◊ï◊™◊ô◊ï◊™ ◊ß◊ô◊®◊ô◊ú◊ô◊ï◊™). ◊û◊™◊ï◊õ◊ù 72 ◊†◊©◊ô◊ù ◊ï◊î◊©◊ê◊® (1514) ◊í◊ë◊®◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊®◊ß 4.54% ◊î◊ü ◊©◊ï◊ê◊ú◊ï◊™. ◊ê◊ù ◊†◊°◊™◊õ◊ú ◊¢◊ú ◊õ◊û◊ï◊™ ◊î◊§◊¢◊û◊ô◊ù ◊©◊õ◊ú ◊ê◊ì◊ù ◊©◊ê◊ú ◊°◊§◊®, ◊†◊®◊ê◊î ◊©◊û◊™◊ï◊ö 5145 ◊©◊ê◊ô◊ú◊ï◊™ ◊®◊ß 100 ◊†◊¢◊©◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊î◊©◊ê◊ú◊ï◊™, 1.94% ◊û◊î◊°◊§◊®◊ô◊ù ◊†◊©◊ê◊ú◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊ë◊û◊û◊ï◊¶◊¢ ◊†◊©◊ô◊ù ◊©◊ê◊ú◊ï 1.39 ◊°◊§◊®◊ô◊ù ◊ú◊¢◊ï◊û◊™ ◊í◊ë◊®◊ô◊ù ◊©◊©◊ê◊ú◊ï 3.33 ◊°◊§◊®◊ô◊ù ◊ë◊û◊û◊ï◊¶◊¢.\n",
        "\n",
        "\n",
        "\n",
        "◊¢◊ë◊ï◊® ◊©◊†◊™ 1902: ◊ë◊õ◊ú ◊î◊†◊™◊ï◊†◊ô◊ù ◊©◊ú ◊©◊†◊î ◊ñ◊ï ◊ô◊©◊†◊ù 622 ◊ê◊†◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù. ◊û◊™◊ï◊õ◊ù 1 ◊†◊©◊ô◊ù ◊ï◊î◊©◊ê◊® (621) ◊í◊ë◊®◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊®◊ß\n",
        "0.16\n",
        " ◊î◊ü ◊©◊ï◊ê◊ú◊ï◊™. ◊ê◊ù ◊†◊°◊™◊õ◊ú ◊¢◊ú ◊õ◊û◊ï◊™ ◊î◊§◊¢◊û◊ô◊ù ◊©◊õ◊ú ◊ê◊ì◊ù ◊©◊ê◊ú ◊°◊§◊®, ◊†◊®◊ê◊î ◊©◊û◊™◊ï◊ö 23537 ◊©◊ê◊ô◊ú◊ï◊™ ◊®◊ß 2 ◊†◊¢◊©◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊î◊©◊ê◊ú◊ï◊™,\n",
        "0.01\n",
        " ◊û◊î◊°◊§◊®◊ô◊ù ◊†◊©◊ê◊ú◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊ë◊û◊û◊ï◊¶◊¢ ◊†◊©◊ô◊ù ◊©◊ê◊ú◊ï\n",
        "2.00\n",
        " ◊°◊§◊®◊ô◊ù ◊ú◊¢◊ï◊û◊™ ◊í◊ë◊®◊ô◊ù ◊©◊©◊ê◊ú◊ï\n",
        "37.84\n",
        " ◊°◊§◊®◊ô◊ù ◊ë◊û◊û◊ï◊¶◊¢.\n",
        "\n",
        "◊¢◊ë◊ï◊® ◊©◊†◊™ 1920: ◊ë◊õ◊ú ◊î◊†◊™◊ï◊†◊ô◊ù ◊©◊ú ◊©◊†◊î ◊ñ◊ï ◊ô◊©◊†◊ù 747 ◊ê◊†◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù. ◊û◊™◊ï◊õ◊ù 29 ◊†◊©◊ô◊ù ◊ï◊î◊©◊ê◊® (733) ◊í◊ë◊®◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊®◊ß\n",
        "3.88\n",
        " ◊î◊ü ◊©◊ï◊ê◊ú◊ï◊™. ◊ê◊ù ◊†◊°◊™◊õ◊ú ◊¢◊ú ◊õ◊û◊ï◊™ ◊î◊§◊¢◊û◊ô◊ù ◊©◊õ◊ú ◊ê◊ì◊ù ◊©◊ê◊ú ◊°◊§◊®, ◊†◊®◊ê◊î ◊©◊û◊™◊ï◊ö 14299 ◊©◊ê◊ô◊ú◊ï◊™ ◊®◊ß 42 ◊†◊¢◊©◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊î◊©◊ê◊ú◊ï◊™,\n",
        "0.29\n",
        " ◊û◊î◊°◊§◊®◊ô◊ù ◊†◊©◊ê◊ú◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊ë◊û◊û◊ï◊¶◊¢ ◊†◊©◊ô◊ù ◊©◊ê◊ú◊ï\n",
        "1.45\n",
        " ◊°◊§◊®◊ô◊ù ◊ú◊¢◊ï◊û◊™ ◊í◊ë◊®◊ô◊ù ◊©◊©◊ê◊ú◊ï\n",
        "19.45\n",
        " ◊°◊§◊®◊ô◊ù ◊ë◊û◊û◊ï◊¶◊¢.\n",
        "\n",
        "◊¢◊ë◊ï◊® ◊©◊†◊™ 1934: ◊ë◊õ◊ú ◊î◊†◊™◊ï◊†◊ô◊ù ◊©◊ú ◊©◊†◊î ◊ñ◊ï ◊ô◊©◊†◊ù 350 ◊ê◊†◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù. ◊û◊™◊ï◊õ◊ù 66 ◊†◊©◊ô◊ù ◊ï◊î◊©◊ê◊® (284) ◊í◊ë◊®◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊®◊ß\n",
        "18.86\n",
        " ◊î◊ü ◊©◊ï◊ê◊ú◊ï◊™. ◊ê◊ù ◊†◊°◊™◊õ◊ú ◊¢◊ú ◊õ◊û◊ï◊™ ◊î◊§◊¢◊û◊ô◊ù ◊©◊õ◊ú ◊ê◊ì◊ù ◊©◊ê◊ú ◊°◊§◊®, ◊†◊®◊ê◊î ◊©◊û◊™◊ï◊ö 367 ◊©◊ê◊ô◊ú◊ï◊™ ◊®◊ß 67 ◊†◊¢◊©◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊î◊©◊ê◊ú◊ï◊™,\n",
        "18.26\n",
        " ◊û◊î◊°◊§◊®◊ô◊ù ◊†◊©◊ê◊ú◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊ë◊û◊û◊ï◊¶◊¢ ◊†◊©◊ô◊ù ◊©◊ê◊ú◊ï\n",
        "1.02\n",
        " ◊°◊§◊®◊ô◊ù ◊ú◊¢◊ï◊û◊™ ◊í◊ë◊®◊ô◊ù ◊©◊©◊ê◊ú◊ï\n",
        "1.06\n",
        " ◊°◊§◊®◊ô◊ù ◊ë◊û◊û◊ï◊¶◊¢.\n",
        "\n",
        "◊¢◊ë◊ï◊® ◊©◊†◊™ 1940: ◊ë◊õ◊ú ◊î◊†◊™◊ï◊†◊ô◊ù ◊©◊ú ◊©◊†◊î ◊ñ◊ï ◊ô◊©◊†◊ù 109 ◊ê◊†◊©◊ô◊ù ◊©◊ï◊†◊ô◊ù. ◊û◊™◊ï◊õ◊ù 2 ◊†◊©◊ô◊ù ◊ï◊î◊©◊ê◊® (107) ◊í◊ë◊®◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊®◊ß\n",
        "1.83\n",
        " ◊î◊ü ◊©◊ï◊ê◊ú◊ï◊™. ◊ê◊ù ◊†◊°◊™◊õ◊ú ◊¢◊ú ◊õ◊û◊ï◊™ ◊î◊§◊¢◊û◊ô◊ù ◊©◊õ◊ú ◊ê◊ì◊ù ◊©◊ê◊ú ◊°◊§◊®, ◊†◊®◊ê◊î ◊©◊û◊™◊ï◊ö 208 ◊©◊ê◊ô◊ú◊ï◊™ ◊®◊ß 2 ◊†◊¢◊©◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊û◊î ◊©◊ê◊ï◊û◊® ◊©◊û◊™◊ï◊ö ◊õ◊ú◊ú ◊î◊î◊©◊ê◊ú◊ï◊™,\n",
        "0.96\n",
        " ◊û◊î◊°◊§◊®◊ô◊ù ◊†◊©◊ê◊ú◊ï ◊¢◊ú ◊ô◊ì◊ô ◊†◊©◊ô◊ù. ◊ë◊û◊û◊ï◊¶◊¢ ◊†◊©◊ô◊ù ◊©◊ê◊ú◊ï\n",
        "1.00\n",
        " ◊°◊§◊®◊ô◊ù ◊ú◊¢◊ï◊û◊™ ◊í◊ë◊®◊ô◊ù ◊©◊©◊ê◊ú◊ï\n",
        "1.93\n",
        " ◊°◊§◊®◊ô◊ù ◊ë◊û◊û◊ï◊¶◊¢."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G458vuhB19PW"
      },
      "source": [
        "◊¶◊¢◊ì◊ô◊ù ◊ú◊î◊û◊©◊ö:\n",
        "◊î◊ê◊ù ◊ô◊© ◊¶◊ï◊®◊ö ◊ë◊ú◊ó◊©◊ë ◊ê◊™ ◊õ◊ú ◊î◊†◊™◊ï◊†◊ô◊ù ◊î◊†◊¥◊ú ◊ú◊§◊ô ◊õ◊ú ◊ò◊ë◊ú◊î ◊ï◊ú◊ê ◊®◊ß ◊¢◊ú ◊õ◊ú ◊î◊†◊™◊ï◊†◊ô◊ù?\n",
        "\n",
        "◊ô◊î◊ô◊î ◊û◊¢◊†◊ô◊ô◊ü ◊ú◊ó◊©◊ë ◊ê◊™ ◊î◊î◊ë◊ì◊ú◊ô◊ù ◊î◊û◊í◊ì◊®◊ô◊ô◊ù ◊ë◊ô◊ó◊° ◊ú◊°◊ï◊í◊î? ◊ú◊û◊©◊ú, ◊õ◊û◊î ◊†◊©◊ô◊ù ◊©◊ê◊ú◊ï ◊°◊§◊®◊ô ◊ô◊î◊ì◊ï◊™ ◊ê◊ï ◊û◊î ◊î◊ó◊ú◊ï◊ß◊î ◊î◊û◊í◊ì◊®◊ô◊™ ◊©◊ú ◊©◊ê◊ô◊ú◊™ ◊°◊§◊®◊ô ◊§◊®◊ï◊ñ◊î.\n",
        "\n",
        "◊®◊ï◊¶◊ô◊ù ◊©◊ê◊†◊ô ◊ê◊ô◊ô◊¶◊® ◊í◊®◊§◊ô◊ù ◊©◊ú ◊î◊™◊ï◊¶◊ê◊ï◊™?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt7mnrRi2SKZ"
      },
      "source": [
        "**◊ë◊¢◊ú◊ô ◊©◊û◊ï◊™ ◊¢◊ë◊®◊ô◊ô◊ù / ◊ô◊ô◊ì◊©◊ê◊ô◊ô◊ù / ◊®◊ï◊°◊ô◊ô◊ù / ◊ê◊ó◊®◊ô◊ù**\n",
        "\n",
        "◊ô◊õ◊ï◊ú ◊ú◊î◊ô◊ï◊™ ◊¢◊ì◊ï◊™ ◊ú◊§◊®◊ß◊ò◊ô◊° ◊®◊ô◊©◊ï◊ù ◊ï◊ú◊ê ◊ú◊û◊ß◊ï◊ù ◊î◊ó◊ë◊®◊™◊ô ◊©◊ú ◊î◊ß◊ï◊®◊ê◊ô◊ù."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OT8_4LsD2b4T"
      },
      "source": [
        "◊ú◊§◊†◊ô ◊©◊ê◊™◊ó◊ô◊ú ◊ú◊¢◊ë◊ï◊ì ◊¢◊ú ◊ñ◊î ◊û◊°◊§◊® ◊©◊ê◊ú◊ï◊™:\n",
        "\n",
        "◊ê◊ô◊ö ◊ê◊†◊ó◊†◊ï ◊û◊í◊ì◊ô◊®◊ô◊ù ◊©◊ù ◊ë◊ô◊ô◊ì◊ô◊© ◊ï◊©◊ù ◊ë◊¢◊ë◊®◊ô◊™? ◊ó◊©◊ë◊™◊ô ◊ú◊†◊°◊ï◊™ ◊ú◊§◊ô ◊©◊ù ◊§◊®◊ò◊ô, ◊ê◊ë◊ú ◊î◊ê◊ù ◊ú◊ê◊î ◊ê◊ï ◊©◊®◊î ◊î◊ü ◊©◊û◊ï◊™ ◊ë◊¢◊ë◊®◊ô◊™?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3wDwUX12rQI"
      },
      "source": [
        "**◊©◊ô◊ô◊õ◊ï◊™ ◊ñ◊î◊ï◊™◊ô◊™**\n",
        "\n",
        "◊†◊í◊ñ◊®◊™ ◊û◊®◊©◊™◊ï◊™ ◊©◊†◊ú◊û◊ì◊ï◊™ ◊û◊™◊ï◊ö ◊î◊°◊§◊®◊ô◊ù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C0x-Zhi2u0C"
      },
      "source": [
        "◊ê◊©◊û◊ó ◊ú◊î◊°◊ë◊® ◊ú◊û◊î ◊î◊õ◊ï◊ï◊†◊î"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkEq8RtX28Uk"
      },
      "source": [
        "# ◊®◊©◊™◊ï◊™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pjZXtdn2_Y7"
      },
      "source": [
        "**◊ê◊†◊©◊ô◊ù ◊©◊ß◊ï◊®◊ê◊ô◊ù ◊ê◊ï◊™◊ù ◊°◊§◊®◊ô◊ù**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsCLj2DL3pMd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import networkx as nx\n",
        "import os\n",
        "\n",
        "# --- REQUIRED LIBRARIES ---\n",
        "# To run this script, you first need to install the required libraries.\n",
        "# Open a terminal or command prompt and run the following command:\n",
        "# pip install pandas networkx pyvis networkx[community]\n",
        "from pyvis.network import Network\n",
        "from networkx.algorithms import community\n",
        "\n",
        "def analyze_shared_books(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyzes borrower data to find people who read the same books,\n",
        "    generates CSVs, and creates interactive HTML network graphs\n",
        "    with community detection and enhanced user controls.\n",
        "    \"\"\"\n",
        "    # --- Set a threshold to only show stronger connections ---\n",
        "    MIN_SHARED_BOOKS = 2\n",
        "\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "        df_clean = df.dropna(subset=[\"person's name\", 'id']).copy()\n",
        "        df_clean['id'] = df_clean['id'].astype(int)\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid borrowing records.\")\n",
        "\n",
        "        # --- 2. Group by Book and Create Pairs ---\n",
        "        readers_per_book = df_clean.groupby('id')[\"person's name\"].apply(list)\n",
        "        all_pairs = []\n",
        "        for readers in readers_per_book:\n",
        "            unique_readers = sorted(list(set(readers)))\n",
        "            if len(unique_readers) > 1:\n",
        "                pairs = list(combinations(unique_readers, 2))\n",
        "                all_pairs.extend(pairs)\n",
        "        print(f\"Generated {len(all_pairs)} initial reader pairs.\")\n",
        "\n",
        "        # --- 3. Calculate Weights and Filter ---\n",
        "        if not all_pairs:\n",
        "            print(\"No shared books found. Exiting.\")\n",
        "            return\n",
        "        pair_counts = pd.DataFrame(all_pairs, columns=['Person1', 'Person2']).value_counts().reset_index()\n",
        "        pair_counts.columns = ['Person1', 'Person2', 'Shared_Book_Count']\n",
        "        filtered_pairs = pair_counts[pair_counts['Shared_Book_Count'] >= MIN_SHARED_BOOKS].copy()\n",
        "        print(f\"Filtered pairs: Kept {len(filtered_pairs)} connections with {MIN_SHARED_BOOKS} or more shared books.\")\n",
        "\n",
        "        # --- 4. Generate Connection CSV Files ---\n",
        "        filtered_pairs.to_csv('shared_book_connections_weighted.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Successfully generated weighted CSV file: shared_book_connections_weighted.csv\")\n",
        "\n",
        "        simple_pairs_df = filtered_pairs[['Person1', 'Person2']]\n",
        "        simple_pairs_df.to_csv('shared_book_connections_simple.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Successfully generated simple pairs CSV file: shared_book_connections_simple.csv\")\n",
        "\n",
        "        # --- 5. Create Graph, Detect Communities, and Calculate Layout ---\n",
        "        G = nx.from_pandas_edgelist(filtered_pairs, 'Person1', 'Person2', edge_attr='Shared_Book_Count')\n",
        "\n",
        "        print(\"Performing community detection...\")\n",
        "        communities_generator = community.louvain_communities(G, seed=42)\n",
        "        communities_list = list(communities_generator)\n",
        "\n",
        "        node_community = {}\n",
        "        for i, comm in enumerate(communities_list):\n",
        "            for node in comm:\n",
        "                node_community[node] = i\n",
        "\n",
        "        nx.set_node_attributes(G, node_community, 'community')\n",
        "        community_df = pd.DataFrame(list(node_community.items()), columns=['Person', 'Community_ID'])\n",
        "        community_df.to_csv('reader_communities.csv', index=False, encoding='utf-8-sig')\n",
        "        print(\"Successfully generated community data CSV: reader_communities.csv\")\n",
        "\n",
        "        # --- 6. Visualize the FULL INTERACTIVE Network with Pyvis ---\n",
        "        print(\"Generating main interactive network graph with all communities...\")\n",
        "        net_full = Network(\n",
        "            height='900px',\n",
        "            width='100%',\n",
        "            bgcolor='#222222',\n",
        "            font_color='white',\n",
        "            notebook=True,\n",
        "            cdn_resources='in_line'\n",
        "        )\n",
        "\n",
        "        # Use from_nx which handles positioning better\n",
        "        net_full.from_nx(G)\n",
        "\n",
        "        # Enhanced node customization\n",
        "        max_degree = max(dict(G.degree()).values()) if G.nodes() else 1\n",
        "        for node in net_full.nodes:\n",
        "            node_name = node['id']\n",
        "            community_id = node_community.get(node_name, 0)\n",
        "            degree = G.degree(node_name)\n",
        "\n",
        "            # Better size scaling\n",
        "            node['size'] = 10 + (degree / max_degree) * 30\n",
        "            node['group'] = community_id\n",
        "            node['title'] = f\"Name: {node_name}<br>Community: {community_id}<br>Connections: {degree}\"\n",
        "            node['font'] = {'size': 12, 'color': 'white'}\n",
        "\n",
        "        # Enhanced edge customization\n",
        "        for edge in net_full.edges:\n",
        "            # Make edge thickness proportional to shared book count\n",
        "            edge_data = G.get_edge_data(edge['from'], edge['to'])\n",
        "            shared_count = edge_data.get('Shared_Book_Count', 1)\n",
        "            edge['width'] = max(1, shared_count * 2)\n",
        "            edge['title'] = f\"Shared books: {shared_count}\"\n",
        "\n",
        "        # Better physics configuration for natural spreading\n",
        "        net_full.repulsion(node_distance=200, central_gravity=0.01, spring_length=200, spring_strength=0.05, damping=0.09)\n",
        "\n",
        "        # Add proper pyvis controls that integrate with the interface\n",
        "        net_full.show_buttons(filter_=['physics', 'interaction', 'layout', 'selection', 'renderer'])\n",
        "\n",
        "        # Configure the network with proper options for built-in controls\n",
        "        net_full.set_edge_smooth(False)  # Better performance with many edges\n",
        "\n",
        "        # Generate the HTML file with built-in pyvis controls\n",
        "        output_interactive_path = 'shared_book_network_interactive.html'\n",
        "        net_full.show(output_interactive_path)\n",
        "\n",
        "        # Add custom JavaScript functionality to work with pyvis controls\n",
        "        try:\n",
        "            with open(output_interactive_path, 'r', encoding='utf-8') as file:\n",
        "                html_content = file.read()\n",
        "\n",
        "            # JavaScript code as a simple string without complex formatting\n",
        "            js_code = \"\"\"\n",
        "<script type=\"text/javascript\">\n",
        "// Custom network analysis functionality\n",
        "var originalNodes = [];\n",
        "var originalEdges = [];\n",
        "var isHighlighting = false;\n",
        "\n",
        "function initCustomControls() {\n",
        "    if (typeof network !== 'undefined' && network.body && network.body.data) {\n",
        "        originalNodes = network.body.data.nodes.get();\n",
        "        originalEdges = network.body.data.edges.get();\n",
        "        addControlPanel();\n",
        "        setupNetworkEvents();\n",
        "        console.log('Custom controls initialized!');\n",
        "    } else {\n",
        "        setTimeout(initCustomControls, 500);\n",
        "    }\n",
        "}\n",
        "\n",
        "function addControlPanel() {\n",
        "    var panel = document.createElement('div');\n",
        "    panel.id = 'customControls';\n",
        "    panel.style.cssText = 'position:fixed;top:150px;left:10px;background:white;border:1px solid #ccc;border-radius:8px;padding:15px;z-index:1000;font-family:Arial;box-shadow:0 2px 10px rgba(0,0,0,0.1);max-width:280px;';\n",
        "\n",
        "    panel.innerHTML = '<h3 style=\"margin:0 0 15px 0;color:#333;\">üìä Network Analysis</h3>' +\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">üîç Search:</label>' +\n",
        "    '<input type=\"text\" id=\"searchBox\" placeholder=\"Enter name...\" style=\"width:150px;padding:5px;border:1px solid #ccc;border-radius:4px;\">' +\n",
        "    '<button onclick=\"searchNode()\" style=\"padding:5px 10px;margin-left:5px;background:#4CAF50;color:white;border:none;border-radius:4px;cursor:pointer;\">Go</button></div>' +\n",
        "\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">üìà Min Connections:</label>' +\n",
        "    '<input type=\"range\" id=\"connectionSlider\" min=\"1\" max=\"20\" value=\"1\" style=\"width:180px;\" onchange=\"filterConnections(this.value)\">' +\n",
        "    '<span id=\"connectionValue\" style=\"margin-left:10px;\">1</span></div>' +\n",
        "\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">üèòÔ∏è Community:</label>' +\n",
        "    '<select id=\"communitySelect\" onchange=\"filterCommunity(this.value)\" style=\"width:150px;padding:5px;border:1px solid #ccc;border-radius:4px;\">' +\n",
        "    '<option value=\"all\">All Communities</option></select></div>' +\n",
        "\n",
        "    '<div style=\"margin-bottom:10px;\"><label style=\"display:block;margin-bottom:5px;font-weight:bold;\">üîÑ Node Spacing:</label>' +\n",
        "    '<input type=\"range\" id=\"spacingSlider\" min=\"0\" max=\"2000\" value=\"200\" style=\"width:180px;\" onchange=\"adjustSpacing(this.value)\">' +\n",
        "    '<span id=\"spacingValue\" style=\"margin-left:10px;\">200</span></div>' +\n",
        "\n",
        "    '<div style=\"margin:15px 0;\"><button onclick=\"resetAll()\" style=\"padding:8px 15px;margin-right:5px;background:#2196F3;color:white;border:none;border-radius:4px;cursor:pointer;\">üîÑ Reset</button>' +\n",
        "    '<button onclick=\"stabilize()\" style=\"padding:8px 15px;background:#FF9800;color:white;border:none;border-radius:4px;cursor:pointer;\">‚ö° Stabilize</button></div>' +\n",
        "\n",
        "    '<div id=\"statusPanel\" style=\"margin-top:15px;padding:10px;background:#f5f5f5;border-radius:4px;font-size:12px;\">' +\n",
        "    '<strong>üí° Instructions:</strong><br>‚Ä¢ Click nodes to highlight connections<br>‚Ä¢ Double-click to focus on a node<br>‚Ä¢ Use sliders to filter and adjust layout</div>';\n",
        "\n",
        "    document.body.appendChild(panel);\n",
        "    populateCommunities();\n",
        "}\n",
        "\n",
        "function populateCommunities() {\n",
        "    var communities = new Set();\n",
        "    originalNodes.forEach(function(node) {\n",
        "        if (node.group !== undefined) {\n",
        "            communities.add(node.group);\n",
        "        }\n",
        "    });\n",
        "\n",
        "    var select = document.getElementById('communitySelect');\n",
        "    Array.from(communities).sort(function(a,b){return a-b;}).forEach(function(community) {\n",
        "        var option = document.createElement('option');\n",
        "        option.value = community;\n",
        "        option.textContent = 'Community ' + community;\n",
        "        select.appendChild(option);\n",
        "    });\n",
        "}\n",
        "\n",
        "function setupNetworkEvents() {\n",
        "    network.on('click', function(params) {\n",
        "        if (params.nodes.length > 0) {\n",
        "            highlightNode(params.nodes[0]);\n",
        "        } else if (isHighlighting) {\n",
        "            resetHighlight();\n",
        "        }\n",
        "    });\n",
        "\n",
        "    network.on('doubleClick', function(params) {\n",
        "        if (params.nodes.length > 0) {\n",
        "            focusNode(params.nodes[0]);\n",
        "        }\n",
        "    });\n",
        "}\n",
        "\n",
        "function searchNode() {\n",
        "    var searchTerm = document.getElementById('searchBox').value.toLowerCase().trim();\n",
        "    if (!searchTerm) {\n",
        "        alert('Please enter a search term');\n",
        "        return;\n",
        "    }\n",
        "\n",
        "    var node = originalNodes.find(function(n) {\n",
        "        return (n.label && n.label.toLowerCase().includes(searchTerm)) ||\n",
        "               (n.id && n.id.toString().toLowerCase().includes(searchTerm));\n",
        "    });\n",
        "\n",
        "    if (node) {\n",
        "        focusNode(node.id);\n",
        "        highlightNode(node.id);\n",
        "        updateStatus('Found: ' + (node.label || node.id));\n",
        "    } else {\n",
        "        alert('Node \"' + searchTerm + '\" not found');\n",
        "    }\n",
        "}\n",
        "\n",
        "function focusNode(nodeId) {\n",
        "    network.focus(nodeId, {\n",
        "        scale: 1.5,\n",
        "        animation: {\n",
        "            duration: 1000,\n",
        "            easingFunction: 'easeInOutQuad'\n",
        "        }\n",
        "    });\n",
        "}\n",
        "\n",
        "function highlightNode(nodeId) {\n",
        "    var connectedNodes = network.getConnectedNodes(nodeId);\n",
        "    var connectedEdges = network.getConnectedEdges(nodeId);\n",
        "\n",
        "    var updatedNodes = originalNodes.map(function(node) {\n",
        "        var newNode = Object.assign({}, node);\n",
        "        if (node.id === nodeId) {\n",
        "            newNode.color = {background: '#ff0000', border: '#cc0000'};\n",
        "            newNode.borderWidth = 4;\n",
        "            newNode.size = (newNode.size || 15) * 1.5;\n",
        "        } else if (connectedNodes.includes(node.id)) {\n",
        "            newNode.color = {background: '#ffa500', border: '#ff8c00'};\n",
        "            newNode.borderWidth = 2;\n",
        "            newNode.size = (newNode.size || 15) * 1.2;\n",
        "        } else {\n",
        "            newNode.color = {background: '#d3d3d3', border: '#a9a9a9'};\n",
        "            newNode.opacity = 0.3;\n",
        "        }\n",
        "        return newNode;\n",
        "    });\n",
        "\n",
        "    var updatedEdges = originalEdges.map(function(edge) {\n",
        "        var newEdge = Object.assign({}, edge);\n",
        "        if (connectedEdges.includes(edge.id)) {\n",
        "            newEdge.color = {color: '#ff0000'};\n",
        "            newEdge.width = (newEdge.width || 1) * 3;\n",
        "        } else {\n",
        "            newEdge.opacity = 0.1;\n",
        "        }\n",
        "        return newEdge;\n",
        "    });\n",
        "\n",
        "    network.body.data.nodes.update(updatedNodes);\n",
        "    network.body.data.edges.update(updatedEdges);\n",
        "    isHighlighting = true;\n",
        "\n",
        "    var node = originalNodes.find(function(n) { return n.id === nodeId; });\n",
        "    if (node) {\n",
        "        updateStatus('Highlighting: ' + (node.label || node.id) + ' (' + connectedNodes.length + ' connections)');\n",
        "    }\n",
        "}\n",
        "\n",
        "function resetHighlight() {\n",
        "    network.body.data.nodes.update(originalNodes);\n",
        "    network.body.data.edges.update(originalEdges);\n",
        "    isHighlighting = false;\n",
        "    updateStatus('Highlight reset');\n",
        "}\n",
        "\n",
        "function filterConnections(minConnections) {\n",
        "    document.getElementById('connectionValue').textContent = minConnections;\n",
        "\n",
        "    var filteredNodes = originalNodes.filter(function(node) {\n",
        "        var connections = network.getConnectedNodes(node.id);\n",
        "        return connections.length >= parseInt(minConnections);\n",
        "    });\n",
        "\n",
        "    var filteredNodeIds = new Set(filteredNodes.map(function(n) { return n.id; }));\n",
        "    var filteredEdges = originalEdges.filter(function(edge) {\n",
        "        return filteredNodeIds.has(edge.from) && filteredNodeIds.has(edge.to);\n",
        "    });\n",
        "\n",
        "    // Clear and add filtered data\n",
        "    network.body.data.nodes.clear();\n",
        "    network.body.data.edges.clear();\n",
        "    network.body.data.nodes.add(filteredNodes);\n",
        "    network.body.data.edges.add(filteredEdges);\n",
        "\n",
        "    // Fit the view to the filtered nodes\n",
        "    network.fit();\n",
        "\n",
        "    updateStatus('Showing ' + filteredNodes.length + ' nodes with ' + minConnections + '+ connections');\n",
        "}\n",
        "\n",
        "function filterCommunity(communityId) {\n",
        "    if (communityId === 'all') {\n",
        "        // Show all nodes and edges\n",
        "        network.body.data.nodes.update(originalNodes);\n",
        "        network.body.data.edges.update(originalEdges);\n",
        "        updateStatus('Showing all communities');\n",
        "    } else {\n",
        "        // Filter to show only nodes from the selected community\n",
        "        var filteredNodes = originalNodes.filter(function(node) {\n",
        "            return node.group == communityId;\n",
        "        });\n",
        "\n",
        "        // Only show edges between nodes in the selected community\n",
        "        var filteredNodeIds = new Set(filteredNodes.map(function(n) { return n.id; }));\n",
        "        var filteredEdges = originalEdges.filter(function(edge) {\n",
        "            return filteredNodeIds.has(edge.from) && filteredNodeIds.has(edge.to);\n",
        "        });\n",
        "\n",
        "        // Clear the network first, then add only the filtered nodes and edges\n",
        "        network.body.data.nodes.clear();\n",
        "        network.body.data.edges.clear();\n",
        "\n",
        "        // Add the filtered nodes and edges\n",
        "        network.body.data.nodes.add(filteredNodes);\n",
        "        network.body.data.edges.add(filteredEdges);\n",
        "\n",
        "        // Fit the view to the filtered community\n",
        "        network.fit();\n",
        "\n",
        "        updateStatus('Showing Community ' + communityId + ' (' + filteredNodes.length + ' nodes)');\n",
        "    }\n",
        "}\n",
        "\n",
        "function adjustSpacing(spacing) {\n",
        "    document.getElementById('spacingValue').textContent = spacing;\n",
        "\n",
        "    var options = {\n",
        "        physics: {\n",
        "            enabled: true,\n",
        "            repulsion: {\n",
        "                nodeDistance: parseInt(spacing),\n",
        "                centralGravity: 0.01,\n",
        "                springLength: parseInt(spacing) * 0.8,\n",
        "                springConstant: 0.05,\n",
        "                damping: 0.09\n",
        "            }\n",
        "        }\n",
        "    };\n",
        "\n",
        "    network.setOptions(options);\n",
        "    updateStatus('Node spacing adjusted to ' + spacing);\n",
        "}\n",
        "\n",
        "function resetAll() {\n",
        "    // Clear and restore all nodes and edges\n",
        "    network.body.data.nodes.clear();\n",
        "    network.body.data.edges.clear();\n",
        "    network.body.data.nodes.add(originalNodes);\n",
        "    network.body.data.edges.add(originalEdges);\n",
        "    network.fit();\n",
        "\n",
        "    document.getElementById('connectionSlider').value = 1;\n",
        "    document.getElementById('connectionValue').textContent = '1';\n",
        "    document.getElementById('communitySelect').value = 'all';\n",
        "    document.getElementById('searchBox').value = '';\n",
        "    document.getElementById('spacingSlider').value = 200;\n",
        "    document.getElementById('spacingValue').textContent = '200';\n",
        "\n",
        "    isHighlighting = false;\n",
        "    updateStatus('All filters reset');\n",
        "}\n",
        "\n",
        "function stabilize() {\n",
        "    network.stabilize();\n",
        "    updateStatus('Network stabilized');\n",
        "}\n",
        "\n",
        "function updateStatus(message) {\n",
        "    var panel = document.getElementById('statusPanel');\n",
        "    if (panel) {\n",
        "        panel.innerHTML = '<strong>üí° Status:</strong><br>' + message;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Initialize when page loads\n",
        "setTimeout(initCustomControls, 1000);\n",
        "</script>\n",
        "</body>\"\"\"\n",
        "\n",
        "            # Insert before closing body tag\n",
        "            html_content = html_content.replace('</body>', js_code)\n",
        "\n",
        "            with open(output_interactive_path, 'w', encoding='utf-8') as file:\n",
        "                file.write(html_content)\n",
        "\n",
        "            print(f\"Successfully generated enhanced interactive HTML graph: {output_interactive_path}\")\n",
        "            print(\"üéâ Custom controls integrated with pyvis interface!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not add custom controls: {e}\")\n",
        "            print(f\"Basic interactive graph available at: {output_interactive_path}\")\n",
        "\n",
        "        # --- 7. Generate a separate graph for each community ---\n",
        "        print(\"\\nGenerating individual graphs for each community...\")\n",
        "        if not os.path.exists('communities'):\n",
        "            os.makedirs('communities')\n",
        "\n",
        "        for i, comm_nodes in enumerate(communities_list):\n",
        "            if not comm_nodes: continue\n",
        "            subgraph = G.subgraph(comm_nodes)\n",
        "\n",
        "            net_comm = Network(\n",
        "                height='800px',\n",
        "                width='100%',\n",
        "                bgcolor='#333333',\n",
        "                font_color='white',\n",
        "                notebook=True,\n",
        "                cdn_resources='in_line',\n",
        "                heading=f'Community {i}'\n",
        "            )\n",
        "            net_comm.from_nx(subgraph)\n",
        "\n",
        "            # Enhanced community graph styling\n",
        "            for node in net_comm.nodes:\n",
        "                node['title'] = f\"Name: {node['label']}<br>Connections: {subgraph.degree(node['id'])}\"\n",
        "                node['size'] = 15 + subgraph.degree(node['id']) * 3\n",
        "                node['font'] = {'size': 14, 'color': 'white'}\n",
        "\n",
        "            # Better physics for smaller community graphs\n",
        "            net_comm.repulsion(node_distance=120, central_gravity=0.01, spring_length=150, spring_strength=0.08, damping=0.2)\n",
        "\n",
        "            net_comm.show_buttons(filter_=['physics'])\n",
        "\n",
        "            comm_file_path = f'communities/community_{i}.html'\n",
        "            net_comm.show(comm_file_path)\n",
        "            print(f\"  - Created community graph: {comm_file_path}\")\n",
        "\n",
        "        print(\"\\nAnalysis complete. Open the HTML files in your browser to explore.\")\n",
        "        print(\"The graph will initially spread out naturally. You can:\")\n",
        "        print(\"- Use the physics controls to adjust the layout\")\n",
        "        print(\"- Drag nodes to reposition them\")\n",
        "        print(\"- Use mouse wheel to zoom\")\n",
        "        print(\"- Click and drag to pan around\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Run the analysis\n",
        "analyze_shared_books()\n",
        "\n",
        "def create_daily_borrower_list(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Loads borrower data, cleans it, and creates a simple CSV file\n",
        "    listing all borrowers with their ID, sorted chronologically by their visit date.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Drop rows where person's name, date, or ID is missing, as they are essential.\n",
        "        df_clean = df.dropna(subset=[\"person's name\", 'date', 'index_name']).copy()\n",
        "\n",
        "        # Convert the 'date' column to a standardized datetime format.\n",
        "        # `errors='coerce'` will handle various formats and turn any un-parseable\n",
        "        # dates into 'NaT' (Not a Time), which we can then remove.\n",
        "        df_clean['date_parsed'] = pd.to_datetime(df_clean['date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "        # Convert the 'index_name' to a clean integer format.\n",
        "        df_clean['index_name'] = df_clean['index_name'].astype(int)\n",
        "\n",
        "        # Remove any rows where the date could not be successfully parsed.\n",
        "        df_clean = df_clean.dropna(subset=['date_parsed'])\n",
        "\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid visit records.\")\n",
        "\n",
        "        # --- 2. Sort by Date ---\n",
        "        # Sort the DataFrame chronologically based on the new 'date_parsed' column.\n",
        "        sorted_df = df_clean.sort_values(by='date_parsed')\n",
        "\n",
        "        # --- 3. Select Relevant Columns ---\n",
        "        # Create a final, clean list with the date, person's ID, and person's name.\n",
        "        output_df = sorted_df[['date_parsed', 'index_name', \"person's name\"]]\n",
        "\n",
        "        # --- 4. Save to CSV ---\n",
        "        output_csv_path = 'daily_borrower_list.csv'\n",
        "        output_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nSuccessfully created the file: {output_csv_path}\")\n",
        "        print(\"This file contains a list of all borrowers with their IDs, ordered by date.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Run the analysis\n",
        "create_daily_borrower_list()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE7vgCS8cVUD"
      },
      "source": [
        "◊î◊ß◊ï◊ì ◊ú◊¢◊ô◊ú ◊î◊™◊û◊ß◊ì ◊ë◊†◊ô◊™◊ï◊ó ◊®◊©◊™◊ï◊™ ◊ó◊ë◊®◊™◊ô◊ï◊™ ◊ë◊ô◊ü ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊©◊†◊û◊¶◊ê◊ô◊ù ◊ê◊¶◊ú◊†◊ï ◊ë◊ì◊ê◊ò◊ê, ◊ë◊ì◊í◊© ◊¢◊ú ◊ê◊†◊©◊ô◊ù ◊©◊©◊ê◊ú◊ï ◊ê◊™ ◊ê◊ï◊™◊ù ◊°◊§◊®◊ô◊ù.\n",
        "\n",
        "◊î◊ß◊ë◊¶◊ô◊ù ◊©◊î◊ï◊ê ◊ô◊¶◊® ◊î◊ù: -\n",
        "\n",
        "\n",
        "[◊®◊©◊ô◊û◊î ◊©◊ú ◊õ◊ú ◊ñ◊ï◊í ◊©◊ï◊ê◊ú◊ô◊ù ◊¢◊ù ◊û◊°◊§◊® ◊î◊°◊§◊®◊ô◊ù ◊î◊û◊©◊ï◊™◊§◊ô◊ù ◊©◊î◊ù ◊©◊ê◊ú◊ï](https://docs.google.com/spreadsheets/d/1l-pr-9JMPwCErBZJE1wSDQPbZWZpTcHMgSPjzYwPJdU/edit?usp=sharing)\n",
        "\n",
        "[◊®◊©◊ô◊û◊î ◊©◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊û◊°◊ï◊ì◊®◊™ ◊ú◊§◊ô ◊™◊ê◊®◊ô◊õ◊ô◊ù](https://docs.google.com/spreadsheets/d/1kyJ-I3jG0KqPFgz_6VGx_vCKRWz4s8DG5uzORHebeYw/edit?usp=sharing)\n",
        "\n",
        "[◊®◊©◊ô◊û◊î ◊©◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù ◊û◊ó◊ï◊ú◊ß◊ô◊ù ◊ú◊ß◊î◊ô◊ú◊ï◊™](https://docs.google.com/spreadsheets/d/1RCfHlzG3QziYMq7E43GBKHrbIC_R8Ed3we8FP8GidjA/edit?usp=sharing)\n",
        "\n",
        "◊î◊õ◊ï◊ï◊†◊î ◊ë◊ß◊î◊ô◊ú◊ï◊™ ◊î◊ô◊ê ◊ê◊†◊©◊ô◊ù ◊©◊ô◊ï◊™◊® ◊û◊ß◊ï◊©◊®◊ô◊ù ◊ê◊ó◊ì ◊ú◊©◊†◊ô  - ◊ó◊ï◊ú◊ß◊ô◊ù ◊ê◊™ ◊ê◊ï◊™◊ù ◊°◊§◊®◊ô◊ù, ◊û◊ê◊©◊® ◊ú◊©◊ê◊® ◊î◊ê◊†◊©◊ô◊ù ◊ë◊ì◊ê◊ò◊ê.\n",
        "\n",
        "[◊ß◊ï◊ë◊• ◊ê◊ô◊†◊ò◊®◊ê◊ß◊ò◊ô◊ë◊ô ◊ú◊†◊ô◊™◊ï◊ó ◊í◊®◊£ ◊ß◊©◊®◊ô◊ù ◊©◊ú ◊î◊©◊ï◊ê◊ú◊ô◊ù](https://drive.google.com/file/d/1NQCWZhGMjaz-Zs_RmqP3V7Rs9ToTGtCv/view?usp=drive_link)\n",
        "\n",
        "◊ë◊©◊ë◊ô◊ú ◊ú◊î◊©◊™◊û◊© ◊ë◊ß◊ï◊ë◊• ◊ñ◊î, ◊¶◊®◊ô◊ö ◊ú◊§◊™◊ï◊ó ◊ê◊ï◊™◊ï ◊ë◊ì◊®◊ô◊ô◊ë ◊ï◊ú◊î◊ï◊®◊ô◊ì ◊ê◊ï◊™◊ï ◊ú◊û◊ó◊©◊ë. ◊ú◊ê◊ó◊® ◊û◊õ◊ü ◊î◊ï◊ê ◊ô◊§◊™◊ó ◊ë◊ì◊§◊ì◊§◊ü ◊©◊ú◊õ◊ù.\n",
        "◊ë◊ß◊ï◊ë◊• ◊ê◊§◊©◊® ◊ú◊î◊í◊ì◊ô◊ú ◊ï◊ú◊î◊ß◊ò◊ô◊ü ◊ê◊™ ◊î◊û◊®◊ó◊ß◊ô◊ù ◊ë◊ô◊ü ◊î◊¶◊û◊™◊ô◊ù, ◊ë◊©◊ë◊ô◊ú ◊ú◊®◊ê◊ï◊™ ◊ô◊ï◊™◊® ◊ò◊ï◊ë ◊ê◊™ ◊î◊ó◊ú◊ß◊ô◊ù ◊î◊û◊¢◊†◊ô◊ô◊†◊ô◊ù ◊ë◊í◊®◊£, ◊ú◊ë◊ó◊ï◊® ◊©◊ï◊ê◊ú ◊ë◊ï ◊ê◊™◊ù ◊®◊ï◊¶◊ô◊ù ◊ú◊î◊™◊û◊ß◊ì, ◊ú◊ë◊ó◊ï◊® ◊ß◊î◊ô◊ú◊î ◊ê◊ï◊™◊î ◊ê◊™◊ù ◊®◊ï◊¶◊ô◊ù ◊ú◊†◊™◊ó ◊ï◊õ◊û◊ï◊™ ◊ß◊©◊®◊ô◊ù ◊û◊ô◊†◊ô◊û◊ê◊ú◊ô◊™.\n",
        "◊ê◊ù ◊™◊¶◊ë◊ô◊¢◊ï ◊¢◊ú ◊¶◊ï◊û◊™ ◊¢◊ù ◊î◊¢◊õ◊ë◊®, ◊ô◊ï◊§◊ô◊¢ ◊ú◊õ◊ù ◊§◊ï◊§ ◊ê◊§ ◊¢◊ù ◊©◊ù ◊î◊©◊ï◊ê◊ú, ◊û◊°◊§◊® ◊î◊ß◊©◊®◊ô◊ù ◊ï◊û◊ê◊ô◊ñ◊î ◊ß◊î◊ô◊ú◊î ◊î◊ï◊ê.\n",
        "\n",
        "◊î◊ß◊ï◊ë◊• ◊ß◊¶◊™ ◊õ◊ë◊ì, ◊ê◊ñ ◊ô◊ô◊™◊õ◊ü ◊©◊ô◊ß◊ó ◊ß◊¶◊™ ◊ñ◊û◊ü ◊¢◊ì ◊©◊î◊ï◊ê ◊ô◊ï◊§◊ô◊¢ ◊ï◊ú◊§◊¢◊û◊ô◊ù ◊§◊¢◊ï◊ú◊ï◊™ ◊©◊™◊¢◊©◊ï ◊ô◊í◊®◊û◊ï ◊ú◊í◊®◊£ ◊ú◊®◊¢◊ï◊ì ◊ß◊¶◊™ ◊ï◊ô◊ß◊ó ◊ú◊ï ◊õ◊û◊î ◊©◊†◊ô◊ï◊™ ◊ú◊î◊™◊ô◊ô◊¶◊ë."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4p4xszItcFk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def track_book_trends(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyzes borrower data to track the popularity of each book over all years.\n",
        "    It creates a pivot table CSV showing borrow counts for each book per year.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the borrower data CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Drop rows where essential information is missing.\n",
        "        df_clean = df.dropna(subset=['date', 'book name', 'id']).copy()\n",
        "\n",
        "        # Convert 'date' to datetime objects, coercing errors.\n",
        "        df_clean['date_parsed'] = pd.to_datetime(df_clean['date'], dayfirst=True, errors='coerce')\n",
        "\n",
        "        # Ensure book ID is a clean integer.\n",
        "        df_clean['id'] = df_clean['id'].astype(int)\n",
        "\n",
        "        # Drop rows where date parsing failed.\n",
        "        df_clean = df_clean.dropna(subset=['date_parsed'])\n",
        "\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid borrowing records.\")\n",
        "\n",
        "        # --- 2. Extract Year ---\n",
        "        df_clean['year'] = df_clean['date_parsed'].dt.year\n",
        "\n",
        "        # --- 3. Calculate Annual Popularity for All Books ---\n",
        "        # Group by year, book id, and book name, then count the size of each group.\n",
        "        yearly_counts = df_clean.groupby(['year', 'id', 'book name']).size().reset_index(name='borrow_count')\n",
        "\n",
        "        print(\"Calculated borrow counts for all books across all years.\")\n",
        "\n",
        "        # --- 4. Create Pivot Table ---\n",
        "        # This will transform the data so that:\n",
        "        # - Each book (id and name) is a row.\n",
        "        # - Each year is a column.\n",
        "        # - The values are the number of times the book was borrowed in that year.\n",
        "        # 'fill_value=0' ensures that if a book wasn't borrowed in a year, it shows a 0 instead of being blank.\n",
        "        trends_pivot = yearly_counts.pivot_table(\n",
        "            index=['id', 'book name'],\n",
        "            columns='year',\n",
        "            values='borrow_count',\n",
        "            fill_value=0\n",
        "        )\n",
        "\n",
        "        print(\"Created a pivot table to show trends over time.\")\n",
        "\n",
        "        # --- 5. Save to CSV ---\n",
        "        output_csv_path = 'book_trends_over_time.csv'\n",
        "        trends_pivot.to_csv(output_csv_path, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nSuccessfully created the file: {output_csv_path}\")\n",
        "        print(\"This file contains the borrow count for every book for every year.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Run the trend analysis.\n",
        "track_book_trends()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_PFRcTvO7o"
      },
      "source": [
        "◊î◊ß◊ï◊ì ◊ú◊¢◊ô◊ú ◊ô◊¶◊® ◊ò◊ë◊ú◊™ ◊§◊ô◊ë◊ï◊ò ◊©◊ú [◊õ◊û◊ï◊™ ◊î◊§◊¢◊û◊ô◊ù ◊©◊©◊ê◊ú◊ï ◊°◊§◊® ◊ú◊ê◊ï◊®◊ö ◊î◊©◊†◊ô◊ù](https://docs.google.com/spreadsheets/d/1H9VmIlKmtE74x93art9RbmM3TsnWvUa5euiUIe90Ido/edit?usp=sharing)\n",
        "\n",
        "\n",
        "◊©◊ô◊û◊ï ◊ú◊ë ◊©◊ô◊© ◊î◊ë◊ì◊ú◊ô◊ù ◊ë◊õ◊†◊ï◊™ ◊î◊†◊™◊ï◊†◊ô◊ù ◊©◊ô◊© ◊ú◊†◊ï ◊ë◊õ◊ú ◊©◊†◊î, ◊û◊î ◊©◊ô◊õ◊ï◊ú ◊ú◊ô◊¶◊ï◊® ◊î◊ò◊ô◊î ◊ë◊ë◊ì◊ô◊ß◊î ◊ñ◊ï"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKIgp7gj0a4w"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_person_folder_matrix(filepath='borrowers_data.csv'):\n",
        "    \"\"\"\n",
        "    Analyzes borrower data to create a matrix showing which people appear\n",
        "    in which folders.\n",
        "\n",
        "    The output CSV will have:\n",
        "    - A row for each unique person.\n",
        "    - A column for the person's ID and name.\n",
        "    - A column counting the total number of unique folders they appear in.\n",
        "    - A column for every unique folder, with a 1 if the person is present, 0 otherwise.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The path to the borrower data CSV file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- 1. Load and Clean Data ---\n",
        "        df = pd.read_csv(filepath)\n",
        "\n",
        "        # Drop rows where the essential columns are missing.\n",
        "        # NOTE: Only check for name and folder - index_name is only available for some folders\n",
        "        df_clean = df.dropna(subset=[\"person's name\", 'Folder']).copy()\n",
        "\n",
        "        print(f\"Loaded and cleaned data. Found {len(df_clean)} valid records with folder information.\")\n",
        "\n",
        "        # --- 2. Create a consistent person identifier ---\n",
        "        # Since index_name is only available for Vol_1_1902, we'll create our own unique identifier\n",
        "        # based on the person's name and handle the existing index_name where available\n",
        "\n",
        "        # First, let's see which records have index_name\n",
        "        has_index = df_clean['index_name'].notna()\n",
        "        print(f\"Records with index_name: {has_index.sum()}\")\n",
        "        print(f\"Records without index_name: {(~has_index).sum()}\")\n",
        "\n",
        "        # Create a unique person identifier\n",
        "        # For records with index_name, use it. For others, create a new sequential ID\n",
        "        df_clean = df_clean.copy()\n",
        "\n",
        "        # Get the maximum existing index_name to continue numbering from there\n",
        "        max_existing_index = df_clean['index_name'].max() if df_clean['index_name'].notna().any() else -1\n",
        "        max_existing_index = int(max_existing_index) if pd.notna(max_existing_index) else -1\n",
        "\n",
        "        # Create a mapping for people without index_name\n",
        "        people_without_index = df_clean[df_clean['index_name'].isna()][\"person's name\"].unique()\n",
        "\n",
        "        # Assign new index numbers to people without existing index_name\n",
        "        new_index_mapping = {}\n",
        "        current_index = max_existing_index + 1\n",
        "\n",
        "        for person in people_without_index:\n",
        "            new_index_mapping[person] = current_index\n",
        "            current_index += 1\n",
        "\n",
        "        # Fill in the missing index_name values\n",
        "        mask = df_clean['index_name'].isna()\n",
        "        df_clean.loc[mask, 'index_name'] = df_clean.loc[mask, \"person's name\"].map(new_index_mapping)\n",
        "\n",
        "        # Ensure index_name is integer\n",
        "        df_clean['index_name'] = df_clean['index_name'].astype(int)\n",
        "\n",
        "        print(f\"Assigned unique IDs to all people. Total unique people: {df_clean['index_name'].nunique()}\")\n",
        "\n",
        "        # --- 3. Handle Multiple Folders in a Single Cell (Robustly) ---\n",
        "        # Convert 'Folder' column to string type to handle any non-string values safely.\n",
        "        df_clean['Folder'] = df_clean['Folder'].astype(str)\n",
        "\n",
        "        # Split the 'Folder' string by a comma into a list of folder names.\n",
        "        df_clean['Folder_list'] = df_clean['Folder'].str.split(r',')\n",
        "\n",
        "        # Explode the DataFrame on the new list column. Each folder for a person now gets its own row.\n",
        "        df_exploded = df_clean.explode('Folder_list')\n",
        "\n",
        "        # Clean up each individual folder name after exploding.\n",
        "        df_exploded['Folder'] = df_exploded['Folder_list'].str.strip()\n",
        "        df_exploded['Folder'] = df_exploded['Folder'].str.strip('\\'\"')\n",
        "\n",
        "        # Remove any rows that might be empty after stripping\n",
        "        df_exploded.dropna(subset=['Folder'], inplace=True)\n",
        "        df_exploded = df_exploded[df_exploded['Folder'] != '']\n",
        "\n",
        "        print(\"Processed records to handle multiple folders per entry and cleaned folder names.\")\n",
        "\n",
        "        # Print folder distribution\n",
        "        folder_counts = df_exploded['Folder'].value_counts()\n",
        "        print(f\"\\nFolder distribution:\")\n",
        "        for folder, count in folder_counts.items():\n",
        "            print(f\"  {folder}: {count} records\")\n",
        "\n",
        "        # --- 4. Create the Person-Folder Matrix ---\n",
        "        # We use crosstab on the exploded and cleaned data to create a frequency table.\n",
        "        person_folder_crosstab = pd.crosstab(\n",
        "            df_exploded['index_name'],\n",
        "            df_exploded['Folder']\n",
        "        )\n",
        "\n",
        "        # Convert the counts to a binary format (1 for present, 0 for absent).\n",
        "        person_folder_matrix = (person_folder_crosstab > 0).astype(int)\n",
        "\n",
        "        print(f\"\\nCreated a matrix for the following folders: {person_folder_matrix.columns.tolist()}\")\n",
        "\n",
        "        # --- 5. Calculate Folder Count ---\n",
        "        # Sum across the rows to get the total number of unique folders for each person.\n",
        "        person_folder_matrix['folder_count'] = person_folder_matrix.sum(axis=1)\n",
        "\n",
        "        # --- 6. Add Person's Name ---\n",
        "        # Create a unique mapping of ID to name from the original cleaned data.\n",
        "        name_map = df_clean[['index_name', \"person's name\"]].drop_duplicates().set_index('index_name')\n",
        "\n",
        "        # Join the names to our matrix.\n",
        "        final_matrix = name_map.join(person_folder_matrix, on='index_name')\n",
        "\n",
        "        # Reset the index to bring 'index_name' back as a column.\n",
        "        final_matrix.reset_index(inplace=True)\n",
        "\n",
        "        # --- 7. Format and Save the Final CSV ---\n",
        "        # Reorder columns to put the count first for better readability.\n",
        "        folder_columns = sorted([col for col in person_folder_crosstab.columns])\n",
        "        final_columns = ['index_name', \"person's name\", 'folder_count'] + folder_columns\n",
        "        final_matrix = final_matrix[final_columns]\n",
        "\n",
        "        # Sort by the folder count to see the most active people at the top.\n",
        "        final_matrix.sort_values(by='folder_count', ascending=False, inplace=True)\n",
        "\n",
        "        output_csv_path = 'person_folder_matrix.csv'\n",
        "        final_matrix.to_csv(output_csv_path, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"\\nSuccessfully created the file: {output_csv_path}\")\n",
        "        print(\"This file contains the matrix of people and their folder appearances.\")\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(f\"\\nSummary Statistics:\")\n",
        "        print(f\"Total unique people: {len(final_matrix)}\")\n",
        "        print(f\"Total unique folders: {len(folder_columns)}\")\n",
        "        print(f\"People appearing in multiple folders: {(final_matrix['folder_count'] > 1).sum()}\")\n",
        "        print(f\"Maximum folders per person: {final_matrix['folder_count'].max()}\")\n",
        "\n",
        "        return final_matrix\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{filepath}' was not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Run the analysis\n",
        "if __name__ == \"__main__\":\n",
        "    result = create_person_folder_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZ0SAREHbRc"
      },
      "source": [
        "◊ë◊ß◊ï◊ì ◊î◊ñ◊î ◊ô◊¶◊®◊™◊ô [◊ò◊ë◊ú◊î ◊©◊û◊®◊ê◊î ◊õ◊û◊î ◊©◊ï◊ê◊ú◊ô◊ù ◊û◊ï◊§◊ô◊¢◊ô◊ù ◊ë◊õ◊û◊î ◊ò◊ë◊ú◊ê◊ï◊™](https://docs.google.com/spreadsheets/d/15ebSK2RHUX1BAYgAh_V8ScDO383K7aaAejLw7Xepe0A/edit?usp=sharing)\n",
        "\n",
        "◊î◊ô◊ê ◊ú◊ê ◊õ◊ú ◊õ◊ö ◊û◊¢◊†◊ô◊ô◊†◊™ ◊õ◊ô ◊†◊®◊ê◊î ◊©◊ô◊© ◊®◊ß ◊©◊ï◊ê◊ú ◊ê◊ó◊ì ◊©◊†◊û◊¶◊ê ◊ë◊ô◊ï◊™◊® ◊û◊ò◊ë◊ú◊î ◊ê◊ó◊™ ◊ï◊û◊ì◊ï◊ë◊® ◊ë◊ë◊®◊¢◊í◊¢◊® ◊®◊ê◊ï◊ë◊ü - ◊ê◊ô◊†◊ì◊ß◊° 1560"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdVBu40BkKt"
      },
      "source": [
        "# ◊ì◊ê◊©◊ë◊ï◊®◊ì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kV8KtBJaBnl8",
        "outputId": "ac01fe85-cda5-46cf-e5c7-614e75e8836f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 12:52:50--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 99.83.220.108, 35.71.179.82, 13.248.244.96, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|99.83.220.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‚Äòngrok-stable-linux-amd64.zip‚Äô\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  17.3MB/s    in 0.8s    \n",
            "\n",
            "2025-07-28 12:52:51 (17.3 MB/s) - ‚Äòngrok-stable-linux-amd64.zip‚Äô saved [13921656/13921656]\n",
            "\n",
            "Please enter the full path to your CSV file:\n",
            "   Example: /content/borrowers_data.csv\n",
            "   Or simply: borrowers_data.csv (if file is in same directory)\n",
            "\n",
            "File path: /content/borrowers_data.csv\n",
            "File found: /content/borrowers_data.csv\n",
            "\n",
            "Creating dashboard with file: /content/borrowers_data.csv\n",
            "Loading data and building networks...\n",
            "\n",
            "Dashboard created successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2025-07-28T12:53:24+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic tunnel setup failed.\n",
            "Please run this command in a NEW Colab cell:\n",
            "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "!unzip ngrok-stable-linux-amd64.zip\n",
            "!./ngrok http 8050\n",
            "Then copy the https URL that appears\n",
            "Local access: http://localhost:8050\n",
            "\n",
            "Starting dashboard server...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install required packages (run this cell first in Google Colab)\n",
        "!pip install plotly dash dash-bootstrap-components networkx pandas numpy pyngrok -q\n",
        "!pip install dash_bootstrap_components -q\n",
        "!pip install dash -q\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import dash\n",
        "from dash import dcc, html, Input, Output, State, callback_context, dash_table\n",
        "import dash_bootstrap_components as dbc\n",
        "from networkx.algorithms import community\n",
        "import warnings\n",
        "import time\n",
        "from functools import lru_cache\n",
        "import json\n",
        "import os\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class OptimizedLibraryNetworkAnalyzer:\n",
        "    \"\"\"\n",
        "    Optimized version with all original features restored and performance improvements\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath):\n",
        "        self.df = pd.read_csv(filepath)\n",
        "        self.prepare_data()\n",
        "        self.create_networks()\n",
        "        self._layout_cache = {}\n",
        "        self._subgraph_cache = {}\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Clean and prepare data with optimizations\"\"\"\n",
        "        # Clean the data\n",
        "        self.df_clean = self.df.dropna(subset=[\"person's name\", 'id', 'index_name']).copy()\n",
        "\n",
        "        # Optimize data types\n",
        "        self.df_clean['id'] = self.df_clean['id'].apply(lambda x: f\"{float(x):.0f}\" if pd.notna(x) else str(x))\n",
        "        self.df_clean['index_name'] = self.df_clean['index_name'].astype(int)\n",
        "\n",
        "        # Process categorical data\n",
        "        self.df_clean['gender_processed'] = self.df_clean['gender'].apply(\n",
        "            lambda x: 'Female' if pd.notna(x) and str(x).strip().lower() == 'f' else 'Male'\n",
        "        )\n",
        "\n",
        "        # Fill NaN values efficiently and process ebooks\n",
        "        self.df_clean['language_processed'] = self.df_clean['language_nli'].fillna('Unknown')\n",
        "        self.df_clean['type_processed'] = self.df_clean['type'].fillna('Unknown')\n",
        "        # Convert ebooks/ebook variations to Unknown (case insensitive, handles variations)\n",
        "        self.df_clean['type_processed'] = self.df_clean['type_processed'].apply(\n",
        "            lambda x: 'Unknown' if str(x).lower().strip() in ['ebooks', 'ebook', 'e-book', 'e-books'] else x\n",
        "        )\n",
        "        self.df_clean['subject_processed'] = self.df_clean['subject'].fillna('Unknown')\n",
        "\n",
        "        # Cache unique values\n",
        "        self.unique_languages = sorted(self.df_clean['language_processed'].unique())\n",
        "        self.unique_types = sorted(self.df_clean['type_processed'].unique())\n",
        "        self.unique_subjects = sorted(self.df_clean['subject_processed'].unique())\n",
        "        self.unique_folders = sorted(self.df_clean['Folder'].unique())\n",
        "\n",
        "        # Pre-compute person and book stats\n",
        "        self._compute_basic_stats()\n",
        "\n",
        "    def _compute_basic_stats(self):\n",
        "        \"\"\"Pre-compute basic statistics for faster filtering\"\"\"\n",
        "        # Books per person\n",
        "        self.books_per_person = self.df_clean.groupby('index_name')['id'].nunique().to_dict()\n",
        "\n",
        "        # Readers per book\n",
        "        self.readers_per_book = self.df_clean.groupby('id')['index_name'].nunique().to_dict()\n",
        "\n",
        "        # Person metadata\n",
        "        person_meta = self.df_clean.groupby('index_name').first()\n",
        "        self.person_metadata = {\n",
        "            idx: {\n",
        "                'name': row[\"person's name\"],\n",
        "                'gender': row['gender_processed'],\n",
        "                'folder': row['Folder']\n",
        "            } for idx, row in person_meta.iterrows()\n",
        "        }\n",
        "\n",
        "        # Book metadata\n",
        "        book_meta = self.df_clean.groupby('id').first()\n",
        "        self.book_metadata = {\n",
        "            book_id: {\n",
        "                'name': row['book name'],\n",
        "                'folder': row['Folder'],\n",
        "                'language': row['language_processed'],\n",
        "                'type': row['type_processed'],\n",
        "                'subject': row['subject_processed']\n",
        "            } for book_id, row in book_meta.iterrows()\n",
        "        }\n",
        "\n",
        "    def create_networks(self):\n",
        "        \"\"\"Create networks with optimization\"\"\"\n",
        "        self.create_person_network()\n",
        "        self.create_book_network()\n",
        "        self.detect_communities()\n",
        "        self.calculate_network_metrics()\n",
        "\n",
        "        # Calculate maximum nodes for dynamic limits\n",
        "        self.max_people_nodes = len([n for n in self.person_graph.nodes() if self.person_graph.degree(n) > 0])\n",
        "        self.max_book_nodes = len([n for n in self.book_graph.nodes() if self.book_graph.degree(n) > 0])\n",
        "\n",
        "        # Calculate metric ranges for dynamic sliders\n",
        "        self._calculate_metric_ranges()\n",
        "\n",
        "    def create_person_network(self):\n",
        "        \"\"\"Optimized person network creation\"\"\"\n",
        "        # Group by book to find shared readers\n",
        "        readers_per_book = self.df_clean.groupby('id')['index_name'].apply(list)\n",
        "\n",
        "        person_connections = []\n",
        "        shared_books_info = {}\n",
        "\n",
        "        for book_id, readers in readers_per_book.items():\n",
        "            unique_readers = list(set(readers))\n",
        "            if len(unique_readers) > 1:\n",
        "                pairs = list(combinations(unique_readers, 2))\n",
        "                person_connections.extend(pairs)\n",
        "\n",
        "                # Store shared book info\n",
        "                book_name = self.book_metadata[book_id]['name']\n",
        "                for pair in pairs:\n",
        "                    pair_key = tuple(sorted(pair))\n",
        "                    if pair_key not in shared_books_info:\n",
        "                        shared_books_info[pair_key] = []\n",
        "                    shared_books_info[pair_key].append((book_name, book_id))\n",
        "\n",
        "        if person_connections:\n",
        "            # Create weighted edges\n",
        "            person_df = pd.DataFrame(person_connections, columns=['Person1', 'Person2'])\n",
        "            person_weights = person_df.value_counts().reset_index()\n",
        "            person_weights.columns = ['Person1', 'Person2', 'Weight']\n",
        "\n",
        "            # Create graph\n",
        "            self.person_graph = nx.from_pandas_edgelist(\n",
        "                person_weights, 'Person1', 'Person2', edge_attr='Weight'\n",
        "            )\n",
        "\n",
        "            # Add node attributes efficiently\n",
        "            for node in self.person_graph.nodes():\n",
        "                attrs = self.person_metadata[node].copy()\n",
        "                attrs['books_count'] = self.books_per_person[node]\n",
        "                self.person_graph.nodes[node].update(attrs)\n",
        "\n",
        "            # Add edge attributes\n",
        "            for idx, row in person_weights.iterrows():\n",
        "                pair_key = tuple(sorted([row['Person1'], row['Person2']]))\n",
        "                if pair_key in shared_books_info:\n",
        "                    self.person_graph.edges[row['Person1'], row['Person2']]['shared_books'] = shared_books_info[pair_key]\n",
        "\n",
        "            self.shared_books_info = shared_books_info\n",
        "        else:\n",
        "            self.person_graph = nx.Graph()\n",
        "            self.shared_books_info = {}\n",
        "\n",
        "    def create_book_network(self):\n",
        "        \"\"\"Optimized book network creation\"\"\"\n",
        "        books_per_person = self.df_clean.groupby('index_name')['id'].apply(list)\n",
        "\n",
        "        book_connections = []\n",
        "        shared_readers_info = {}\n",
        "\n",
        "        for person_id, books in books_per_person.items():\n",
        "            unique_books = list(set(books))\n",
        "            if len(unique_books) > 1:\n",
        "                pairs = list(combinations(unique_books, 2))\n",
        "                book_connections.extend(pairs)\n",
        "\n",
        "                person_name = self.person_metadata[person_id]['name']\n",
        "                for pair in pairs:\n",
        "                    pair_key = tuple(sorted(pair))\n",
        "                    if pair_key not in shared_readers_info:\n",
        "                        shared_readers_info[pair_key] = []\n",
        "                    shared_readers_info[pair_key].append((person_name, person_id))\n",
        "\n",
        "        if book_connections:\n",
        "            book_df = pd.DataFrame(book_connections, columns=['Book1', 'Book2'])\n",
        "            book_weights = book_df.value_counts().reset_index()\n",
        "            book_weights.columns = ['Book1', 'Book2', 'Weight']\n",
        "\n",
        "            self.book_graph = nx.from_pandas_edgelist(\n",
        "                book_weights, 'Book1', 'Book2', edge_attr='Weight'\n",
        "            )\n",
        "\n",
        "            # Add node attributes efficiently\n",
        "            for node in self.book_graph.nodes():\n",
        "                attrs = self.book_metadata[node].copy()\n",
        "                attrs['readers_count'] = self.readers_per_book[node]\n",
        "                self.book_graph.nodes[node].update(attrs)\n",
        "\n",
        "            # Add edge attributes\n",
        "            for idx, row in book_weights.iterrows():\n",
        "                pair_key = tuple(sorted([row['Book1'], row['Book2']]))\n",
        "                if pair_key in shared_readers_info:\n",
        "                    self.book_graph.edges[row['Book1'], row['Book2']]['shared_readers'] = shared_readers_info[pair_key]\n",
        "\n",
        "            self.shared_readers_info = shared_readers_info\n",
        "        else:\n",
        "            self.book_graph = nx.Graph()\n",
        "            self.shared_readers_info = {}\n",
        "\n",
        "    def detect_communities(self):\n",
        "        \"\"\"Detect communities with caching\"\"\"\n",
        "        if len(self.person_graph.nodes()) > 0:\n",
        "            communities = community.louvain_communities(self.person_graph, seed=42)\n",
        "\n",
        "            self.node_communities = {}\n",
        "            for i, comm in enumerate(communities):\n",
        "                for node in comm:\n",
        "                    self.node_communities[node] = i\n",
        "                    self.person_graph.nodes[node]['community'] = i\n",
        "\n",
        "            self.communities = list(communities)\n",
        "        else:\n",
        "            self.node_communities = {}\n",
        "            self.communities = []\n",
        "\n",
        "    def calculate_network_metrics(self):\n",
        "        \"\"\"Calculate centrality measures with optimization for both networks\"\"\"\n",
        "        # Calculate metrics for person network\n",
        "        if len(self.person_graph.nodes()) > 0:\n",
        "            # Use faster algorithms for large graphs\n",
        "            if len(self.person_graph.nodes()) > 1000:\n",
        "                # Sample for betweenness centrality\n",
        "                sample_size = min(1000, len(self.person_graph.nodes()))\n",
        "                sample_nodes = np.random.choice(list(self.person_graph.nodes()), sample_size, replace=False)\n",
        "                self.person_betweenness_centrality = nx.betweenness_centrality_subset(\n",
        "                    self.person_graph, sample_nodes, sample_nodes\n",
        "                )\n",
        "            else:\n",
        "                self.person_betweenness_centrality = nx.betweenness_centrality(self.person_graph)\n",
        "\n",
        "            self.person_degree_centrality = nx.degree_centrality(self.person_graph)\n",
        "            self.person_closeness_centrality = nx.closeness_centrality(self.person_graph)\n",
        "\n",
        "            # Add to node attributes\n",
        "            for node in self.person_graph.nodes():\n",
        "                self.person_graph.nodes[node]['degree_centrality'] = self.person_degree_centrality.get(node, 0)\n",
        "                self.person_graph.nodes[node]['betweenness_centrality'] = self.person_betweenness_centrality.get(node, 0)\n",
        "                self.person_graph.nodes[node]['closeness_centrality'] = self.person_closeness_centrality.get(node, 0)\n",
        "        else:\n",
        "            self.person_degree_centrality = {}\n",
        "            self.person_betweenness_centrality = {}\n",
        "            self.person_closeness_centrality = {}\n",
        "\n",
        "        # Calculate metrics for book network\n",
        "        if len(self.book_graph.nodes()) > 0:\n",
        "            # Use faster algorithms for large graphs\n",
        "            if len(self.book_graph.nodes()) > 1000:\n",
        "                # Sample for betweenness centrality\n",
        "                sample_size = min(1000, len(self.book_graph.nodes()))\n",
        "                sample_nodes = np.random.choice(list(self.book_graph.nodes()), sample_size, replace=False)\n",
        "                self.book_betweenness_centrality = nx.betweenness_centrality_subset(\n",
        "                    self.book_graph, sample_nodes, sample_nodes\n",
        "                )\n",
        "            else:\n",
        "                self.book_betweenness_centrality = nx.betweenness_centrality(self.book_graph)\n",
        "\n",
        "            self.book_degree_centrality = nx.degree_centrality(self.book_graph)\n",
        "            self.book_closeness_centrality = nx.closeness_centrality(self.book_graph)\n",
        "\n",
        "            # Add to node attributes\n",
        "            for node in self.book_graph.nodes():\n",
        "                self.book_graph.nodes[node]['degree_centrality'] = self.book_degree_centrality.get(node, 0)\n",
        "                self.book_graph.nodes[node]['betweenness_centrality'] = self.book_betweenness_centrality.get(node, 0)\n",
        "                self.book_graph.nodes[node]['closeness_centrality'] = self.book_closeness_centrality.get(node, 0)\n",
        "        else:\n",
        "            self.book_degree_centrality = {}\n",
        "            self.book_betweenness_centrality = {}\n",
        "            self.book_closeness_centrality = {}\n",
        "\n",
        "        # Keep backward compatibility\n",
        "        self.degree_centrality = self.person_degree_centrality\n",
        "        self.betweenness_centrality = self.person_betweenness_centrality\n",
        "        self.closeness_centrality = self.person_closeness_centrality\n",
        "\n",
        "    def _calculate_metric_ranges(self):\n",
        "        \"\"\"Calculate ranges for network metrics for dynamic sliders\"\"\"\n",
        "        # People network ranges\n",
        "        if self.person_betweenness_centrality:\n",
        "            self.person_betweenness_max = max(self.person_betweenness_centrality.values())\n",
        "            self.person_degree_max = max(self.person_degree_centrality.values())\n",
        "        else:\n",
        "            self.person_betweenness_max = 0.1\n",
        "            self.person_degree_max = 1.0\n",
        "\n",
        "        # Book network ranges\n",
        "        if self.book_betweenness_centrality:\n",
        "            self.book_betweenness_max = max(self.book_betweenness_centrality.values())\n",
        "            self.book_degree_max = max(self.book_degree_centrality.values())\n",
        "        else:\n",
        "            self.book_betweenness_max = 0.1\n",
        "            self.book_degree_max = 1.0\n",
        "\n",
        "    def get_optimized_layout(self, graph, max_iterations=30):\n",
        "        \"\"\"Get optimized layout based on graph size with maximum spacing\"\"\"\n",
        "        n_nodes = len(graph.nodes())\n",
        "\n",
        "        if n_nodes == 0:\n",
        "            return {}\n",
        "        elif n_nodes == 1:\n",
        "            return {list(graph.nodes())[0]: (0, 0)}\n",
        "        elif n_nodes > 500:\n",
        "            # For large graphs, use faster algorithm with maximum spacing\n",
        "            pos = nx.random_layout(graph, seed=42)\n",
        "            # Quick spring layout iterations with much larger k for maximum spacing\n",
        "            for _ in range(15):\n",
        "                pos = nx.spring_layout(\n",
        "                    graph, pos=pos, iterations=3,\n",
        "                    k=6/np.sqrt(n_nodes), seed=42\n",
        "                )\n",
        "            return pos\n",
        "        else:\n",
        "            # Standard spring layout for smaller graphs with maximum spacing\n",
        "            return nx.spring_layout(\n",
        "                graph, k=8/np.sqrt(n_nodes),  # Increased k significantly for maximum spacing\n",
        "                iterations=max_iterations, seed=42,\n",
        "                threshold=1e-4\n",
        "            )\n",
        "\n",
        "    def get_filtered_subgraph(self, graph_type, person_filter=None, book_filter=None,\n",
        "                              community_filter=None, folder_filter=None, gender_filter=None,\n",
        "                              centrality_filter=None, degree_centrality_filter=None,\n",
        "                              language_filter=None, type_filter=None, subject_filter=None,\n",
        "                              max_nodes=None, focus_mode=False):\n",
        "        \"\"\"\n",
        "        Get a filtered subgraph with all original filters restored\n",
        "        \"\"\"\n",
        "        if graph_type == 'person':\n",
        "            graph = self.person_graph\n",
        "            # Use all nodes if max_nodes not specified or exceeds available\n",
        "            if max_nodes is None:\n",
        "                max_nodes = self.max_people_nodes\n",
        "            else:\n",
        "                max_nodes = min(max_nodes, self.max_people_nodes)\n",
        "        else:\n",
        "            graph = self.book_graph\n",
        "            # Use all nodes if max_nodes not specified or exceeds available\n",
        "            if max_nodes is None:\n",
        "                max_nodes = self.max_book_nodes\n",
        "            else:\n",
        "                max_nodes = min(max_nodes, self.max_book_nodes)\n",
        "\n",
        "        # Start with all connected nodes\n",
        "        nodes_to_include = {n for n in graph.nodes() if graph.degree(n) > 0}\n",
        "\n",
        "        # Apply filters one by one (AND logic)\n",
        "        if graph_type == 'person':\n",
        "            if person_filter:\n",
        "                person_nodes = {n for n in nodes_to_include\n",
        "                                if (str(person_filter).lower() in graph.nodes[n].get('name', '').lower() or\n",
        "                                    str(person_filter) == str(n))}\n",
        "\n",
        "                if focus_mode:\n",
        "                    focus_nodes = set()\n",
        "                    for node in person_nodes:\n",
        "                        focus_nodes.add(node)\n",
        "                        focus_nodes.update(graph.neighbors(node))\n",
        "                    nodes_to_include = nodes_to_include.intersection(focus_nodes)\n",
        "                else:\n",
        "                    nodes_to_include = nodes_to_include.intersection(person_nodes)\n",
        "\n",
        "            if community_filter is not None and community_filter != 'all':\n",
        "                community_nodes = {n for n in nodes_to_include\n",
        "                                   if self.node_communities.get(n) == community_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(community_nodes)\n",
        "\n",
        "            if gender_filter and gender_filter != 'all':\n",
        "                gender_nodes = {n for n in nodes_to_include\n",
        "                                if graph.nodes[n].get('gender') == gender_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(gender_nodes)\n",
        "\n",
        "            if centrality_filter is not None and centrality_filter > 0:\n",
        "                centrality_nodes = {n for n in nodes_to_include\n",
        "                                    if self.person_betweenness_centrality.get(n, 0) >= centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(centrality_nodes)\n",
        "\n",
        "            if degree_centrality_filter is not None and degree_centrality_filter > 0:\n",
        "                degree_centrality_nodes = {n for n in nodes_to_include\n",
        "                                           if self.person_degree_centrality.get(n, 0) >= degree_centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(degree_centrality_nodes)\n",
        "\n",
        "            if folder_filter and folder_filter != 'all' and 'all' not in str(folder_filter):\n",
        "                if isinstance(folder_filter, list):\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') in folder_filter}\n",
        "                else:\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') == folder_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(folder_nodes)\n",
        "\n",
        "            # Apply language and type filters for person network by filtering books they read\n",
        "            if language_filter and language_filter != 'all':\n",
        "                if isinstance(language_filter, list) and 'all' not in language_filter:\n",
        "                    person_ids_with_language = set(self.df_clean[\n",
        "                        self.df_clean['language_processed'].isin(language_filter)\n",
        "                    ]['index_name'].unique())\n",
        "                elif not isinstance(language_filter, list):\n",
        "                    person_ids_with_language = set(self.df_clean[\n",
        "                        self.df_clean['language_processed'] == language_filter\n",
        "                    ]['index_name'].unique())\n",
        "                else:\n",
        "                    person_ids_with_language = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(person_ids_with_language)\n",
        "\n",
        "            if type_filter and type_filter != 'all':\n",
        "                if isinstance(type_filter, list) and 'all' not in type_filter:\n",
        "                    person_ids_with_type = set(self.df_clean[\n",
        "                        self.df_clean['type_processed'].isin(type_filter)\n",
        "                    ]['index_name'].unique())\n",
        "                elif not isinstance(type_filter, list):\n",
        "                    person_ids_with_type = set(self.df_clean[\n",
        "                        self.df_clean['type_processed'] == type_filter\n",
        "                    ]['index_name'].unique())\n",
        "                else:\n",
        "                    person_ids_with_type = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(person_ids_with_type)\n",
        "\n",
        "            if subject_filter and subject_filter != 'all':\n",
        "                if isinstance(subject_filter, list) and 'all' not in subject_filter:\n",
        "                    person_ids_with_subject = set(self.df_clean[\n",
        "                        self.df_clean['subject_processed'].isin(subject_filter)\n",
        "                    ]['index_name'].unique())\n",
        "                elif not isinstance(subject_filter, list):\n",
        "                    person_ids_with_subject = set(self.df_clean[\n",
        "                        self.df_clean['subject_processed'] == subject_filter\n",
        "                    ]['index_name'].unique())\n",
        "                else:\n",
        "                    person_ids_with_subject = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(person_ids_with_subject)\n",
        "\n",
        "        else:  # book network\n",
        "            if book_filter:\n",
        "                book_nodes = {n for n in nodes_to_include\n",
        "                              if (str(book_filter).lower() in graph.nodes[n].get('name', '').lower() or\n",
        "                                  str(book_filter) == str(n))}\n",
        "\n",
        "                if focus_mode:\n",
        "                    focus_nodes = set()\n",
        "                    for node in book_nodes:\n",
        "                        focus_nodes.add(node)\n",
        "                        focus_nodes.update(graph.neighbors(node))\n",
        "                    nodes_to_include = nodes_to_include.intersection(focus_nodes)\n",
        "                else:\n",
        "                    nodes_to_include = nodes_to_include.intersection(book_nodes)\n",
        "\n",
        "            if folder_filter and folder_filter != 'all' and 'all' not in str(folder_filter):\n",
        "                if isinstance(folder_filter, list):\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') in folder_filter}\n",
        "                else:\n",
        "                    folder_nodes = {n for n in nodes_to_include\n",
        "                                    if graph.nodes[n].get('folder') == folder_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(folder_nodes)\n",
        "\n",
        "            # Apply language filter for book network\n",
        "            if language_filter and language_filter != 'all':\n",
        "                if isinstance(language_filter, list) and 'all' not in language_filter:\n",
        "                    language_nodes = {n for n in nodes_to_include\n",
        "                                      if graph.nodes[n].get('language') in language_filter}\n",
        "                elif not isinstance(language_filter, list):\n",
        "                    language_nodes = {n for n in nodes_to_include\n",
        "                                      if graph.nodes[n].get('language') == language_filter}\n",
        "                else:\n",
        "                    language_nodes = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(language_nodes)\n",
        "\n",
        "            # Apply type filter for book network\n",
        "            if type_filter and type_filter != 'all':\n",
        "                if isinstance(type_filter, list) and 'all' not in type_filter:\n",
        "                    type_nodes = {n for n in nodes_to_include\n",
        "                                  if graph.nodes[n].get('type') in type_filter}\n",
        "                elif not isinstance(type_filter, list):\n",
        "                    type_nodes = {n for n in nodes_to_include\n",
        "                                  if graph.nodes[n].get('type') == type_filter}\n",
        "                else:\n",
        "                    type_nodes = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(type_nodes)\n",
        "\n",
        "            # Apply subject/genre filter for book network\n",
        "            if subject_filter and subject_filter != 'all':\n",
        "                if isinstance(subject_filter, list) and 'all' not in subject_filter:\n",
        "                    subject_nodes = {n for n in nodes_to_include\n",
        "                                     if graph.nodes[n].get('subject') in subject_filter}\n",
        "                elif not isinstance(subject_filter, list):\n",
        "                    subject_nodes = {n for n in nodes_to_include\n",
        "                                     if graph.nodes[n].get('subject') == subject_filter}\n",
        "                else:\n",
        "                    subject_nodes = set(graph.nodes())\n",
        "                nodes_to_include = nodes_to_include.intersection(subject_nodes)\n",
        "\n",
        "            # Apply centrality filters for book network\n",
        "            if centrality_filter is not None and centrality_filter > 0:\n",
        "                centrality_nodes = {n for n in nodes_to_include\n",
        "                                    if self.book_betweenness_centrality.get(n, 0) >= centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(centrality_nodes)\n",
        "\n",
        "            if degree_centrality_filter is not None and degree_centrality_filter > 0:\n",
        "                degree_centrality_nodes = {n for n in nodes_to_include\n",
        "                                           if self.book_degree_centrality.get(n, 0) >= degree_centrality_filter}\n",
        "                nodes_to_include = nodes_to_include.intersection(degree_centrality_nodes)\n",
        "\n",
        "        # Limit nodes for performance, prioritize by degree\n",
        "        if len(nodes_to_include) > max_nodes:\n",
        "            # Sort by degree and take top nodes\n",
        "            node_degrees = [(n, graph.degree(n)) for n in nodes_to_include]\n",
        "            node_degrees.sort(key=lambda x: x[1], reverse=True)\n",
        "            nodes_to_include = {n for n, _ in node_degrees[:max_nodes]}\n",
        "\n",
        "        return graph.subgraph(nodes_to_include)\n",
        "\n",
        "def create_network_plot(analyzer, show_people=True, show_books=True,\n",
        "                        person_filter=None, book_filter=None, community_filter=None,\n",
        "                        folder_filter=None, gender_filter=None, centrality_filter=None,\n",
        "                        degree_centrality_filter=None, language_filter=None,\n",
        "                        type_filter=None, subject_filter=None, max_nodes=200):\n",
        "    \"\"\"\n",
        "    Create an interactive network plot using Plotly with all original features restored\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "    focus_mode = bool(person_filter or book_filter)\n",
        "\n",
        "    # Determine which graphs to include based on filters\n",
        "    graphs_to_show = []\n",
        "    if show_people and len(analyzer.person_graph.nodes()) > 0:\n",
        "        person_subgraph = analyzer.get_filtered_subgraph(\n",
        "            'person', person_filter, book_filter, community_filter,\n",
        "            folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "            language_filter, type_filter, subject_filter, max_nodes//2 if show_books else max_nodes, focus_mode=focus_mode\n",
        "        )\n",
        "        if len(person_subgraph.nodes()) > 0:\n",
        "            graphs_to_show.append(('person', person_subgraph))\n",
        "\n",
        "    if show_books and len(analyzer.book_graph.nodes()) > 0:\n",
        "        book_subgraph = analyzer.get_filtered_subgraph(\n",
        "            'book', person_filter, book_filter, community_filter,\n",
        "            folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "            language_filter, type_filter, subject_filter, max_nodes//2 if show_people else max_nodes, focus_mode=focus_mode\n",
        "        )\n",
        "        if len(book_subgraph.nodes()) > 0:\n",
        "            graphs_to_show.append(('book', book_subgraph))\n",
        "\n",
        "    if not graphs_to_show:\n",
        "        fig.add_annotation(\n",
        "            text=\"No data to display with current filters\",\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "            showarrow=False, font=dict(size=16)\n",
        "        )\n",
        "        fig.update_layout(\n",
        "            title=\"Network Analysis\",\n",
        "            showlegend=False,\n",
        "            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "        return fig\n",
        "\n",
        "    # Create combined graph for layout calculation\n",
        "    combined_graph = nx.Graph()\n",
        "    node_types = {}\n",
        "    highlighted_nodes = set()\n",
        "\n",
        "    for graph_type, graph in graphs_to_show:\n",
        "        for node in graph.nodes():\n",
        "            combined_graph.add_node(node)\n",
        "            node_types[node] = graph_type\n",
        "\n",
        "            if graph_type == 'person' and person_filter:\n",
        "                node_name = analyzer.person_graph.nodes[node].get('name', '')\n",
        "                if (str(person_filter).lower() in node_name.lower() or\n",
        "                        str(person_filter) == str(node)):\n",
        "                    highlighted_nodes.add(node)\n",
        "            elif graph_type == 'book' and book_filter:\n",
        "                node_name = analyzer.book_graph.nodes[node].get('name', '')\n",
        "                if (str(book_filter).lower() in node_name.lower() or\n",
        "                        str(book_filter) == str(node)):\n",
        "                    highlighted_nodes.add(node)\n",
        "\n",
        "        for edge in graph.edges(data=True):\n",
        "            combined_graph.add_edge(edge[0], edge[1], **edge[2])\n",
        "\n",
        "    # Calculate layout\n",
        "    pos = analyzer.get_optimized_layout(combined_graph)\n",
        "\n",
        "    # Create traces for edges with hover functionality\n",
        "    edge_traces = []\n",
        "\n",
        "    for graph_type, graph in graphs_to_show:\n",
        "        edge_x = []\n",
        "        edge_y = []\n",
        "        edge_midpoint_x = []\n",
        "        edge_midpoint_y = []\n",
        "        edge_midpoint_info = []\n",
        "\n",
        "        for edge in graph.edges(data=True):\n",
        "            if edge[0] in pos and edge[1] in pos:\n",
        "                x0, y0 = pos[edge[0]]\n",
        "                x1, y1 = pos[edge[1]]\n",
        "                edge_x.extend([x0, x1, None])\n",
        "                edge_y.extend([y0, y1, None])\n",
        "\n",
        "                mid_x = (x0 + x1) / 2\n",
        "                mid_y = (y0 + y1) / 2\n",
        "                edge_midpoint_x.append(mid_x)\n",
        "                edge_midpoint_y.append(mid_y)\n",
        "\n",
        "                if graph_type == 'person':\n",
        "                    shared_books = edge[2].get('shared_books', [])\n",
        "                    book_count = len(shared_books)\n",
        "                    book_names = [book[0] for book in shared_books]\n",
        "\n",
        "                    person1_name = analyzer.person_graph.nodes[edge[0]].get('name', f'Person {edge[0]}')\n",
        "                    person2_name = analyzer.person_graph.nodes[edge[1]].get('name', f'Person {edge[1]}')\n",
        "\n",
        "                    hover_text = f\"<b>{person1_name}</b> ‚Üî <b>{person2_name}</b><br>\"\n",
        "                    hover_text += f\"<b>Shared Books:</b> {book_count}<br>\"\n",
        "                    for i, book_name in enumerate(book_names[:3]):\n",
        "                        hover_text += f\"‚Ä¢ {book_name}<br>\"\n",
        "                    if book_count > 3:\n",
        "                        hover_text += f\"... and {book_count - 3} more books\"\n",
        "\n",
        "                else:\n",
        "                    shared_readers = edge[2].get('shared_readers', [])\n",
        "                    reader_count = len(shared_readers)\n",
        "                    reader_names = [reader[0] for reader in shared_readers]\n",
        "\n",
        "                    book1_name = analyzer.book_graph.nodes[edge[0]].get('name', f'Book {edge[0]}')\n",
        "                    book2_name = analyzer.book_graph.nodes[edge[1]].get('name', f'Book {edge[1]}')\n",
        "\n",
        "                    hover_text = f\"<b>{book1_name}</b> ‚Üî <b>{book2_name}</b><br>\"\n",
        "                    hover_text += f\"<b>Shared Readers:</b> {reader_count}<br>\"\n",
        "                    for i, reader_name in enumerate(reader_names[:3]):\n",
        "                        hover_text += f\"‚Ä¢ {reader_name}<br>\"\n",
        "                    if reader_count > 3:\n",
        "                        hover_text += f\"... and {reader_count - 3} more readers\"\n",
        "\n",
        "                edge_midpoint_info.append(hover_text)\n",
        "\n",
        "        # Add edge lines with thinner width\n",
        "        if edge_x:\n",
        "            edge_trace = go.Scatter(\n",
        "                x=edge_x, y=edge_y,\n",
        "                line=dict(width=1.5, color='rgba(136,136,136,0.6)'),  # Reduced from 4 to 1.5\n",
        "                mode='lines',\n",
        "                showlegend=False,\n",
        "                name=f'{graph_type}_edges',\n",
        "                opacity=0.8,\n",
        "                hoverinfo='skip'\n",
        "            )\n",
        "            fig.add_trace(edge_trace)\n",
        "\n",
        "            # Add invisible midpoint markers for hover\n",
        "            edge_hover_trace = go.Scatter(\n",
        "                x=edge_midpoint_x,\n",
        "                y=edge_midpoint_y,\n",
        "                mode='markers',\n",
        "                marker=dict(\n",
        "                    size=8,\n",
        "                    color='rgba(0,0,0,0)',\n",
        "                    line=dict(width=0)\n",
        "                ),\n",
        "                hovertemplate='%{text}<extra></extra>',\n",
        "                text=edge_midpoint_info,\n",
        "                showlegend=False,\n",
        "                name=f'{graph_type}_edge_hovers'\n",
        "            )\n",
        "            fig.add_trace(edge_hover_trace)\n",
        "\n",
        "    # Create traces for nodes with full detail\n",
        "    for graph_type, graph in graphs_to_show:\n",
        "        node_x = []\n",
        "        node_y = []\n",
        "        node_info = []\n",
        "        node_colors = []\n",
        "        node_sizes = []\n",
        "        node_symbols = []\n",
        "\n",
        "        for node in graph.nodes():\n",
        "            if node in pos:\n",
        "                x, y = pos[node]\n",
        "                node_x.append(x)\n",
        "                node_y.append(y)\n",
        "\n",
        "                if graph_type == 'person':\n",
        "                    person_data = analyzer.person_graph.nodes[node]\n",
        "                    community_id = analyzer.node_communities.get(node, 'Unknown')\n",
        "                    connections_count = graph.degree(node)\n",
        "                    books_count = person_data.get('books_count', 0)\n",
        "                    betweenness = analyzer.person_betweenness_centrality.get(node, 0)\n",
        "                    degree_cent = analyzer.person_degree_centrality.get(node, 0)\n",
        "\n",
        "                    hover_text = f\"\"\"\n",
        "                    <b>Name:</b> {person_data.get('name', 'Unknown')}<br>\n",
        "                    <b>Index:</b> {node}<br>\n",
        "                    <b>Gender:</b> {person_data.get('gender', 'Unknown')}<br>\n",
        "                    <b>Folder:</b> {person_data.get('folder', 'Unknown')}<br>\n",
        "                    <b>Connected People:</b> {connections_count}<br>\n",
        "                    <b>Books Borrowed:</b> {books_count}<br>\n",
        "                    <b>Community:</b> {community_id}<br>\n",
        "                    <b>Betweenness Centrality:</b> {betweenness:.4f}<br>\n",
        "                    <b>Degree Centrality:</b> {degree_cent:.4f}\n",
        "                    \"\"\".strip()\n",
        "\n",
        "                    if node in highlighted_nodes:\n",
        "                        node_colors.append('red')\n",
        "                        node_sizes.append(max(20, min(40, 20 + connections_count * 3)))\n",
        "                    else:\n",
        "                        node_colors.append(community_id)\n",
        "                        node_sizes.append(max(8, min(25, 8 + connections_count * 1.5)))\n",
        "\n",
        "                    node_symbols.append('circle')\n",
        "\n",
        "                else:\n",
        "                    book_data = analyzer.book_graph.nodes[node]\n",
        "                    readers_count = book_data.get('readers_count', 0)\n",
        "                    connections_count = graph.degree(node)\n",
        "                    betweenness = analyzer.book_betweenness_centrality.get(node, 0)\n",
        "                    degree_cent = analyzer.book_degree_centrality.get(node, 0)\n",
        "\n",
        "                    hover_text = f\"\"\"\n",
        "                    <b>Book:</b> {book_data.get('name', 'Unknown')}<br>\n",
        "                    <b>ID:</b> {node}<br>\n",
        "                    <b>Readers:</b> {readers_count}<br>\n",
        "                    <b>Connections:</b> {connections_count}<br>\n",
        "                    <b>Folder:</b> {book_data.get('folder', 'Unknown')}<br>\n",
        "                    <b>Language:</b> {book_data.get('language', 'Unknown')}<br>\n",
        "                    <b>Type:</b> {book_data.get('type', 'Unknown')}<br>\n",
        "                    <b>Subject/Genre:</b> {book_data.get('subject', 'Unknown')}<br>\n",
        "                    <b>Betweenness Centrality:</b> {betweenness:.4f}<br>\n",
        "                    <b>Degree Centrality:</b> {degree_cent:.4f}\n",
        "                    \"\"\".strip()\n",
        "\n",
        "                    if node in highlighted_nodes:\n",
        "                        node_colors.append(10)\n",
        "                        node_sizes.append(max(15, min(35, 15 + readers_count * 2)))\n",
        "                    else:\n",
        "                        folder_map = {\n",
        "                            'Vol_1_1902': 0,\n",
        "                            'Vol 1_1': 1,\n",
        "                            'SL Ledger 1934': 2,\n",
        "                            'SL Ledger_1940': 3\n",
        "                        }\n",
        "                        folder_value = folder_map.get(book_data.get('folder', 'Unknown'), 0)\n",
        "                        node_colors.append(folder_value)\n",
        "                        node_sizes.append(max(6, min(20, 6 + readers_count * 1)))\n",
        "\n",
        "                    node_symbols.append('triangle-up')\n",
        "\n",
        "                node_info.append(hover_text)\n",
        "\n",
        "        # Add node trace\n",
        "        if node_x:\n",
        "            node_trace = go.Scatter(\n",
        "                x=node_x, y=node_y,\n",
        "                mode='markers',\n",
        "                hovertemplate='%{text}<extra></extra>',\n",
        "                text=node_info,\n",
        "                marker=dict(\n",
        "                    size=node_sizes,\n",
        "                    color=node_colors,\n",
        "                    symbol=node_symbols,\n",
        "                    colorscale='Viridis',\n",
        "                    showscale=True,\n",
        "                    colorbar=dict(\n",
        "                        title=f\"{'Community' if graph_type == 'person' else 'Volume'}\",\n",
        "                        x=1.02 if graph_type == 'person' else 1.1\n",
        "                    ),\n",
        "                    line=dict(width=1, color='DarkSlateGrey')\n",
        "                ),\n",
        "                name=f\"{'People' if graph_type == 'person' else 'Books'}\",\n",
        "                showlegend=True\n",
        "            )\n",
        "            fig.add_trace(node_trace)\n",
        "\n",
        "    # Update layout for maximum space usage\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            'text': f'Strashun Library Network Analysis{\"\" if not focus_mode else \" - Focus View\"}',\n",
        "            'font': {'size': 16}\n",
        "        },\n",
        "        showlegend=True,\n",
        "        hovermode='closest',\n",
        "        margin=dict(b=10, l=10, r=10, t=50),  # Reduced margins for more graph space\n",
        "        annotations=[dict(\n",
        "            text=\"Hover over nodes and edges to see details\",\n",
        "            showarrow=False,\n",
        "            xref=\"paper\", yref=\"paper\",\n",
        "            x=0.005, y=-0.002)],\n",
        "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "        plot_bgcolor='white',\n",
        "        height=900,  # Increased from 800 to 900 for maximum space\n",
        "        dragmode='pan',\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "def calculate_network_statistics(analyzer, show_people=True, show_books=True,\n",
        "                                 person_filter=None, book_filter=None,\n",
        "                                 community_filter=None, folder_filter=None,\n",
        "                                 gender_filter=None, centrality_filter=None,\n",
        "                                 degree_centrality_filter=None, language_filter=None,\n",
        "                                 type_filter=None, subject_filter=None, max_nodes=200):\n",
        "    \"\"\"\n",
        "    Calculate and format network statistics for display with full functionality\n",
        "    \"\"\"\n",
        "    try:\n",
        "        stats_components = []\n",
        "\n",
        "        if show_people and len(analyzer.person_graph.nodes()) > 0:\n",
        "            person_subgraph = analyzer.get_filtered_subgraph(\n",
        "                'person', person_filter, book_filter, community_filter,\n",
        "                folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "                language_filter, type_filter, subject_filter,\n",
        "                max_nodes//2 if show_books else max_nodes,\n",
        "                focus_mode=bool(person_filter or book_filter)\n",
        "            )\n",
        "\n",
        "            if len(person_subgraph.nodes()) > 0:\n",
        "                central_people = []\n",
        "                for node in person_subgraph.nodes():\n",
        "                    centrality = analyzer.person_betweenness_centrality.get(node, 0)\n",
        "                    if isinstance(centrality, (int, float)) and centrality > 0:\n",
        "                        name = analyzer.person_graph.nodes[node].get('name', f'Person {node}')\n",
        "                        central_people.append((name, centrality, node))\n",
        "\n",
        "                central_people.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_central = central_people[:5]\n",
        "\n",
        "                stats_components.extend([\n",
        "                    html.H5(\"People Network Statistics\"),\n",
        "                    html.P(f\"Displayed People: {len(person_subgraph.nodes())}\"),\n",
        "                    html.P(f\"Displayed Connections: {len(person_subgraph.edges())}\"),\n",
        "                    html.P(f\"Communities: {len(set(analyzer.node_communities.get(n, 0) for n in person_subgraph.nodes()))}\"),\n",
        "                    html.H6(\"Most Central People (Betweenness):\"),\n",
        "                    html.Ul([\n",
        "                        html.Li(f\"{name} (Index: {idx}) - {centrality:.3f}\")\n",
        "                        for name, centrality, idx in top_central\n",
        "                    ]) if top_central else html.P(\"No central people found\"),\n",
        "                    html.Hr()\n",
        "                ])\n",
        "\n",
        "        if show_books and len(analyzer.book_graph.nodes()) > 0:\n",
        "            book_subgraph = analyzer.get_filtered_subgraph(\n",
        "                'book', person_filter, book_filter, community_filter,\n",
        "                folder_filter, gender_filter, centrality_filter, degree_centrality_filter,\n",
        "                language_filter, type_filter, subject_filter,\n",
        "                max_nodes//2 if show_people else max_nodes,\n",
        "                focus_mode=bool(person_filter or book_filter)\n",
        "            )\n",
        "\n",
        "            if len(book_subgraph.nodes()) > 0:\n",
        "                popular_books = []\n",
        "                central_books = []\n",
        "                for node in book_subgraph.nodes():\n",
        "                    readers_count = analyzer.book_graph.nodes[node].get('readers_count', 0)\n",
        "                    centrality = analyzer.book_betweenness_centrality.get(node, 0)\n",
        "                    degree_cent = analyzer.book_degree_centrality.get(node, 0)\n",
        "                    name = analyzer.book_graph.nodes[node].get('name', f'Book {node}')\n",
        "\n",
        "                    if isinstance(readers_count, (int, float)) and readers_count > 0:\n",
        "                        popular_books.append((name, readers_count, node))\n",
        "\n",
        "                    if isinstance(centrality, (int, float)) and centrality > 0:\n",
        "                        central_books.append((name, centrality, node))\n",
        "\n",
        "                popular_books.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_popular = popular_books[:5]\n",
        "\n",
        "                central_books.sort(key=lambda x: x[1], reverse=True)\n",
        "                top_central_books = central_books[:5]\n",
        "\n",
        "                stats_components.extend([\n",
        "                    html.H5(\"Books Network Statistics\"),\n",
        "                    html.P(f\"Displayed Books: {len(book_subgraph.nodes())}\"),\n",
        "                    html.P(f\"Displayed Connections: {len(book_subgraph.edges())}\"),\n",
        "                    html.H6(\"Most Popular Books:\"),\n",
        "                    html.Ul([\n",
        "                        html.Li(f\"{name} (ID: {book_id}) - {readers} readers\")\n",
        "                        for name, readers, book_id in top_popular\n",
        "                    ]) if top_popular else html.P(\"No popular books found\"),\n",
        "                    html.H6(\"Most Central Books (Betweenness):\"),\n",
        "                    html.Ul([\n",
        "                        html.Li(f\"{name} (ID: {book_id}) - {centrality:.3f}\")\n",
        "                        for name, centrality, book_id in top_central_books\n",
        "                    ]) if top_central_books else html.P(\"No central books found\")\n",
        "                ])\n",
        "\n",
        "        if not stats_components:\n",
        "            stats_components = [html.P(\"No statistics available for current selection.\")]\n",
        "\n",
        "        return stats_components\n",
        "\n",
        "    except Exception as e:\n",
        "        return [html.P(f\"Error calculating statistics: {str(e)}\")]\n",
        "\n",
        "def create_optimized_dashboard(csv_filepath):\n",
        "    \"\"\"Create the optimized dashboard with all original features restored\"\"\"\n",
        "\n",
        "    analyzer = OptimizedLibraryNetworkAnalyzer(csv_filepath)\n",
        "\n",
        "    # Calculate dynamic slider ranges\n",
        "    people_slider_max = max(50, min(1000, analyzer.max_people_nodes))\n",
        "    books_slider_max = max(50, min(1000, analyzer.max_book_nodes))\n",
        "\n",
        "    # Default values - back to 100\n",
        "    people_default = min(100, analyzer.max_people_nodes)  # Back to 100\n",
        "    books_default = min(100, analyzer.max_book_nodes)     # Back to 100\n",
        "\n",
        "    # Initialize app\n",
        "    app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "    app.title = \"Strashun Library Network Analysis\"\n",
        "\n",
        "    # App layout with all original filters and functionality\n",
        "    app.layout = dbc.Container([\n",
        "        dbc.Row([\n",
        "            dbc.Col([\n",
        "                html.H1(\"Strashun Library Network Analysis Dashboard\",\n",
        "                        className=\"text-center mb-4\"),\n",
        "                html.P(\"Analyzing connections between book borrowers in the historic Strashun Library in Lithuania\",\n",
        "                       className=\"text-center text-muted mb-4\"),\n",
        "                dbc.Alert([\n",
        "                    html.I(className=\"fas fa-info-circle me-2\"),\n",
        "                    f\"Optimal Network View: Displaying up to 100 most connected nodes by default for maximum readability and spacing. Use Max Nodes slider to show up to {max(people_slider_max, books_slider_max)} nodes or search fields to focus on specific individuals or books.\"\n",
        "                ], color=\"info\", className=\"mb-4\")\n",
        "            ])\n",
        "        ]),\n",
        "\n",
        "        dbc.Row([\n",
        "            # Filter Panel with all original filters\n",
        "            dbc.Col([\n",
        "                dbc.Card([\n",
        "                    dbc.CardHeader(html.H4(\"Filters\", className=\"mb-0\")),\n",
        "                    dbc.CardBody([\n",
        "                        # 1. Data Sources Selection\n",
        "                        html.H5(\"1. Data Sources\", className=\"mt-3\"),\n",
        "                        dbc.Checklist(\n",
        "                            id=\"data-sources-filter\",\n",
        "                            options=[\n",
        "                                {\"label\": \"People Network\", \"value\": \"people\"},\n",
        "                                {\"label\": \"Books Network\", \"value\": \"books\"}\n",
        "                            ],\n",
        "                            value=[\"people\", \"books\"],\n",
        "                            inline=True\n",
        "                        ),\n",
        "\n",
        "                        # 2. People Filters\n",
        "                        html.H5(\"2. People Filters\", className=\"mt-3\"),\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Search Person:\"),\n",
        "                                dbc.Input(\n",
        "                                    id=\"person-search\",\n",
        "                                    placeholder=\"Name or Index\",\n",
        "                                    type=\"text\"\n",
        "                                )\n",
        "                            ], width=6),\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Gender:\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"gender-filter\",\n",
        "                                    options=[\n",
        "                                        {\"label\": \"All\", \"value\": \"all\"},\n",
        "                                        {\"label\": \"Male\", \"value\": \"Male\"},\n",
        "                                        {\"label\": \"Female\", \"value\": \"Female\"}\n",
        "                                    ],\n",
        "                                    value=\"all\"\n",
        "                                )\n",
        "                            ], width=6)\n",
        "                        ]),\n",
        "\n",
        "                        # 3. Books Filters\n",
        "                        html.H5(\"3. Books Filters\", className=\"mt-3\"),\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Search Book:\"),\n",
        "                                dbc.Input(\n",
        "                                    id=\"book-search\",\n",
        "                                    placeholder=\"Name or ID\",\n",
        "                                    type=\"text\"\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        # Language, Type and Subject Filters - multi-select\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Language:\", className=\"mt-2\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"language-filter\",\n",
        "                                    options=[{\"label\": \"All Languages\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": lang, \"value\": lang} for lang in analyzer.unique_languages],\n",
        "                                    value=\"all\",\n",
        "                                    multi=True,\n",
        "                                    style={'fontSize': '12px'}\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Type:\", className=\"mt-2\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"type-filter\",\n",
        "                                    options=[{\"label\": \"All Types\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": type_val, \"value\": type_val} for type_val in analyzer.unique_types],\n",
        "                                    value=\"all\",\n",
        "                                    multi=True,\n",
        "                                    style={'fontSize': '12px'}\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label(\"Subject/Genre:\", className=\"mt-2\"),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"subject-filter\",\n",
        "                                    options=[{\"label\": \"All Subjects\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": subj, \"value\": subj} for subj in analyzer.unique_subjects],\n",
        "                                    value=\"all\",\n",
        "                                    multi=True,\n",
        "                                    style={'fontSize': '12px', 'whiteSpace': 'nowrap'},\n",
        "                                    optionHeight=35\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        # 4. Folder Filter\n",
        "                        html.H5(\"4. Folder Filter\", className=\"mt-3\"),\n",
        "                        dcc.Dropdown(\n",
        "                            id=\"folder-filter\",\n",
        "                            options=[\n",
        "                                {\"label\": \"All Folders\", \"value\": \"all\"}\n",
        "                            ] + [{\"label\": folder, \"value\": folder} for folder in analyzer.unique_folders],\n",
        "                            value=\"all\",\n",
        "                            multi=True\n",
        "                        ),\n",
        "\n",
        "                        # 5. Network Metrics\n",
        "                        html.H5(\"5. Network Metrics\", className=\"mt-3\"),\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label([\n",
        "                                    \"Community:\",\n",
        "                                    dbc.Button(\"?\", id=\"community-help\", size=\"sm\", color=\"info\",\n",
        "                                              className=\"ms-1\", style={\"fontSize\": \"10px\", \"padding\": \"0 5px\"})\n",
        "                                ]),\n",
        "                                dcc.Dropdown(\n",
        "                                    id=\"community-filter\",\n",
        "                                    options=[{\"label\": \"All\", \"value\": \"all\"}] +\n",
        "                                            [{\"label\": f\"Community {i}\", \"value\": i}\n",
        "                                             for i in range(len(analyzer.communities))],\n",
        "                                    value=\"all\"\n",
        "                                )\n",
        "                            ], width=6),\n",
        "                            dbc.Col([\n",
        "                                dbc.Label([\n",
        "                                    \"Min Betweenness Centrality:\",\n",
        "                                    dbc.Button(\"?\", id=\"betweenness-help\", size=\"sm\", color=\"info\",\n",
        "                                              className=\"ms-1\", style={\"fontSize\": \"10px\", \"padding\": \"0 5px\"})\n",
        "                                ]),\n",
        "                                dcc.Slider(\n",
        "                                    id=\"centrality-filter\",\n",
        "                                    min=0,\n",
        "                                    max=0.1,\n",
        "                                    step=0.001,\n",
        "                                    value=0,\n",
        "                                    marks={},\n",
        "                                    tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
        "                                )\n",
        "                            ], width=6)\n",
        "                        ]),\n",
        "\n",
        "                        dbc.Row([\n",
        "                            dbc.Col([\n",
        "                                dbc.Label([\n",
        "                                    \"Min Degree Centrality:\",\n",
        "                                    dbc.Button(\"?\", id=\"degree-help\", size=\"sm\", color=\"info\",\n",
        "                                              className=\"ms-1\", style={\"fontSize\": \"10px\", \"padding\": \"0 5px\"})\n",
        "                                ]),\n",
        "                                dcc.Slider(\n",
        "                                    id=\"degree-centrality-filter\",\n",
        "                                    min=0,\n",
        "                                    max=1.0,\n",
        "                                    step=0.01,\n",
        "                                    value=0,\n",
        "                                    marks={},\n",
        "                                    tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
        "                                )\n",
        "                            ], width=12)\n",
        "                        ]),\n",
        "\n",
        "                        # Max Nodes Slider with dynamic limits\n",
        "                        html.H5(\"6. Display Limits\", className=\"mt-3\"),\n",
        "                        dbc.Label(f\"Max Nodes (up to {max(people_slider_max, books_slider_max)}):\"),\n",
        "                        dcc.Slider(\n",
        "                            id=\"max-nodes-filter\",\n",
        "                            min=50,\n",
        "                            max=max(people_slider_max, books_slider_max),\n",
        "                            step=25,\n",
        "                            value=100,  # Back to 100\n",
        "                            marks={i: str(i) for i in range(50, max(people_slider_max, books_slider_max) + 1, max(50, max(people_slider_max, books_slider_max) // 10))},\n",
        "                            tooltip={\"placement\": \"bottom\", \"always_visible\": True}\n",
        "                        ),\n",
        "\n",
        "                        # Tooltips for help buttons\n",
        "                        dbc.Tooltip(\n",
        "                            \"Communities are groups of nodes that are more densely connected to each other than to the rest of the network. \"\n",
        "                            \"This helps identify groups of people who read similar books or shared populations.\",\n",
        "                            target=\"community-help\",\n",
        "                            placement=\"top\"\n",
        "                        ),\n",
        "                        dbc.Tooltip(\n",
        "                            \"Betweenness Centrality measures how often a node lies on the shortest path between other pairs of nodes. \"\n",
        "                            \"A high value indicates an important 'bridge' or mediator between different groups of people.\",\n",
        "                            target=\"betweenness-help\",\n",
        "                            placement=\"top\"\n",
        "                        ),\n",
        "                        dbc.Tooltip(\n",
        "                            \"Degree Centrality measures the number of direct connections of a node divided by the maximum possible number. \"\n",
        "                            \"A high value indicates a person who is connected to many people - 'popular' or central in the network.\",\n",
        "                            target=\"degree-help\",\n",
        "                            placement=\"top\"\n",
        "                        ),\n",
        "\n",
        "                        # Reset Button\n",
        "                        dbc.Button(\n",
        "                            \"Reset All Filters\",\n",
        "                            id=\"reset-filters\",\n",
        "                            color=\"secondary\",\n",
        "                            className=\"mt-3 w-100\"\n",
        "                        )\n",
        "                    ])\n",
        "                ])\n",
        "            ], width=3),\n",
        "\n",
        "            # Main Network Graph\n",
        "            dbc.Col([\n",
        "                dbc.Card([\n",
        "                    dbc.CardBody([\n",
        "                        dcc.Loading(\n",
        "                            id=\"loading-graph\",\n",
        "                            type=\"default\",\n",
        "                            children=[\n",
        "                                dcc.Graph(\n",
        "                                    id=\"network-graph\",\n",
        "                                    style={'height': '900px'},  # Increased from 700px to 900px\n",
        "                                    config={\n",
        "                                        'displayModeBar': True,\n",
        "                                        'toImageButtonOptions': {'height': 900, 'width': 1200},  # Updated export size\n",
        "                                        'scrollZoom': True,\n",
        "                                        'doubleClick': 'reset',\n",
        "                                        'modeBarButtonsToAdd': ['pan2d'],\n",
        "                                        'modeBarButtonsToRemove': []\n",
        "                                    }\n",
        "                                ),\n",
        "                            ]\n",
        "                        )\n",
        "                    ])\n",
        "                ])\n",
        "            ], width=9)\n",
        "        ]),\n",
        "\n",
        "        # Statistics Row\n",
        "        dbc.Row([\n",
        "            dbc.Col([\n",
        "                dbc.Card([\n",
        "                    dbc.CardHeader(html.H4(\"Network Statistics\", className=\"mb-0\")),\n",
        "                    dbc.CardBody([\n",
        "                        html.Div(id=\"network-stats\")\n",
        "                    ])\n",
        "                ])\n",
        "            ])\n",
        "        ], className=\"mt-4\")\n",
        "    ], fluid=True)\n",
        "\n",
        "    # Callback for updating the network graph with all parameters\n",
        "    @app.callback(\n",
        "        [Output(\"network-graph\", \"figure\"),\n",
        "         Output(\"network-stats\", \"children\")],\n",
        "        [Input(\"data-sources-filter\", \"value\"),\n",
        "         Input(\"person-search\", \"value\"),\n",
        "         Input(\"book-search\", \"value\"),\n",
        "         Input(\"gender-filter\", \"value\"),\n",
        "         Input(\"folder-filter\", \"value\"),\n",
        "         Input(\"community-filter\", \"value\"),\n",
        "         Input(\"centrality-filter\", \"value\"),\n",
        "         Input(\"degree-centrality-filter\", \"value\"),\n",
        "         Input(\"language-filter\", \"value\"),\n",
        "         Input(\"type-filter\", \"value\"),\n",
        "         Input(\"subject-filter\", \"value\"),\n",
        "         Input(\"max-nodes-filter\", \"value\"),\n",
        "         Input(\"reset-filters\", \"n_clicks\")],\n",
        "        prevent_initial_call=False\n",
        "    )\n",
        "    def update_network_graph(data_sources, person_search, book_search, gender_filter,\n",
        "                             folder_filter, community_filter, centrality_filter, degree_centrality_filter,\n",
        "                             language_filter, type_filter, subject_filter, max_nodes, reset_clicks):\n",
        "        try:\n",
        "            ctx = callback_context\n",
        "            if ctx.triggered and ctx.triggered[0]['prop_id'] == 'reset-filters.n_clicks':\n",
        "                show_people = True\n",
        "                show_books = True\n",
        "                person_filter = None\n",
        "                book_filter = None\n",
        "                gender_filter = \"all\"\n",
        "                folder_filter = \"all\"\n",
        "                community_filter = \"all\"\n",
        "                centrality_filter = 0\n",
        "                degree_centrality_filter = 0\n",
        "                language_filter = \"all\"\n",
        "                type_filter = \"all\"\n",
        "                subject_filter = \"all\"\n",
        "                max_nodes = 100  # Back to 100\n",
        "            else:\n",
        "                show_people = \"people\" in (data_sources or [])\n",
        "                show_books = \"books\" in (data_sources or [])\n",
        "                person_filter = person_search if person_search else None\n",
        "                book_filter = book_search if book_search else None\n",
        "\n",
        "                if not folder_filter or folder_filter == \"all\":\n",
        "                    folder_filter = None\n",
        "                elif isinstance(folder_filter, list):\n",
        "                    if \"all\" in folder_filter:\n",
        "                        folder_filter = None\n",
        "\n",
        "                if not language_filter or language_filter == \"all\":\n",
        "                    language_filter = None\n",
        "                elif isinstance(language_filter, list):\n",
        "                    if \"all\" in language_filter or len(language_filter) == 0:\n",
        "                        language_filter = None\n",
        "\n",
        "                if not type_filter or type_filter == \"all\":\n",
        "                    type_filter = None\n",
        "                elif isinstance(type_filter, list):\n",
        "                    if \"all\" in type_filter or len(type_filter) == 0:\n",
        "                        type_filter = None\n",
        "\n",
        "                if not subject_filter or subject_filter == \"all\":\n",
        "                    subject_filter = None\n",
        "                elif isinstance(subject_filter, list):\n",
        "                    if \"all\" in subject_filter or len(subject_filter) == 0:\n",
        "                        subject_filter = None\n",
        "\n",
        "            fig = create_network_plot(\n",
        "                analyzer,\n",
        "                show_people=show_people,\n",
        "                show_books=show_books,\n",
        "                person_filter=person_filter,\n",
        "                book_filter=book_filter,\n",
        "                community_filter=community_filter,\n",
        "                folder_filter=folder_filter,\n",
        "                gender_filter=gender_filter,\n",
        "                centrality_filter=centrality_filter,\n",
        "                degree_centrality_filter=degree_centrality_filter,\n",
        "                language_filter=language_filter,\n",
        "                type_filter=type_filter,\n",
        "                subject_filter=subject_filter,\n",
        "                max_nodes=max_nodes\n",
        "            )\n",
        "\n",
        "            stats = calculate_network_statistics(\n",
        "                analyzer, show_people, show_books, person_filter, book_filter,\n",
        "                community_filter, folder_filter, gender_filter, centrality_filter,\n",
        "                degree_centrality_filter, language_filter, type_filter, subject_filter, max_nodes\n",
        "            )\n",
        "\n",
        "            return fig, stats\n",
        "\n",
        "        except Exception as e:\n",
        "            empty_fig = go.Figure()\n",
        "            empty_fig.add_annotation(\n",
        "                text=f\"Error loading graph: {str(e)}\",\n",
        "                xref=\"paper\", yref=\"paper\",\n",
        "                x=0.5, y=0.5, xanchor='center', yanchor='middle',\n",
        "                showarrow=False, font=dict(size=16)\n",
        "            )\n",
        "            empty_fig.update_layout(\n",
        "                title=\"Error\",\n",
        "                showlegend=False,\n",
        "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
        "                plot_bgcolor='white'\n",
        "            )\n",
        "            return empty_fig, [html.P(f\"Error: {str(e)}\")]\n",
        "\n",
        "    # Callback for updating slider ranges based on network type\n",
        "    @app.callback(\n",
        "        [Output(\"centrality-filter\", \"max\"),\n",
        "         Output(\"centrality-filter\", \"marks\"),\n",
        "         Output(\"centrality-filter\", \"step\"),\n",
        "         Output(\"degree-centrality-filter\", \"max\"),\n",
        "         Output(\"degree-centrality-filter\", \"marks\")],\n",
        "        [Input(\"data-sources-filter\", \"value\")]\n",
        "    )\n",
        "    def update_centrality_sliders(data_sources):\n",
        "        show_people = \"people\" in (data_sources or [])\n",
        "        show_books = \"books\" in (data_sources or [])\n",
        "\n",
        "        if show_books and not show_people:\n",
        "            # Books only - use book ranges\n",
        "            betweenness_max = max(0.1, analyzer.book_betweenness_max)\n",
        "            degree_max = max(0.1, analyzer.book_degree_max)\n",
        "            step = betweenness_max / 1000\n",
        "            betweenness_marks = {0: \"0\", betweenness_max: f\"{betweenness_max:.3f}\"}\n",
        "            degree_marks = {0: \"0\", degree_max: f\"{degree_max:.3f}\"}\n",
        "        elif show_people and not show_books:\n",
        "            # People only - use people ranges\n",
        "            betweenness_max = max(0.1, analyzer.person_betweenness_max)\n",
        "            degree_max = max(0.1, analyzer.person_degree_max)\n",
        "            step = betweenness_max / 100\n",
        "            betweenness_marks = {0: \"0\", betweenness_max: f\"{betweenness_max:.3f}\"}\n",
        "            degree_marks = {0: \"0\", degree_max: f\"{degree_max:.3f}\"}\n",
        "        else:\n",
        "            # Both or default - use people ranges\n",
        "            betweenness_max = max(0.1, analyzer.person_betweenness_max)\n",
        "            degree_max = max(0.1, analyzer.person_degree_max)\n",
        "            step = betweenness_max / 100\n",
        "            betweenness_marks = {0: \"0\", betweenness_max: f\"{betweenness_max:.3f}\"}\n",
        "            degree_marks = {0: \"0\", degree_max: f\"{degree_max:.3f}\"}\n",
        "\n",
        "        return betweenness_max, betweenness_marks, step, degree_max, degree_marks\n",
        "\n",
        "    # Callback for resetting filters\n",
        "    @app.callback(\n",
        "        [Output(\"data-sources-filter\", \"value\"),\n",
        "         Output(\"person-search\", \"value\"),\n",
        "         Output(\"book-search\", \"value\"),\n",
        "         Output(\"gender-filter\", \"value\"),\n",
        "         Output(\"folder-filter\", \"value\"),\n",
        "         Output(\"community-filter\", \"value\"),\n",
        "         Output(\"centrality-filter\", \"value\"),\n",
        "         Output(\"degree-centrality-filter\", \"value\"),\n",
        "         Output(\"language-filter\", \"value\"),\n",
        "         Output(\"type-filter\", \"value\"),\n",
        "         Output(\"subject-filter\", \"value\"),\n",
        "         Output(\"max-nodes-filter\", \"value\")],\n",
        "        [Input(\"reset-filters\", \"n_clicks\")],\n",
        "        prevent_initial_call=True\n",
        "    )\n",
        "    def reset_all_filters(n_clicks):\n",
        "        if n_clicks:\n",
        "            return (\n",
        "                [\"people\", \"books\"],\n",
        "                \"\",\n",
        "                \"\",\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                0,\n",
        "                0,\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                \"all\",\n",
        "                100  # Back to 100\n",
        "            )\n",
        "        return dash.no_update\n",
        "\n",
        "    return app\n",
        "\n",
        "def get_csv_filepath():\n",
        "    \"\"\"Ask user for CSV file path and verify it exists\"\"\"\n",
        "    while True:\n",
        "        print(\"Please enter the full path to your CSV file:\")\n",
        "        print(\"   Example: /content/borrowers_data.csv\")\n",
        "        print(\"   Or simply: borrowers_data.csv (if file is in same directory)\")\n",
        "\n",
        "        filepath = input(\"\\nFile path: \").strip()\n",
        "\n",
        "        # Check if file exists\n",
        "        if os.path.exists(filepath):\n",
        "            print(f\"File found: {filepath}\")\n",
        "            return filepath\n",
        "        else:\n",
        "            print(f\"Error: File {filepath} not found!\")\n",
        "            print(\"Please try again with a valid path\\n\")\n",
        "\n",
        "def setup_simple_tunnel(port=8050):\n",
        "    \"\"\"Setup tunnel using the most reliable method\"\"\"\n",
        "    import subprocess\n",
        "    import threading\n",
        "    import time\n",
        "    import re\n",
        "\n",
        "    # Method 1: Try localtunnel first (most reliable)\n",
        "    try:\n",
        "        # Install Node.js and localtunnel\n",
        "        subprocess.run([\"curl\", \"-fsSL\", \"https://deb.nodesource.com/setup_16.x\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        subprocess.run([\"sudo\", \"bash\", \"/tmp/setup_16.x\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        subprocess.run([\"sudo\", \"apt-get\", \"install\", \"-y\", \"nodejs\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        subprocess.run([\"sudo\", \"npm\", \"install\", \"-g\", \"localtunnel\"],\n",
        "                      stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "        # Start localtunnel with custom subdomain to avoid password issues\n",
        "        subdomain = f\"dashboard-{int(time.time() % 10000)}\"\n",
        "\n",
        "        def run_localtunnel():\n",
        "            try:\n",
        "                process = subprocess.Popen([\n",
        "                    \"lt\", \"--port\", str(port), \"--subdomain\", subdomain\n",
        "                ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "                time.sleep(3)\n",
        "                stdout, stderr = process.communicate(timeout=10)\n",
        "\n",
        "                # Look for URL in output\n",
        "                if f\"https://{subdomain}.loca.lt\" in stdout or f\"https://{subdomain}.loca.lt\" in stderr:\n",
        "                    public_url = f\"https://{subdomain}.loca.lt\"\n",
        "                    print(f\"Public URL: {public_url}\")\n",
        "                    return True\n",
        "\n",
        "            except Exception:\n",
        "                return False\n",
        "\n",
        "        if run_localtunnel():\n",
        "            return True\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Method 2: Try ngrok without auth (limited time but works)\n",
        "    try:\n",
        "        from pyngrok import ngrok\n",
        "\n",
        "        # Try to create tunnel without auth (works for limited time)\n",
        "        public_url = ngrok.connect(port)\n",
        "\n",
        "        print(f\"Public URL: {public_url}\")\n",
        "        return True\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Method 3: Manual instructions as fallback\n",
        "    print(\"Automatic tunnel setup failed.\")\n",
        "    print(\"Please run this command in a NEW Colab cell:\")\n",
        "    print(\"!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\")\n",
        "    print(\"!unzip ngrok-stable-linux-amd64.zip\")\n",
        "    print(\"!./ngrok http 8050\")\n",
        "    print(\"Then copy the https URL that appears\")\n",
        "\n",
        "    return False\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Ask user for CSV file path\n",
        "    csv_path = get_csv_filepath()\n",
        "\n",
        "    print(f\"\\nCreating dashboard with file: {csv_path}\")\n",
        "    print(\"Loading data and building networks...\")\n",
        "\n",
        "    app = create_optimized_dashboard(csv_path)\n",
        "\n",
        "    print(\"\\nDashboard created successfully!\")\n",
        "\n",
        "    # Setup automatic public tunnel with multiple fallbacks\n",
        "    tunnel_success = setup_simple_tunnel(8050)\n",
        "\n",
        "    if not tunnel_success:\n",
        "        print(f\"Local access: http://localhost:8050\")\n",
        "\n",
        "    print(\"\\nStarting dashboard server...\")\n",
        "    app.run(debug=False, host='0.0.0.0', port=8050)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "!./ngrok http 8050"
      ],
      "metadata": {
        "id": "G_74BGynGJ8p",
        "outputId": "dbe76464-af4a-494e-bc22-293c3449e13e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 12:53:47--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 75.2.60.68, 35.71.179.82, 99.83.220.108, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|75.2.60.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‚Äòngrok-stable-linux-amd64.zip.1‚Äô\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  18.8MB/s    in 0.7s    \n",
            "\n",
            "2025-07-28 12:53:48 (18.8 MB/s) - ‚Äòngrok-stable-linux-amd64.zip.1‚Äô saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Usage of ngrok requires a verified account and authtoken.\n",
            "\n",
            "Sign up for an account: https://dashboard.ngrok.com/signup\n",
            "Install your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\n",
            "\n",
            "ERR_NGROK_4018\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD8lYyPuTZWG",
        "outputId": "e546af53-a1d0-476d-8fc3-49026985d92b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}